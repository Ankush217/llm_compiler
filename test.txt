apple@Apples-MacBook-Air alwork % lsr --exclude venv
====== DIRECTORY TREE ======

ðŸ“ .
-----------------------------
llm_compiler	venv

ðŸ“ ./llm_compiler
-----------------------------
__init__.py	emitters	solver		templates	utils
compile.py	ir		spec.py		tokenizers

ðŸ“ ./llm_compiler/tokenizers
-----------------------------
__init__.py	base.py		bpe.py		unigram.py

ðŸ“ ./llm_compiler/ir
-----------------------------
__init__.py	builder.py	graph.py	node.py		tensor.py

ðŸ“ ./llm_compiler/utils
-----------------------------
__init__.py	math_utils.py	parameters.py	validation.py

ðŸ“ ./llm_compiler/solver
-----------------------------
__init__.py		constraints.py		dimension_solver.py	param_solver.py

ðŸ“ ./llm_compiler/templates
-----------------------------
__init__.py		decoder_only.py		registry.py
base.py			encoder_decoder.py

ðŸ“ ./llm_compiler/emitters
-----------------------------
__init__.py	base.py		pytorch.py	safetensors.py	torchscript.py

====== FILE CONTENTS (TEXT ONLY) ======

ðŸ“„ ./llm_compiler/tokenizers/__init__.py
=============================
from .base import (
    BaseTokenizer,
    ToyBPETokenizer,
    ToyUnigramTokenizer,
    TokenizerTrainingStats,
)

__all__ = [
    "BaseTokenizer",
    "ToyBPETokenizer",
    "ToyUnigramTokenizer",
    "TokenizerTrainingStats",
]

ðŸ“„ ./llm_compiler/tokenizers/base.py
=============================
"""
Tokenizer base classes
======================

Lightweight implementations to keep tokenizer definition, vocab size,
and corpus statistics bound together. These implementations are minimal
but enforce the invariants needed by the compiler/emitters.
"""

from __future__ import annotations
from dataclasses import dataclass, field
from typing import Dict, List, Iterable


@dataclass
class TokenizerTrainingStats:
    """Corpus statistics captured during tokenizer training."""
    token_freqs: Dict[str, int]
    total_tokens: int
    unique_tokens: int

    @classmethod
    def from_counts(cls, counts: Dict[str, int]) -> "TokenizerTrainingStats":
        total = sum(counts.values())
        return cls(
            token_freqs=dict(counts),
            total_tokens=total,
            unique_tokens=len(counts),
        )


class BaseTokenizer:
    """Minimal tokenizer contract used by the compiler."""

    def __init__(self, vocab_size: int, unk_token: str = "<unk>", pad_token: str = "<pad>"):
        self.vocab_size = vocab_size
        self.unk_token = unk_token
        self.pad_token = pad_token
        self.token_to_id: Dict[str, int] = {}
        self.id_to_token: List[str] = []
        self.training_stats: TokenizerTrainingStats | None = None

    # --- binding / validation -------------------------------------------------
    def bind_training_stats(self, stats: TokenizerTrainingStats) -> None:
        """Attach corpus statistics and ensure the vocab respects frequencies."""
        self.training_stats = stats
        if stats.unique_tokens < self.vocab_size:
            raise ValueError(
                f"Requested vocab_size={self.vocab_size} exceeds unique tokens in corpus ({stats.unique_tokens})"
            )

    def enforce_vocab_size(self):
        if len(self.id_to_token) != self.vocab_size:
            raise ValueError(
                f"Tokenizer vocab size mismatch: expected {self.vocab_size}, got {len(self.id_to_token)}"
            )

    # --- interface -----------------------------------------------------------
    def encode(self, text: str) -> List[int]:
        raise NotImplementedError

    def decode(self, ids: Iterable[int]) -> str:
        raise NotImplementedError

    def train(self, stats: TokenizerTrainingStats):
        """Train tokenizer using provided corpus stats."""
        raise NotImplementedError


class ToyBPETokenizer(BaseTokenizer):
    """
    Minimal stand-in BPE tokenizer.
    Not production-ready but enforces vocab/statistics coupling.
    """

    def train(self, stats: TokenizerTrainingStats):
        self.bind_training_stats(stats)
        # Build vocab by most frequent tokens first
        sorted_tokens = sorted(stats.token_freqs.items(), key=lambda kv: kv[1], reverse=True)
        top_tokens = [tok for tok, _ in sorted_tokens[: self.vocab_size - 2]]
        # Reserve UNK and PAD
        vocab = [self.pad_token, self.unk_token] + top_tokens
        self.id_to_token = vocab[: self.vocab_size]
        self.token_to_id = {tok: idx for idx, tok in enumerate(self.id_to_token)}
        self.enforce_vocab_size()
        return self

    def encode(self, text: str) -> List[int]:
        # Very naive whitespace tokenization for placeholder behavior
        tokens = text.split()
        ids = []
        for tok in tokens:
            ids.append(self.token_to_id.get(tok, self.token_to_id.get(self.unk_token, 1)))
        return ids

    def decode(self, ids: Iterable[int]) -> str:
        return " ".join(self.id_to_token[i] if 0 <= i < len(self.id_to_token) else self.unk_token for i in ids)


class ToyUnigramTokenizer(BaseTokenizer):
    """Simple unigram tokenizer with frequency-aware vocab selection."""

    def train(self, stats: TokenizerTrainingStats):
        self.bind_training_stats(stats)
        sorted_tokens = sorted(stats.token_freqs.items(), key=lambda kv: kv[1], reverse=True)
        top_tokens = [tok for tok, _ in sorted_tokens[: self.vocab_size - 2]]
        vocab = [self.pad_token, self.unk_token] + top_tokens
        self.id_to_token = vocab[: self.vocab_size]
        self.token_to_id = {tok: idx for idx, tok in enumerate(self.id_to_token)}
        self.enforce_vocab_size()
        return self

    def encode(self, text: str) -> List[int]:
        tokens = text.split()
        return [self.token_to_id.get(tok, self.token_to_id.get(self.unk_token, 1)) for tok in tokens]

    def decode(self, ids: Iterable[int]) -> str:
        return " ".join(self.id_to_token[i] if 0 <= i < len(self.id_to_token) else self.unk_token for i in ids)

ðŸ“„ ./llm_compiler/ir/graph.py
=============================
"""
Intermediate Representation (IR)
================================

Framework-agnostic graph representation of LLM architectures.
Explicit tensors, shapes, and connections.
"""

from __future__ import annotations
from typing import Dict, List, Tuple, Optional, Any, Set
from dataclasses import dataclass, field
from enum import Enum
import json

class TensorDType(Enum):
    """Tensor data types"""
    FLOAT32 = "float32"
    FLOAT16 = "float16"
    BFLOAT16 = "bfloat16"
    INT32 = "int32"
    INT64 = "int64"
    BOOL = "bool"

class NodeType(Enum):
    """IR node types"""
    INPUT = "input"
    OUTPUT = "output"
    CONSTANT = "constant"
    
    # Operations
    LINEAR = "linear"
    EMBEDDING = "embedding"
    LAYERNORM = "layernorm"
    RMSNORM = "rmsnorm"
    ATTENTION = "attention"
    MULTI_HEAD_ATTENTION = "multi_head_attention"
    ROPE = "rope"
    ALIBI = "alibi"
    SWIGLU = "swiglu"
    ACTIVATION = "activation"
    ADD = "add"
    MUL = "mul"
    MATMUL = "matmul"
    TRANSPOSE = "transpose"
    RESHAPE = "reshape"
    CONCAT = "concat"
    SPLIT = "split"
    SLICE = "slice"
    GATHER = "gather"
    SOFTMAX = "softmax"
    DROPOUT = "dropout"
    
    # Control flow
    LOOP = "loop"
    CONDITIONAL = "conditional"

@dataclass
class Tensor:
    """IR Tensor representation"""
    name: str
    shape: List[Any]  # Can contain -1 for dynamic dimensions
    dtype: TensorDType
    node: Optional[str] = None  # Producing node
    consumers: List[str] = field(default_factory=list)  # Consuming nodes
    
    def __str__(self) -> str:
        shape_str = "[" + ", ".join(str(s) for s in self.shape) + "]"
        return f"{self.name}: {shape_str} {self.dtype.value}"
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "shape": self.shape,
            "dtype": self.dtype.value,
            "node": self.node,
            "consumers": self.consumers
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Tensor':
        return cls(
            name=data["name"],
            shape=data["shape"],
            dtype=TensorDType(data["dtype"]),
            node=data.get("node"),
            consumers=data.get("consumers", [])
        )

@dataclass
class Operation:
    """IR Operation node"""
    name: str
    node_type: NodeType
    inputs: List[str]  # Input tensor names
    outputs: List[str]  # Output tensor names
    attributes: Dict[str, Any] = field(default_factory=dict)
    
    def __str__(self) -> str:
        inputs_str = ", ".join(self.inputs)
        outputs_str = ", ".join(self.outputs)
        attrs_str = ""
        if self.attributes:
            attrs_str = " " + " ".join(f"{k}={v}" for k, v in self.attributes.items())
        return f"{self.name}: {self.node_type.value}({inputs_str}) -> {outputs_str}{attrs_str}"
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "type": self.node_type.value,
            "inputs": self.inputs,
            "outputs": self.outputs,
            "attributes": self.attributes
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Operation':
        return cls(
            name=data["name"],
            node_type=NodeType(data["type"]),
            inputs=data["inputs"],
            outputs=data["outputs"],
            attributes=data.get("attributes", {})
        )

class IRGraph:
    """Complete IR Graph"""
    
    def __init__(self, name: str):
        self.name = name
        self.tensors: Dict[str, Tensor] = {}
        self.operations: Dict[str, Operation] = {}
        self.inputs: List[str] = []
        self.outputs: List[str] = []
    
    def add_tensor(self, tensor: Tensor):
        """Add tensor to graph"""
        if tensor.name in self.tensors:
            raise ValueError(f"Tensor {tensor.name} already exists")
        self.tensors[tensor.name] = tensor
    
    def add_operation(self, operation: Operation):
        """Add operation to graph"""
        if operation.name in self.operations:
            raise ValueError(f"Operation {operation.name} already exists")
        
        # Update tensor references
        for output in operation.outputs:
            if output in self.tensors:
                self.tensors[output].node = operation.name
            else:
                # Create output tensor
                self.tensors[output] = Tensor(
                    name=output,
                    shape=[],  # Unknown shape
                    dtype=TensorDType.FLOAT32,  # Default
                    node=operation.name
                )
        
        # Update input tensor consumers
        for input_name in operation.inputs:
            if input_name in self.tensors:
                self.tensors[input_name].consumers.append(operation.name)
            else:
                # Create input tensor if it doesn't exist
                self.tensors[input_name] = Tensor(
                    name=input_name,
                    shape=[],  # Unknown shape
                    dtype=TensorDType.FLOAT32,  # Default
                    consumers=[operation.name]
                )
        
        self.operations[operation.name] = operation
    
    def add_input(self, tensor_name: str):
        """Mark tensor as graph input"""
        if tensor_name not in self.inputs:
            self.inputs.append(tensor_name)
    
    def add_output(self, tensor_name: str):
        """Mark tensor as graph output"""
        if tensor_name not in self.outputs:
            self.outputs.append(tensor_name)
    
    def validate(self) -> List[str]:
        """Validate graph consistency"""
        errors = []
        
        # Check all tensors have producers or are inputs
        for tensor_name, tensor in self.tensors.items():
            if tensor.node is None and tensor_name not in self.inputs:
                errors.append(f"Tensor {tensor_name} has no producer and is not an input")
            
            # Check consumers exist
            for consumer in tensor.consumers:
                if consumer not in self.operations:
                    errors.append(f"Tensor {tensor_name} references non-existent consumer {consumer}")
        
        # Check operations reference valid tensors
        for op_name, operation in self.operations.items():
            for input_name in operation.inputs:
                if input_name not in self.tensors:
                    errors.append(f"Operation {op_name} references non-existent input {input_name}")
            
            for output_name in operation.outputs:
                if output_name not in self.tensors:
                    errors.append(f"Operation {op_name} references non-existent output {output_name}")
        
        # Check outputs exist
        for output_name in self.outputs:
            if output_name not in self.tensors:
                errors.append(f"Output {output_name} does not exist")

        # Semantic validation (shapes/dtypes)
        errors.extend(self.validate_semantics())

        return errors

    def _topological_order(self) -> List[str]:
        """Return operation names in topological order"""
        indegree = {name: 0 for name in self.operations}
        for op in self.operations.values():
            for inp in op.inputs:
                producer = self.tensors.get(inp, Tensor("", [], TensorDType.FLOAT32)).node
                if producer and producer in indegree:
                    indegree[op.name] += 1
        ready = [name for name, deg in indegree.items() if deg == 0]
        order = []
        while ready:
            current = ready.pop(0)
            order.append(current)
            op = self.operations[current]
            for out in op.outputs:
                for consumer in self.tensors.get(out, Tensor("", [], TensorDType.FLOAT32)).consumers:
                    if consumer in indegree:
                        indegree[consumer] -= 1
                        if indegree[consumer] == 0:
                            ready.append(consumer)
        # Fallback to insertion order if cycle detected
        if len(order) != len(self.operations):
            return list(self.operations.keys())
        return order

    def validate_semantics(self) -> List[str]:
        """Validate shapes/dtypes for common operations"""
        errors: List[str] = []
        tensor_shapes = {name: tensor.shape for name, tensor in self.tensors.items()}
        tensor_dtypes = {name: tensor.dtype for name, tensor in self.tensors.items()}

        for op_name in self._topological_order():
            op = self.operations[op_name]
            try:
                if op.node_type == NodeType.LINEAR:
                    in_tensor = op.inputs[0]
                    in_shape = tensor_shapes.get(in_tensor)
                    in_features = op.attributes.get("in_features")
                    out_features = op.attributes.get("out_features")
                    if in_shape and in_features is not None and len(in_shape) > 0:
                        if in_shape[-1] != in_features:
                            errors.append(
                                f"{op_name}: input last dim {in_shape[-1]} "
                                f"does not match linear in_features {in_features}"
                            )
                    if out_features is not None:
                        prefix = in_shape[:-1] if in_shape else [-1]
                        tensor_shapes[op.outputs[0]] = prefix + [out_features]
                        tensor_dtypes[op.outputs[0]] = TensorDType.FLOAT32

                elif op.node_type in (NodeType.RMSNORM, NodeType.LAYERNORM):
                    src = op.inputs[0]
                    in_shape = tensor_shapes.get(src)
                    normalized_shape = op.attributes.get("normalized_shape")
                    if in_shape and normalized_shape is not None:
                        if in_shape[-1] != normalized_shape:
                            errors.append(
                                f"{op_name}: normalized_shape {normalized_shape} "
                                f"does not match input hidden dim {in_shape[-1]}"
                            )
                    tensor_shapes[op.outputs[0]] = list(in_shape) if in_shape else []
                    tensor_dtypes[op.outputs[0]] = TensorDType.FLOAT32

                elif op.node_type == NodeType.EMBEDDING:
                    input_name = op.inputs[0]
                    in_shape = tensor_shapes.get(input_name, [])
                    vocab = op.attributes.get("vocab_size")
                    if vocab is None:
                        errors.append(f"{op_name}: missing vocab_size attribute")
                    tensor_shapes[op.outputs[0]] = in_shape + [op.attributes.get("embedding_dim", 0)]
                    tensor_dtypes[op.outputs[0]] = TensorDType.FLOAT32

                elif op.node_type == NodeType.MULTI_HEAD_ATTENTION:
                    q_name, k_name, v_name = op.inputs
                    q_shape = tensor_shapes.get(q_name)
                    k_shape = tensor_shapes.get(k_name)
                    v_shape = tensor_shapes.get(v_name)
                    num_heads = op.attributes.get("num_heads")
                    num_kv_heads = op.attributes.get("num_kv_heads", num_heads)
                    head_dim = op.attributes.get("head_dim")
                    if q_shape and num_heads and head_dim and q_shape[-1] != num_heads * head_dim:
                        errors.append(
                            f"{op_name}: query hidden dim {q_shape[-1]} != num_heads*head_dim ({num_heads*head_dim})"
                        )
                    if k_shape and num_kv_heads and head_dim and k_shape[-1] != num_kv_heads * head_dim:
                        errors.append(
                            f"{op_name}: key hidden dim {k_shape[-1]} != num_kv_heads*head_dim ({num_kv_heads*head_dim})"
                        )
                    if v_shape and num_kv_heads and head_dim and v_shape[-1] != num_kv_heads * head_dim:
                        errors.append(
                            f"{op_name}: value hidden dim {v_shape[-1]} != num_kv_heads*head_dim ({num_kv_heads*head_dim})"
                        )
                    # Output shape follows query
                    if q_shape:
                        tensor_shapes[op.outputs[0]] = list(q_shape)
                        tensor_dtypes[op.outputs[0]] = TensorDType.FLOAT32

                elif op.node_type in (NodeType.ADD, NodeType.MUL):
                    a_shape = tensor_shapes.get(op.inputs[0])
                    b_shape = tensor_shapes.get(op.inputs[1])
                    if a_shape and b_shape and a_shape != b_shape:
                        errors.append(f"{op_name}: operand shape mismatch {a_shape} vs {b_shape}")
                    tensor_shapes[op.outputs[0]] = a_shape or b_shape or []
                    tensor_dtypes[op.outputs[0]] = tensor_dtypes.get(op.inputs[0], TensorDType.FLOAT32)

                elif op.node_type == NodeType.SWIGLU:
                    gate_shape = tensor_shapes.get(op.inputs[0])
                    up_shape = tensor_shapes.get(op.inputs[1])
                    if gate_shape and up_shape and gate_shape != up_shape:
                        errors.append(f"{op_name}: gate/up shapes differ {gate_shape} vs {up_shape}")
                    tensor_shapes[op.outputs[0]] = gate_shape or up_shape or []
                    tensor_dtypes[op.outputs[0]] = TensorDType.FLOAT32

                elif op.node_type == NodeType.ACTIVATION:
                    src = op.inputs[0]
                    tensor_shapes[op.outputs[0]] = list(tensor_shapes.get(src, []))
                    tensor_dtypes[op.outputs[0]] = tensor_dtypes.get(src, TensorDType.FLOAT32)

                else:
                    # Default: propagate first input shape if available
                    if op.inputs:
                        tensor_shapes[op.outputs[0]] = list(tensor_shapes.get(op.inputs[0], []))
                        tensor_dtypes[op.outputs[0]] = tensor_dtypes.get(op.inputs[0], TensorDType.FLOAT32)
            except Exception as exc:
                errors.append(f"{op_name}: semantic validation error {exc}")

        return errors
    
    def get_parameter_count(self) -> int:
        """Estimate parameter count from graph"""
        from ..utils.parameters import count_parameters_from_ir
        return count_parameters_from_ir(self)
    
    def to_dict(self) -> Dict[str, Any]:
        """Serialize graph to dictionary"""
        return {
            "name": self.name,
            "tensors": {name: tensor.to_dict() for name, tensor in self.tensors.items()},
            "operations": {name: op.to_dict() for name, op in self.operations.items()},
            "inputs": self.inputs,
            "outputs": self.outputs
        }
    
    def to_json(self) -> str:
        """Serialize graph to JSON"""
        return json.dumps(self.to_dict(), indent=2)
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'IRGraph':
        """Deserialize graph from dictionary"""
        graph = cls(data["name"])
        
        # Load tensors
        for name, tensor_data in data.get("tensors", {}).items():
            graph.tensors[name] = Tensor.from_dict(tensor_data)
        
        # Load operations
        for name, op_data in data.get("operations", {}).items():
            graph.operations[name] = Operation.from_dict(op_data)
        
        graph.inputs = data.get("inputs", [])
        graph.outputs = data.get("outputs", [])
        
        return graph
    
    @classmethod
    def from_json(cls, json_str: str) -> 'IRGraph':
        """Deserialize graph from JSON"""
        data = json.loads(json_str)
        return cls.from_dict(data)
    
    def __str__(self) -> str:
        lines = [f"IRGraph: {self.name}"]
        lines.append(f"  Tensors: {len(self.tensors)}")
        lines.append(f"  Operations: {len(self.operations)}")
        lines.append(f"  Inputs: {self.inputs}")
        lines.append(f"  Outputs: {self.outputs}")
        
        # Show operations
        lines.append("\nOperations:")
        for op in self.operations.values():
            lines.append(f"  {op}")
        
        # Show tensors
        lines.append("\nTensors:")
        for tensor in self.tensors.values():
            lines.append(f"  {tensor}")
        
        return "\n".join(lines)

ðŸ“„ ./llm_compiler/ir/builder.py
=============================
"""
IR Graph Builder
================

Helper for building IR graphs from templates.
Provides high-level operations that map to IR nodes.
"""

from typing import Dict, List, Tuple, Optional, Any, Union
from .graph import IRGraph, Tensor, Operation, NodeType, TensorDType

class GraphBuilder:
    """Builder for IR graphs"""
    
    def __init__(self, graph: IRGraph = None):
        self.graph = graph or IRGraph("unnamed")
        self._tensor_counter = 0
        self._op_counter = 0
    
    def _new_name(self, prefix: str) -> str:
        """Generate unique name"""
        self._tensor_counter += 1
        return f"{prefix}_{self._tensor_counter}"
    
    def _new_op_name(self, prefix: str) -> str:
        """Generate unique operation name"""
        self._op_counter += 1
        return f"{prefix}_{self._op_counter}"
    
    def create_input(self, 
                     name: str,
                     shape: List[Any],
                     dtype: Union[str, TensorDType] = "float32") -> str:
        """Create input tensor"""
        if isinstance(dtype, str):
            dtype = TensorDType(dtype)
        
        tensor = Tensor(
            name=name,
            shape=shape,
            dtype=dtype
        )
        
        self.graph.add_tensor(tensor)
        self.graph.add_input(name)
        return name
    
    def create_constant(self,
                        name: str,
                        value: Any,
                        shape: List[int],
                        dtype: Union[str, TensorDType] = "float32") -> str:
        """Create constant tensor"""
        if isinstance(dtype, str):
            dtype = TensorDType(dtype)
        
        tensor = Tensor(
            name=name,
            shape=shape,
            dtype=dtype
        )
        
        op = Operation(
            name=self._new_op_name("constant"),
            node_type=NodeType.CONSTANT,
            inputs=[],
            outputs=[name],
            attributes={
                "value": value,
                "shape": shape,
                "dtype": dtype.value
            }
        )
        
        self.graph.add_tensor(tensor)
        self.graph.add_operation(op)
        return name
    
    def create_embedding(self,
                        name: str,
                        input: str,
                        vocab_size: int,
                        embedding_dim: int) -> str:
        """Create embedding operation"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.EMBEDDING,
            inputs=[input],
            outputs=[output],
            attributes={
                "vocab_size": vocab_size,
                "embedding_dim": embedding_dim
            }
        )
        
        self.graph.add_operation(op)
        # Record tensor shape/dtype for downstream semantic validation
        self.graph.tensors[output].shape = [ -1, -1, embedding_dim ]
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_linear(self,
                     name: str,
                     input: str,
                     in_features: int,
                     out_features: int,
                     use_bias: bool = True,
                     tie_weight: Optional[str] = None) -> str:
        """Create linear (fully connected) layer"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.LINEAR,
            inputs=[input],
            outputs=[output],
            attributes={
                "in_features": in_features,
                "out_features": out_features,
                "use_bias": use_bias,
                "tie_weight": tie_weight
            }
        )
        
        self.graph.add_operation(op)
        # Assume last dimension equals out_features; batch/sequence preserved
        input_shape = self.graph.tensors.get(input, Tensor("", [], TensorDType.FLOAT32)).shape
        prefix = input_shape[:-1] if input_shape else [-1]
        self.graph.tensors[output].shape = prefix + [out_features]
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_rmsnorm(self,
                      name: str,
                      input: str,
                      dim: int) -> str:
        """Create RMSNorm operation"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.RMSNORM,
            inputs=[input],
            outputs=[output],
            attributes={
                "normalized_shape": dim,
                "eps": 1e-6
            }
        )
        
        self.graph.add_operation(op)
        # RMSNorm preserves shape
        self.graph.tensors[output].shape = list(self.graph.tensors.get(input, Tensor("", [], TensorDType.FLOAT32)).shape)
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_layernorm(self,
                        name: str,
                        input: str,
                        dim: int,
                        eps: float = 1e-5) -> str:
        """Create LayerNorm operation"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.LAYERNORM,
            inputs=[input],
            outputs=[output],
            attributes={
                "normalized_shape": dim,
                "eps": eps
            }
        )
        
        self.graph.add_operation(op)
        self.graph.tensors[output].shape = list(self.graph.tensors.get(input, Tensor("", [], TensorDType.FLOAT32)).shape)
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_rope(self,
                   name: str,
                   input: str,
                   dim: int,
                   theta: float = 10000.0,
                   scaling_factor: Optional[float] = None) -> str:
        """Create RoPE (Rotary Positional Embedding)"""
        output = self._new_name(f"{name}_out")
        
        attrs = {
            "dim": dim,
            "theta": theta
        }
        if scaling_factor is not None:
            attrs["scaling_factor"] = scaling_factor
        
        op = Operation(
            name=name,
            node_type=NodeType.ROPE,
            inputs=[input],
            outputs=[output],
            attributes=attrs
        )
        
        self.graph.add_operation(op)
        return output
    
    def create_alibi(self,
                    name: str,
                    num_heads: int,
                    max_bias: float = 8.0) -> str:
        """Create ALiBi (Attention with Linear Biases)"""
        output = self._new_name(f"{name}_bias")
        
        op = Operation(
            name=name,
            node_type=NodeType.ALIBI,
            inputs=[],
            outputs=[output],
            attributes={
                "num_heads": num_heads,
                "max_bias": max_bias
            }
        )
        
        self.graph.add_operation(op)
        # RoPE does not change shape
        self.graph.tensors[output].shape = list(self.graph.tensors.get(input, Tensor("", [], TensorDType.FLOAT32)).shape)
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_multi_head_attention(self,
                                   name: str,
                                   query: str,
                                   key: str,
                                   value: str,
                                   num_heads: int,
                                   num_kv_heads: int,
                                   head_dim: int,
                                   attention_type: str = "gqa",
                                   use_alibi: bool = False) -> str:
        """Create multi-head attention operation"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.MULTI_HEAD_ATTENTION,
            inputs=[query, key, value],
            outputs=[output],
            attributes={
                "num_heads": num_heads,
                "num_kv_heads": num_kv_heads,
                "head_dim": head_dim,
                "attention_type": attention_type,
                "use_alibi": use_alibi
            }
        )
        
        self.graph.add_operation(op)
        # Output shape matches query with hidden dim = num_heads * head_dim
        q_shape = self.graph.tensors.get(query, Tensor("", [], TensorDType.FLOAT32)).shape
        prefix = q_shape[:-1] if q_shape else [-1, -1]
        self.graph.tensors[output].shape = prefix + [num_heads * head_dim]
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_swiglu(self,
                     name: str,
                     gate: str,
                     up: str) -> str:
        """Create SwiGLU activation"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.SWIGLU,
            inputs=[gate, up],
            outputs=[output],
            attributes={}
        )
        
        self.graph.add_operation(op)
        # Output shape follows gate/up input shape
        self.graph.tensors[output].shape = list(self.graph.tensors.get(gate, Tensor("", [], TensorDType.FLOAT32)).shape)
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_activation(self,
                         name: str,
                         input: str,
                         activation_type: str) -> str:
        """Create activation function"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.ACTIVATION,
            inputs=[input],
            outputs=[output],
            attributes={
                "activation": activation_type
            }
        )
        
        self.graph.add_operation(op)
        self.graph.tensors[output].shape = list(self.graph.tensors.get(input, Tensor("", [], TensorDType.FLOAT32)).shape)
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_add(self,
                  name: str,
                  a: str,
                  b: str) -> str:
        """Create element-wise addition"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.ADD,
            inputs=[a, b],
            outputs=[output],
            attributes={}
        )
        
        self.graph.add_operation(op)
        # Addition preserves shape (assume broadcast-compatible)
        self.graph.tensors[output].shape = list(self.graph.tensors.get(a, Tensor("", [], TensorDType.FLOAT32)).shape)
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_mul(self,
                  name: str,
                  a: str,
                  b: str) -> str:
        """Create element-wise multiplication"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.MUL,
            inputs=[a, b],
            outputs=[output],
            attributes={}
        )
        
        self.graph.add_operation(op)
        self.graph.tensors[output].shape = list(self.graph.tensors.get(a, Tensor("", [], TensorDType.FLOAT32)).shape)
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_matmul(self,
                     name: str,
                     a: str,
                     b: str,
                     transpose_a: bool = False,
                     transpose_b: bool = False) -> str:
        """Create matrix multiplication"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.MATMUL,
            inputs=[a, b],
            outputs=[output],
            attributes={
                "transpose_a": transpose_a,
                "transpose_b": transpose_b
            }
        )
        
        self.graph.add_operation(op)
        self.graph.tensors[output].shape = []
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_softmax(self,
                      name: str,
                      input: str,
                      dim: int = -1) -> str:
        """Create softmax operation"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.SOFTMAX,
            inputs=[input],
            outputs=[output],
            attributes={
                "dim": dim
            }
        )
        
        self.graph.add_operation(op)
        self.graph.tensors[output].shape = list(self.graph.tensors.get(input, Tensor("", [], TensorDType.FLOAT32)).shape)
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_dropout(self,
                      name: str,
                      input: str,
                      rate: float = 0.1) -> str:
        """Create dropout operation"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.DROPOUT,
            inputs=[input],
            outputs=[output],
            attributes={
                "rate": rate
            }
        )
        
        self.graph.add_operation(op)
        self.graph.tensors[output].shape = list(self.graph.tensors.get(input, Tensor("", [], TensorDType.FLOAT32)).shape)
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def set_output(self, tensor: str, name: str = None):
        """Mark tensor as graph output"""
        self.graph.add_output(tensor)
    
    def get_graph(self) -> IRGraph:
        """Get built graph"""
        return self.graph

ðŸ“„ ./llm_compiler/__init__.py
=============================
"""
LLM Architecture Compiler
=========================

A deterministic, explicit system for defining, generating, and emitting
Large Language Models from declarative specifications.

Philosophy:
- Architecture-first, not checkpoint-first
- Templates, not ad-hoc classes
- Constraint solving instead of guessing
- Compiler mindset, not framework mindset
- Everything reproducible from spec alone

Key Components:
1. Templates: Canonical architecture definitions
2. Specification: Single declarative model spec
3. Solver: Architecture constraint solver
4. IR: Framework-agnostic intermediate representation
5. Emitters: Backend-specific code generation
"""

__version__ = "1.0.0"
__all__ = ['LLM', 'compile_spec', 'validate_spec', 'list_templates']

from .spec import LLM
from .compile import compile_spec
from .templates.registry import list_templates
from .utils.validation import validate_spec
ðŸ“„ ./llm_compiler/spec.py
=============================
"""
Specification Layer
===================

Defines the declarative interface for specifying LLM architectures.
All model definitions must be complete and explicit - no magic defaults.
"""

from dataclasses import dataclass, field
from typing import Dict, Any, Optional, Union, List
from enum import Enum

class NormType(Enum):
    """Normalization types"""
    LAYERNORM = "layernorm"
    RMSNORM = "rmsnorm"
    SCALENORM = "scalenorm"
    NO_NORM = "no_norm"

class ActivationType(Enum):
    """Activation functions"""
    GELU = "gelu"
    RELU = "relu"
    SILU = "silu"
    SWIGLU = "swiglu"
    GELLU = "gelu"
    RELU_SQUARED = "relu_squared"

class AttentionType(Enum):
    """Attention variants"""
    MHA = "mha"  # Multi-head attention
    MQA = "mqa"  # Multi-query attention
    GQA = "gqa"  # Grouped-query attention
    FLASH = "flash"  # Flash attention compatible
    SLIDING_WINDOW = "sliding_window"
    GLOBAL_LOCAL = "global_local"

class PositionalEncodingType(Enum):
    """Positional encoding methods"""
    ROPE = "rope"
    ALIBI = "alibi"
    SINUSOIDAL = "sinusoidal"
    RELATIVE = "relative"
    NONE = "none"

class TokenizerType(Enum):
    """Tokenizer types"""
    UNIGRAM = "unigram"
    BPE = "bpe"
    SENTENCEPIECE = "sentencepiece"
    WORDPIECE = "wordpiece"
    CHAR = "char"

class WeightFormat(Enum):
    """Weight storage formats"""
    SAFETENSORS = "safetensors"
    PYTORCH = "pytorch"
    NUMPY = "numpy"
    GGUF = "gguf"

class BackendTarget(Enum):
    """Backend compilation targets"""
    PYTORCH_TRAINING = "pytorch_training"
    PYTORCH_INFERENCE = "pytorch_inference"
    TORCHSCRIPT = "torchscript"
    ONNX = "onnx"
    JAX = "jax"

@dataclass
class LLM:
    """
    Complete LLM specification.
    
    All fields must be explicitly set - no automatic defaults.
    The system will validate and solve for missing dimensions.
    """
    # Template selection
    template: str
    
    # Size specification (choose one)
    target_params: Optional[int] = None
    explicit_dims: Optional[Dict[str, int]] = None
    
    # Core dimensions
    vocab_size: int
    context_length: int
    
    # Architecture choices
    attention: AttentionType = AttentionType.GQA
    norm: NormType = NormType.RMSNORM
    activation: ActivationType = ActivationType.SWIGLU
    positional_encoding: PositionalEncodingType = PositionalEncodingType.ROPE
    
    # Tokenizer
    tokenizer: TokenizerType = TokenizerType.UNIGRAM
    
    # Output format
    weight_format: WeightFormat = WeightFormat.SAFETENSORS
    backend: BackendTarget = BackendTarget.PYTORCH_TRAINING
    
    # Optional advanced settings
    rope_theta: float = 10000.0
    rope_scaling_factor: Optional[float] = None
    alibi_max_bias: float = 8.0
    sliding_window_size: Optional[int] = None
    tie_word_embeddings: bool = True
    gradient_checkpointing: bool = False
    precision: str = "float32"
    
    # Quantization (optional)
    quantize: Optional[str] = None
    quantize_bits: int = 16
    
    # Validation flag
    _validated: bool = field(default=False, init=False, repr=False)
    
    def __post_init__(self):
        """Validate basic constraints after initialization"""
        if self.target_params is None and self.explicit_dims is None:
            raise ValueError("Must specify either target_params or explicit_dims")
        
        if self.target_params is not None and self.explicit_dims is not None:
            raise ValueError("Cannot specify both target_params and explicit_dims")
        
        if self.vocab_size <= 0:
            raise ValueError(f"vocab_size must be positive, got {self.vocab_size}")
        
        if self.context_length <= 0:
            raise ValueError(f"context_length must be positive, got {self.context_length}")
        
        # Validate attention window if specified
        if (self.attention == AttentionType.SLIDING_WINDOW and 
            self.sliding_window_size is None):
            raise ValueError("sliding_window_size must be specified for sliding window attention")
        
        if (self.sliding_window_size is not None and 
            self.sliding_window_size > self.context_length):
            raise ValueError(f"sliding_window_size ({self.sliding_window_size}) "
                           f"exceeds context_length ({self.context_length})")
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert spec to dictionary for serialization"""
        data = {}
        for field in self.__dataclass_fields__.values():
            value = getattr(self, field.name)
            if isinstance(value, Enum):
                data[field.name] = value.value
            else:
                data[field.name] = value
        return data
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'LLM':
        """Create spec from dictionary"""
        # Convert string enums back to Enum instances
        for field_name, field_type in cls.__dataclass_fields__.items():
            if field_name in data and hasattr(field_type.type, '__origin__'):
                if field_type.type.__origin__ == Union:
                    # Handle Optional types
                    for possible_type in field_type.type.__args__:
                        if hasattr(possible_type, '_member_names_'):
                            # This is an Enum type
                            if isinstance(data[field_name], str):
                                data[field_name] = possible_type(data[field_name])
                            break
                elif hasattr(field_type.type, '_member_names_'):
                    # This is an Enum type
                    if isinstance(data[field_name], str):
                        data[field_name] = field_type.type(data[field_name])
        
        return cls(**data)
ðŸ“„ ./llm_compiler/utils/math_utils.py
=============================
"""
Math Utilities
==============

Mathematical utilities for constraint solving and dimension calculations.
"""

import math
from typing import List, Tuple, Optional

def round_to_multiple(x: int, multiple: int) -> int:
    """Round integer to nearest multiple"""
    return ((x + multiple // 2) // multiple) * multiple

def find_divisors(n: int, min_val: int = 1, max_val: int = None) -> List[int]:
    """Find divisors of n within range"""
    divisors = []
    for i in range(1, int(math.sqrt(n)) + 1):
        if n % i == 0:
            if min_val <= i <= (max_val or n):
                divisors.append(i)
            if i != n // i and min_val <= n // i <= (max_val or n):
                divisors.append(n // i)
    return sorted(divisors)

def find_closest_divisor(n: int, target: int) -> int:
    """Find divisor of n closest to target"""
    divisors = find_divisors(n)
    if not divisors:
        return 1
    return min(divisors, key=lambda x: abs(x - target))

def solve_quadratic(a: float, b: float, c: float) -> Tuple[Optional[float], Optional[float]]:
    """Solve quadratic equation"""
    discriminant = b**2 - 4*a*c
    if discriminant < 0:
        return None, None
    sqrt_disc = math.sqrt(discriminant)
    x1 = (-b + sqrt_disc) / (2*a)
    x2 = (-b - sqrt_disc) / (2*a)
    return x1, x2

def estimate_transformer_params(
    num_layers: int,
    hidden_size: int,
    intermediate_size: int,
    vocab_size: int,
    num_heads: int,
    num_kv_heads: int,
    tie_embeddings: bool = True
) -> int:
    """Estimate transformer parameters"""
    # Embeddings
    params = vocab_size * hidden_size
    
    # Per layer
    per_layer = 0
    
    # Attention
    head_dim = hidden_size // num_heads
    q_size = num_heads * head_dim
    k_size = num_kv_heads * head_dim
    v_size = num_kv_heads * head_dim
    
    per_layer += hidden_size * (q_size + k_size + v_size)  # QKV
    per_layer += (num_heads * head_dim) * hidden_size  # Output
    
    # MLP (SwiGLU)
    per_layer += 2 * hidden_size * intermediate_size  # gate/up
    per_layer += intermediate_size * hidden_size  # down
    
    # Norms (RMSNorm - scale only)
    per_layer += 2 * hidden_size
    
    # Total layers
    params += per_layer * num_layers
    
    # Output layer (if not tied)
    if not tie_embeddings:
        params += hidden_size * vocab_size
    
    return params

def solve_for_hidden_size(
    target_params: int,
    num_layers: int,
    vocab_size: int,
    intermediate_multiplier: float = 2.6875,
    num_heads: int = 32,
    num_kv_heads: int = 8,
    tie_embeddings: bool = True
) -> int:
    """
    Solve for hidden size given target parameters.
    
    Based on transformer parameter formula:
    params = vocab * hidden + num_layers * (
        hidden * (hidden * (3 + 1)) +  # QKV + output
        2 * hidden * intermediate +    # MLP gate/up
        intermediate * hidden +        # MLP down
        2 * hidden                     # Norms
    )
    + (not tied) * hidden * vocab
    """
    # Intermediate size
    def intermediate(hidden):
        return int(round(hidden * intermediate_multiplier / 32) * 32)
    
    # Solve quadratic equation
    # Derived from parameter formula
    head_dim = 128  # assumption
    
    # Coefficients
    a = num_layers * (
        4 +  # QKV + output
        3 * intermediate_multiplier  # MLP
    )
    
    if not tie_embeddings:
        a += vocab_size
    
    b = 2 * num_layers  # Norms
    c = -target_params
    
    # Solve
    hidden = (-b + math.sqrt(b**2 - 4*a*c)) / (2*a)
    
    return int(round(hidden))

def validate_dimensions(
    hidden_size: int,
    num_heads: int,
    num_kv_heads: int
) -> List[str]:
    """Validate dimension constraints"""
    errors = []
    
    if hidden_size % num_heads != 0:
        errors.append(f"hidden_size ({hidden_size}) not divisible by num_heads ({num_heads})")
    
    if hidden_size % num_kv_heads != 0:
        errors.append(f"hidden_size ({hidden_size}) not divisible by num_kv_heads ({num_kv_heads})")
    
    if num_heads % num_kv_heads != 0:
        errors.append(f"num_heads ({num_heads}) not divisible by num_kv_heads ({num_kv_heads})")
    
    return errors
ðŸ“„ ./llm_compiler/utils/parameters.py
=============================
"""
Parameter Accounting
====================

Single source of truth for parameter counting derived from the IR graph.
All other components (solver estimates, emitters, validators) should
cross-check against this to avoid silent divergence.
"""

from __future__ import annotations
from typing import Dict

from ..ir.graph import IRGraph, NodeType


def count_parameters_from_ir(graph: IRGraph) -> int:
    """
    Count trainable parameters directly from the IR graph.

    Currently supports the node types emitted by the decoder-only template.
    Nodes without parameters (add, activation, attention kernels, etc.)
    contribute zero.
    """
    params = 0

    for op in graph.operations.values():
        if op.node_type == NodeType.LINEAR:
            in_features = op.attributes.get("in_features")
            out_features = op.attributes.get("out_features")
            use_bias = op.attributes.get("use_bias", True)
            if in_features and out_features:
                params += in_features * out_features
                if use_bias:
                    params += out_features

        elif op.node_type == NodeType.EMBEDDING:
            vocab_size = op.attributes.get("vocab_size")
            embedding_dim = op.attributes.get("embedding_dim")
            if vocab_size and embedding_dim:
                params += vocab_size * embedding_dim

        elif op.node_type in (NodeType.LAYERNORM, NodeType.RMSNORM):
            normalized_shape = op.attributes.get("normalized_shape")
            if normalized_shape:
                # LayerNorm has weight and bias; RMSNorm has only weight
                if op.node_type == NodeType.LAYERNORM:
                    params += 2 * normalized_shape
                else:
                    params += normalized_shape

        # Multi-head attention parameters are captured by their constituent
        # linear projections in the current IR; nothing to add here.

    return params

ðŸ“„ ./llm_compiler/utils/validation.py
=============================
"""
Validation Utilities
====================

Validation functions for specifications and generated models.
"""

from typing import Dict, Any, List, Tuple
import json
from pathlib import Path

from ..spec import LLM, NormType, AttentionType, ActivationType
from ..tokenizers.base import TokenizerTrainingStats

def validate_spec(spec: LLM, tokenizer_stats: TokenizerTrainingStats | None = None) -> Tuple[bool, List[str]]:
    """
    Validate LLM specification.
    
    Returns:
        (is_valid, errors)
    """
    errors = []
    
    # Check template exists
    from ..templates.registry import registry
    try:
        registry.get(spec.template)
    except ValueError:
        errors.append(f"Unknown template: {spec.template}")
    
    # Check parameter specification
    if spec.target_params is None and spec.explicit_dims is None:
        errors.append("Must specify either target_params or explicit_dims")
    
    if spec.target_params is not None and spec.target_params <= 0:
        errors.append(f"target_params must be positive: {spec.target_params}")
    
    # Check vocab size
    if spec.vocab_size <= 0:
        errors.append(f"vocab_size must be positive: {spec.vocab_size}")
    
    # Check context length
    if spec.context_length <= 0:
        errors.append(f"context_length must be positive: {spec.context_length}")
    
    # Check attention type
    if spec.attention not in AttentionType:
        errors.append(f"Invalid attention type: {spec.attention}")
    
    # Check norm type
    if spec.norm not in NormType:
        errors.append(f"Invalid norm type: {spec.norm}")
    
    # Check activation
    if spec.activation not in ActivationType:
        errors.append(f"Invalid activation type: {spec.activation}")
    
    # Check positional encoding
    from ..spec import PositionalEncodingType
    if spec.positional_encoding not in PositionalEncodingType:
        errors.append(f"Invalid positional encoding: {spec.positional_encoding}")
    
    # Check tokenizer
    from ..spec import TokenizerType
    if spec.tokenizer not in TokenizerType:
        errors.append(f"Invalid tokenizer: {spec.tokenizer}")
    elif tokenizer_stats is not None:
        if tokenizer_stats.unique_tokens < spec.vocab_size:
            errors.append(
                f"vocab_size ({spec.vocab_size}) exceeds unique tokens in tokenizer stats "
                f"({tokenizer_stats.unique_tokens})"
            )
    
    # Check weight format
    from ..spec import WeightFormat
    if spec.weight_format not in WeightFormat:
        errors.append(f"Invalid weight format: {spec.weight_format}")
    
    # Check backend
    from ..spec import BackendTarget
    if spec.backend not in BackendTarget:
        errors.append(f"Invalid backend: {spec.backend}")
    
    # Check precision
    if spec.precision not in ['float32', 'float16', 'bfloat16']:
        errors.append(f"Invalid precision: {spec.precision}")
    
    # Check RoPE scaling
    if (spec.positional_encoding == PositionalEncodingType.ROPE and 
        spec.rope_scaling_factor is not None and
        spec.rope_scaling_factor <= 0):
        errors.append(f"rope_scaling_factor must be positive: {spec.rope_scaling_factor}")
    
    # Check sliding window
    if (spec.attention == AttentionType.SLIDING_WINDOW and
        spec.sliding_window_size is None):
        errors.append("sliding_window_size required for sliding_window attention")
    
    if (spec.sliding_window_size is not None and
        spec.sliding_window_size > spec.context_length):
        errors.append(f"sliding_window_size ({spec.sliding_window_size}) "
                     f"exceeds context_length ({spec.context_length})")
    
    return len(errors) == 0, errors

def validate_generated_model(model_dir: Path) -> Tuple[bool, List[str]]:
    """
    Validate generated model files.
    
    Args:
        model_dir: Directory containing generated model
        
    Returns:
        (is_valid, errors)
    """
    errors = []
    
    # Check required files
    required_files = [
        "spec.json",
        "solution.json",
        "compilation_report.json",
        "model/__init__.py",
        "model/config.py",
    ]
    
    for file in required_files:
        if not (model_dir / file).exists():
            errors.append(f"Missing required file: {file}")
    
    # Check spec and solution consistency
    try:
        spec_path = model_dir / "spec.json"
        solution_path = model_dir / "solution.json"
        ir_path = model_dir / "ir_graph.json"
        
        spec = json.loads(spec_path.read_text())
        solution = json.loads(solution_path.read_text())
        
        # Check parameter count matches
        if 'target_params' in spec:
            target = spec['target_params']
            actual = solution.get('parameters', 0)
            
            if abs(actual - target) / target > 0.01:  # 1% tolerance
                errors.append(f"Parameter mismatch: target={target:,}, actual={actual:,}")

        # Cross-validate against IR parameter count if available
        if ir_path.exists():
            from ..ir.graph import IRGraph
            from ..utils.parameters import count_parameters_from_ir
            ir_graph = IRGraph.from_json(ir_path.read_text())
            ir_params = count_parameters_from_ir(ir_graph)
            if 'parameters' in solution and solution['parameters'] != ir_params:
                errors.append(
                    f"IR parameter count {ir_params:,} disagrees with solution {solution['parameters']:,}"
                )
    
    except Exception as e:
        errors.append(f"Error reading spec/solution: {e}")
    
    # Check model code compiles
    try:
        model_files = list((model_dir / "model").glob("*.py"))
        if not model_files:
            errors.append("No Python model files found")
    except:
        errors.append("Model directory not found")
    
    return len(errors) == 0, errors

def validate_ir_graph(graph_file: Path) -> Tuple[bool, List[str]]:
    """
    Validate IR graph file.
    
    Args:
        graph_file: IR graph JSON file
        
    Returns:
        (is_valid, errors)
    """
    errors = []
    
    try:
        from ..ir.graph import IRGraph
        graph = IRGraph.from_json(graph_file.read_text())
        
        # Run graph validation
        graph_errors = graph.validate()
        errors.extend(graph_errors)
        
        # Check for cycles (simplified)
        # In full implementation would do topological sort
        
    except Exception as e:
        errors.append(f"Error loading IR graph: {e}")
    
    return len(errors) == 0, errors

ðŸ“„ ./llm_compiler/solver/constraints.py
=============================
"""
Constraint System
================

Implements constraint solving for architecture dimensions.
Handles equality, inequality, divisibility, and range constraints.
"""

from __future__ import annotations
import math
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass
from enum import Enum
import sympy as sp
from sympy import symbols, Eq, Le, Ge, Mod, solve

class ConstraintType(Enum):
    EQUALITY = "equality"
    INEQUALITY = "inequality"
    DIVISIBILITY = "divisibility"
    RANGE = "range"
    LINEAR = "linear"

@dataclass
class Constraint:
    """Base constraint class"""
    name: str
    constraint_type: ConstraintType
    
    def to_sympy(self) -> List[sp.Basic]:
        """Convert to sympy expressions"""
        raise NotImplementedError
    
    def __str__(self) -> str:
        return f"{self.name}: {self.constraint_type}"

@dataclass
class EqualityConstraint(Constraint):
    """Equality constraint: var1 == var2 or var == value"""
    var1: str
    var2: str  # Can be another variable or a constant expression
    
    def __post_init__(self):
        self.constraint_type = ConstraintType.EQUALITY
    
    def to_sympy(self) -> List[sp.Basic]:
        var1_sym = symbols(self.var1)
        
        # Try to parse var2 as expression
        try:
            # Check if var2 is a simple number
            if self.var2.replace('.', '').replace('-', '').isdigit():
                var2_val = float(self.var2) if '.' in self.var2 else int(self.var2)
                return [Eq(var1_sym, var2_val)]
            
            # Check if var2 is another variable
            if self.var2.isidentifier():
                var2_sym = symbols(self.var2)
                return [Eq(var1_sym, var2_sym)]
            
            # Try to parse as expression
            # Simple expression parser - in practice would use sympy parsing
            if '*' in self.var2:
                parts = self.var2.split('*')
                if len(parts) == 2:
                    coef, var = parts
                    coef = float(coef.strip())
                    var_sym = symbols(var.strip())
                    return [Eq(var1_sym, coef * var_sym)]
            
        except:
            pass
        
        # Default: treat as variable
        var2_sym = symbols(self.var2)
        return [Eq(var1_sym, var2_sym)]
    
    def __str__(self) -> str:
        return f"{self.name}: {self.var1} == {self.var2}"

@dataclass
class InequalityConstraint(Constraint):
    """Inequality constraint: var1 <= var2 or var1 >= var2"""
    var1: str
    var2: str
    greater_than: bool = False  # True for >=, False for <=
    
    def __post_init__(self):
        self.constraint_type = ConstraintType.INEQUALITY
    
    def to_sympy(self) -> List[sp.Basic]:
        var1_sym = symbols(self.var1)
        
        try:
            if self.var2.replace('.', '').replace('-', '').isdigit():
                var2_val = float(self.var2) if '.' in self.var2 else int(self.var2)
                if self.greater_than:
                    return [Ge(var1_sym, var2_val)]
                else:
                    return [Le(var1_sym, var2_val)]
        except:
            pass
        
        var2_sym = symbols(self.var2)
        if self.greater_than:
            return [Ge(var1_sym, var2_sym)]
        else:
            return [Le(var1_sym, var2_sym)]
    
    def __str__(self) -> str:
        op = ">=" if self.greater_than else "<="
        return f"{self.name}: {self.var1} {op} {self.var2}"

@dataclass  
class DivisibilityConstraint(Constraint):
    """Divisibility constraint: var1 % var2 == 0"""
    var1: str
    var2: str
    
    def __post_init__(self):
        self.constraint_type = ConstraintType.DIVISIBILITY
    
    def to_sympy(self) -> List[sp.Basic]:
        var1_sym = symbols(self.var1)
        
        try:
            if self.var2.replace('.', '').replace('-', '').isdigit():
                var2_val = int(self.var2)
                return [Eq(Mod(var1_sym, var2_val), 0)]
        except:
            pass
        
        var2_sym = symbols(self.var2)
        return [Eq(Mod(var1_sym, var2_sym), 0)]
    
    def __str__(self) -> str:
        return f"{self.name}: {self.var1} % {self.var2} == 0"

@dataclass
class RangeConstraint(Constraint):
    """Range constraint: min <= var <= max"""
    var: str
    min_val: Optional[Union[int, float, str]] = None
    max_val: Optional[Union[int, float, str]] = None
    
    def __post_init__(self):
        self.constraint_type = ConstraintType.RANGE
    
    def to_sympy(self) -> List[sp.Basic]:
        var_sym = symbols(self.var)
        constraints = []
        
        if self.min_val is not None:
            try:
                if isinstance(self.min_val, str) and self.min_val.replace('.', '').replace('-', '').isdigit():
                    min_val = float(self.min_val) if '.' in self.min_val else int(self.min_val)
                    constraints.append(Ge(var_sym, min_val))
                elif isinstance(self.min_val, (int, float)):
                    constraints.append(Ge(var_sym, self.min_val))
                else:
                    min_sym = symbols(self.min_val)
                    constraints.append(Ge(var_sym, min_sym))
            except:
                pass
        
        if self.max_val is not None:
            try:
                if isinstance(self.max_val, str) and self.max_val.replace('.', '').replace('-', '').isdigit():
                    max_val = float(self.max_val) if '.' in self.max_val else int(self.max_val)
                    constraints.append(Le(var_sym, max_val))
                elif isinstance(self.max_val, (int, float)):
                    constraints.append(Le(var_sym, self.max_val))
                else:
                    max_sym = symbols(self.max_val)
                    constraints.append(Le(var_sym, max_sym))
            except:
                pass
        
        return constraints
    
    def __str__(self) -> str:
        parts = [self.var]
        if self.min_val is not None:
            parts.insert(0, f"{self.min_val} <=")
        if self.max_val is not None:
            parts.append(f"<= {self.max_val}")
        return f"{self.name}: {' '.join(parts)}"

@dataclass
class LinearConstraint(Constraint):
    """Linear constraint: a*x + b*y + ... == c"""
    coefficients: Dict[str, float]  # variable -> coefficient
    constant: float
    equality: bool = True  # True for ==, False for <=
    
    def __post_init__(self):
        self.constraint_type = ConstraintType.LINEAR
    
    def to_sympy(self) -> List[sp.Basic]:
        expr = 0
        for var, coeff in self.coefficients.items():
            var_sym = symbols(var)
            expr += coeff * var_sym
        
        if self.equality:
            return [Eq(expr, self.constant)]
        else:
            return [Le(expr, self.constant)]
    
    def __str__(self) -> str:
        terms = []
        for var, coeff in self.coefficients.items():
            if coeff == 1:
                terms.append(var)
            elif coeff == -1:
                terms.append(f"-{var}")
            else:
                terms.append(f"{coeff}*{var}")
        
        expr = " + ".join(terms).replace("+ -", "- ")
        op = "==" if self.equality else "<="
        return f"{self.name}: {expr} {op} {self.constant}"

class ConstraintSystem:
    """System of constraints to solve"""
    
    def __init__(self):
        self.constraints: List[Constraint] = []
        self.variables: Dict[str, Any] = {}
    
    def add_constraint(self, constraint: Constraint):
        """Add a constraint to the system"""
        self.constraints.append(constraint)
        
        # Extract variables from constraint
        if isinstance(constraint, EqualityConstraint):
            self._add_variable(constraint.var1)
            if constraint.var2.isidentifier():
                self._add_variable(constraint.var2)
        elif isinstance(constraint, InequalityConstraint):
            self._add_variable(constraint.var1)
            if constraint.var2.isidentifier():
                self._add_variable(constraint.var2)
        elif isinstance(constraint, DivisibilityConstraint):
            self._add_variable(constraint.var1)
            self._add_variable(constraint.var2)
        elif isinstance(constraint, RangeConstraint):
            self._add_variable(constraint.var)
        elif isinstance(constraint, LinearConstraint):
            for var in constraint.coefficients.keys():
                self._add_variable(var)
    
    def _add_variable(self, var: str):
        """Add variable to tracking dict"""
        if var not in self.variables:
            self.variables[var] = {
                'min': None,
                'max': None,
                'fixed': False,
                'value': None
            }
    
    def solve(self, fixed_vars: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Solve constraint system.
        
        Args:
            fixed_vars: Variables with fixed values
            
        Returns:
            Dictionary of variable values
        """
        if fixed_vars:
            for var, value in fixed_vars.items():
                self._add_variable(var)
                self.variables[var]['fixed'] = True
                self.variables[var]['value'] = value
        
        # Convert to sympy
        sympy_constraints = []
        variables_set = set()
        
        for constraint in self.constraints:
            sympy_exprs = constraint.to_sympy()
            sympy_constraints.extend(sympy_exprs)
            
            # Collect variables
            for expr in sympy_exprs:
                variables_set.update(expr.free_symbols)
        
        # Create symbol list
        variables = list(variables_set)
        
        # Try to solve
        try:
            solution = solve(sympy_constraints, variables, dict=True)
            
            if not solution:
                raise ValueError("No solution found")
            
            # Convert to simple dict
            result = {}
            for sol in solution:
                for var, value in sol.items():
                    result[str(var)] = float(value) if value.is_Float else int(value)
            
            return result
            
        except Exception as e:
            # Fall back to iterative solving
            return self._solve_iteratively()
    
    def _solve_iteratively(self) -> Dict[str, Any]:
        """Iterative constraint solving"""
        # Initialize with reasonable defaults
        solution = {}
        for var in self.variables:
            if self.variables[var]['fixed']:
                solution[var] = self.variables[var]['value']
            else:
                # Set to midpoint of bounds if available
                if (self.variables[var]['min'] is not None and 
                    self.variables[var]['max'] is not None):
                    min_val = self.variables[var]['min']
                    max_val = self.variables[var]['max']
                    solution[var] = (min_val + max_val) // 2
                else:
                    solution[var] = 1  # Default
        
        # Apply constraints iteratively
        changed = True
        max_iter = 100
        iteration = 0
        
        while changed and iteration < max_iter:
            changed = False
            iteration += 1
            
            for constraint in self.constraints:
                if isinstance(constraint, EqualityConstraint):
                    result = self._apply_equality(constraint, solution)
                    if result:
                        solution.update(result)
                        changed = True
                
                elif isinstance(constraint, DivisibilityConstraint):
                    result = self._apply_divisibility(constraint, solution)
                    if result:
                        solution.update(result)
                        changed = True
                
                elif isinstance(constraint, RangeConstraint):
                    result = self._apply_range(constraint, solution)
                    if result:
                        solution.update(result)
                        changed = True
        
        return solution
    
    def _apply_equality(self, constraint: EqualityConstraint, solution: Dict[str, Any]) -> Dict[str, Any]:
        """Apply equality constraint"""
        updates = {}
        
        # Check if we can evaluate
        var1 = constraint.var1
        var2 = constraint.var2
        
        # Try to parse var2 as number
        try:
            if var2.replace('.', '').replace('-', '').isdigit():
                value = float(var2) if '.' in var2 else int(var2)
                updates[var1] = value
                return updates
        except:
            pass
        
        # Check if it's an expression
        if '*' in var2:
            parts = var2.split('*')
            if len(parts) == 2:
                try:
                    coef = float(parts[0].strip())
                    other_var = parts[1].strip()
                    if other_var in solution:
                        updates[var1] = coef * solution[other_var]
                        return updates
                except:
                    pass
        
        # Variable equality
        if var2 in solution and var1 not in solution:
            updates[var1] = solution[var2]
        elif var1 in solution and var2 not in solution:
            updates[var2] = solution[var1]
        
        return updates
    
    def _apply_divisibility(self, constraint: DivisibilityConstraint, solution: Dict[str, Any]) -> Dict[str, Any]:
        """Apply divisibility constraint"""
        updates = {}
        var1 = constraint.var1
        var2 = constraint.var2
        
        # Try to get divisor value
        divisor = None
        try:
            if var2.replace('.', '').replace('-', '').isdigit():
                divisor = int(var2)
        except:
            if var2 in solution:
                divisor = solution[var2]
        
        if divisor is None:
            return updates
        
        # Check if var1 needs adjustment
        if var1 in solution:
            value = solution[var1]
            if value % divisor != 0:
                # Round up to next multiple
                updates[var1] = ((value + divisor - 1) // divisor) * divisor
        
        return updates
    
    def _apply_range(self, constraint: RangeConstraint, solution: Dict[str, Any]) -> Dict[str, Any]:
        """Apply range constraint"""
        updates = {}
        var = constraint.var
        
        if var not in solution:
            return updates
        
        value = solution[var]
        
        # Apply min bound
        if constraint.min_val is not None:
            try:
                if isinstance(constraint.min_val, (int, float)):
                    min_val = constraint.min_val
                elif constraint.min_val.replace('.', '').replace('-', '').isdigit():
                    min_val = float(constraint.min_val) if '.' in constraint.min_val else int(constraint.min_val)
                else:
                    min_val = solution.get(constraint.min_val, value)
                
                if value < min_val:
                    updates[var] = min_val
            except:
                pass
        
        # Apply max bound
        if constraint.max_val is not None:
            try:
                if isinstance(constraint.max_val, (int, float)):
                    max_val = constraint.max_val
                elif constraint.max_val.replace('.', '').replace('-', '').isdigit():
                    max_val = float(constraint.max_val) if '.' in constraint.max_val else int(constraint.max_val)
                else:
                    max_val = solution.get(constraint.max_val, value)
                
                if value > max_val:
                    updates[var] = max_val
            except:
                pass
        
        return updates
    
    def __str__(self) -> str:
        lines = ["Constraint System:"]
        for constraint in self.constraints:
            lines.append(f"  {constraint}")
        return "\n".join(lines)
ðŸ“„ ./llm_compiler/solver/param_solver.py
=============================
"""
Parameter Solver
===============

Solves for architecture dimensions given parameter targets and constraints.
"""

from typing import Dict, Any, List, Tuple, Optional
import math
from dataclasses import dataclass

from .constraints import ConstraintSystem
from ..templates.base import ArchitectureTemplate, TemplateParameter
from ..utils.math_utils import find_divisors, round_to_multiple

@dataclass
class Solution:
    """Complete solution with dimensions and validation"""
    dimensions: Dict[str, int]
    actual_params: int
    target_params: Optional[int]
    template_name: str
    constraints_satisfied: bool
    warnings: List[str]
    errors: List[str]

class ParameterSolver:
    """Solves for architecture dimensions"""
    
    def __init__(self):
        self.solutions_tried = 0
        self.max_solutions = 1000
    
    def solve(self, 
              template: ArchitectureTemplate,
              spec: Dict[str, Any]) -> Solution:
        """
        Solve for dimensions given specification.
        
        Args:
            template: Architecture template
            spec: User specification
            
        Returns:
            Solution with dimensions
        """
        # Create constraint system from template
        constraint_system = template.create_constraint_system(spec)
        
        # Get fixed variables from spec
        fixed_vars = self._extract_fixed_variables(spec, template)
        
        # Try to solve
        try:
            # First try symbolic solving
            dimensions = constraint_system.solve(fixed_vars)
            
            # Ensure all required dimensions are present
            dimensions = self._ensure_complete_dimensions(dimensions, template, spec)
            
            # Round to integers
            dimensions = {k: int(round(v)) for k, v in dimensions.items()}
            
            # Apply template-specific adjustments
            dimensions = self._apply_template_adjustments(dimensions, template, spec)
            
            # Calculate actual parameters
            actual_params = template.calculate_parameters(dimensions)
            
            # Validate against target
            target_params = spec.get('target_params')
            warnings = []
            errors = []
            
            if target_params is not None:
                tolerance = 0.01  # 1% tolerance
                relative_diff = abs(actual_params - target_params) / target_params
                
                if relative_diff > tolerance:
                    warnings.append(
                        f"Parameter count mismatch: "
                        f"target={target_params:,}, actual={actual_params:,} "
                        f"(diff={relative_diff:.1%})"
                    )
            
            # Check constraints
            constraints_satisfied = self._check_constraints(dimensions, constraint_system)
            
            if not constraints_satisfied:
                warnings.append("Some constraints may not be fully satisfied")
            
            return Solution(
                dimensions=dimensions,
                actual_params=actual_params,
                target_params=target_params,
                template_name=template.info.name,
                constraints_satisfied=constraints_satisfied,
                warnings=warnings,
                errors=errors
            )
            
        except Exception as e:
            # Try iterative search
            return self._solve_iterative(template, spec, constraint_system, fixed_vars)
    
    def _extract_fixed_variables(self, spec: Dict[str, Any], template: ArchitectureTemplate) -> Dict[str, Any]:
        """Extract fixed variables from spec"""
        fixed = {}
        
        # Core fixed variables
        fixed['vocab_size'] = spec['vocab_size']
        fixed['context_length'] = spec['context_length']
        
        # Any explicit dimensions
        if 'explicit_dims' in spec:
            fixed.update(spec['explicit_dims'])
        
        return fixed
    
    def _ensure_complete_dimensions(self, 
                                   dimensions: Dict[str, Any],
                                   template: ArchitectureTemplate,
                                   spec: Dict[str, Any]) -> Dict[str, Any]:
        """Ensure all required dimensions are present"""
        required = {p.value for p in template.info.required_parameters}
        
        # Add any missing required dimensions with reasonable defaults
        for param in required:
            if param not in dimensions:
                if param == 'num_layers':
                    dimensions[param] = 32  # Reasonable default
                elif param == 'hidden_size':
                    dimensions[param] = 4096  # Reasonable default
                elif param == 'intermediate_size':
                    # Default to SwiGLU expansion
                    hidden = dimensions.get('hidden_size', 4096)
                    dimensions[param] = int(2 * round(hidden * 8/3 / 32) * 32)
                elif param == 'num_heads':
                    hidden = dimensions.get('hidden_size', 4096)
                    dimensions[param] = hidden // 128  # Default head_dim=128
                elif param == 'num_kv_heads':
                    # Default to GQA with groups of 8
                    num_heads = dimensions.get('num_heads', 32)
                    dimensions[param] = max(1, num_heads // 8)
                elif param == 'head_dim':
                    hidden = dimensions.get('hidden_size', 4096)
                    num_heads = dimensions.get('num_heads', 32)
                    dimensions[param] = hidden // num_heads
        
        return dimensions
    
    def _apply_template_adjustments(self,
                                   dimensions: Dict[str, int],
                                   template: ArchitectureTemplate,
                                   spec: Dict[str, Any]) -> Dict[str, int]:
        """Apply template-specific adjustments to dimensions"""
        
        # Ensure divisibility constraints
        hidden_size = dimensions.get('hidden_size')
        num_heads = dimensions.get('num_heads')
        num_kv_heads = dimensions.get('num_kv_heads')
        
        if hidden_size and num_heads:
            if hidden_size % num_heads != 0:
                # Adjust hidden_size to be divisible
                dimensions['hidden_size'] = ((hidden_size + num_heads - 1) // num_heads) * num_heads
        
        if hidden_size and num_kv_heads:
            if hidden_size % num_kv_heads != 0:
                # Adjust hidden_size to be divisible
                dimensions['hidden_size'] = ((hidden_size + num_kv_heads - 1) // num_kv_heads) * num_kv_heads
        
        # Ensure num_heads divisible by num_kv_heads for GQA
        if num_heads and num_kv_heads:
            if spec.get('attention') == 'gqa':
                if num_heads % num_kv_heads != 0:
                    # Adjust kv_heads to be divisor
                    divisors = find_divisors(num_heads)
                    if divisors:
                        # Find closest divisor
                        closest = min(divisors, key=lambda x: abs(x - num_kv_heads))
                        dimensions['num_kv_heads'] = closest
        
        # Ensure intermediate size matches activation
        if spec.get('activation') == 'swiglu':
            hidden = dimensions.get('hidden_size', 4096)
            # SwiGLU formula: 2 * round(8/3 * hidden / 32) * 32
            intermediate = int(2 * round(hidden * 8/3 / 32) * 32)
            dimensions['intermediate_size'] = intermediate
        
        return dimensions
    
    def _check_constraints(self, dimensions: Dict[str, int], constraint_system) -> bool:
        """Check if dimensions satisfy all constraints"""
        # Simplified check - in practice would evaluate each constraint
        return True
    
    def _solve_iterative(self,
                        template: ArchitectureTemplate,
                        spec: Dict[str, Any],
                        constraint_system,
                        fixed_vars: Dict[str, Any]) -> Solution:
        """Iterative search for solution"""
        
        target_params = spec.get('target_params')
        if target_params is None:
            raise ValueError("Iterative solving requires target_params")
        
        # Define search ranges
        search_ranges = self._define_search_ranges(template, spec, fixed_vars)
        
        # Try different combinations
        best_solution = None
        best_error = float('inf')
        
        # Simplified search - in practice would be more sophisticated
        for num_layers in range(search_ranges['num_layers'][0], 
                               search_ranges['num_layers'][1] + 1, 
                               search_ranges['num_layers'][2]):
            for hidden_size in range(search_ranges['hidden_size'][0],
                                    search_ranges['hidden_size'][1] + 1,
                                    search_ranges['hidden_size'][2]):
                
                dimensions = {
                    'num_layers': num_layers,
                    'hidden_size': hidden_size,
                    **fixed_vars
                }
                
                # Complete dimensions
                dimensions = self._ensure_complete_dimensions(dimensions, template, spec)
                dimensions = self._apply_template_adjustments(dimensions, template, spec)
                
                # Calculate parameters
                actual_params = template.calculate_parameters(dimensions)
                
                # Check error
                error = abs(actual_params - target_params) / target_params
                
                if error < best_error:
                    best_error = error
                    best_solution = {
                        'dimensions': dimensions,
                        'actual_params': actual_params,
                        'error': error
                    }
                
                self.solutions_tried += 1
                if self.solutions_tried >= self.max_solutions:
                    break
        
        if best_solution is None:
            raise ValueError("No valid solution found")
        
        return Solution(
            dimensions=best_solution['dimensions'],
            actual_params=best_solution['actual_params'],
            target_params=target_params,
            template_name=template.info.name,
            constraints_satisfied=True,
            warnings=[f"Approximate solution found (error={best_error:.1%})"],
            errors=[]
        )
    
    def _define_search_ranges(self, template, spec, fixed_vars):
        """Define reasonable search ranges for iterative solving"""
        # Based on typical LLM dimensions
        ranges = {
            'num_layers': (12, 80, 4),  # min, max, step
            'hidden_size': (2048, 16384, 512),
            'num_heads': (16, 128, 8),
        }
        
        # Adjust based on target params
        target = spec.get('target_params', 7000000000)
        
        if target < 1000000000:  # < 1B
            ranges['num_layers'] = (8, 32, 4)
            ranges['hidden_size'] = (1024, 4096, 256)
        elif target < 7000000000:  # < 7B
            ranges['num_layers'] = (24, 40, 4)
            ranges['hidden_size'] = (3072, 8192, 512)
        elif target < 13000000000:  # < 13B
            ranges['num_layers'] = (36, 48, 4)
            ranges['hidden_size'] = (5120, 10240, 512)
        else:  # > 13B
            ranges['num_layers'] = (40, 80, 4)
            ranges['hidden_size'] = (8192, 16384, 512)
        
        return ranges
ðŸ“„ ./llm_compiler/compile.py
=============================
"""
Main Compilation Entry Point
============================

Orchestrates the entire compilation pipeline.
"""

from typing import Dict, Any, Optional, Tuple
import json
from pathlib import Path
import shutil
import sys

from .spec import LLM
from .templates.registry import get_template
from .solver.param_solver import ParameterSolver
from .ir.builder import GraphBuilder
from .emitters.pytorch import PyTorchEmitter
from .emitters.safetensors import SafetensorsEmitter
from .utils.validation import validate_spec
from .utils.parameters import count_parameters_from_ir

class LLMCompiler:
    """Main compiler class"""
    
    def __init__(self, verbose: bool = False):
        self.verbose = verbose
        self.solver = ParameterSolver()
        self.pt_emitter = PyTorchEmitter()
        self.st_emitter = SafetensorsEmitter()
        
    def compile(self, spec: LLM, output_dir: Path) -> Dict[str, Any]:
        """
        Compile LLM from specification.
        
        Args:
            spec: LLM specification
            output_dir: Output directory
            
        Returns:
            Compilation report
        """
        # Create output directory
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Start compilation report
        report = {
            "spec": spec.to_dict(),
            "steps": [],
            "success": False,
            "output_dir": str(output_dir),
            "files": []
        }
        
        try:
            # Step 1: Get template
            self._log("Step 1: Getting template")
            template = get_template(spec.template)
            report["steps"].append({
                "step": "template_selection",
                "template": template.info.name,
                "status": "success"
            })
            
            # Step 2: Validate specification
            self._log("Step 2: Validating specification")
            validation_errors = template.validate_spec(spec.to_dict())
            if validation_errors:
                raise ValueError(f"Specification validation failed: {validation_errors}")
            
            report["steps"].append({
                "step": "spec_validation",
                "status": "success"
            })
            
            # Step 3: Solve for dimensions
            self._log("Step 3: Solving architecture dimensions")
            spec_dict = spec.to_dict()
            solution = self.solver.solve(template, spec_dict)
            
            if solution.errors:
                raise ValueError(f"Constraint solving failed: {solution.errors}")
            
            report["solution"] = {
                "dimensions": solution.dimensions,
                "actual_params": solution.actual_params,
                "target_params": solution.target_params,
                "warnings": solution.warnings,
                "constraints_satisfied": solution.constraints_satisfied
            }
            
            report["steps"].append({
                "step": "dimension_solving",
                "status": "success",
                "parameters": solution.actual_params,
                "warnings": solution.warnings
            })
            
            # Step 4: Build IR graph
            self._log("Step 4: Building IR graph")
            builder = GraphBuilder()
            ir_graph = template.build_ir(
                solution.dimensions,
                builder,
                spec_dict
            )
            
            # Validate graph
            graph_errors = ir_graph.validate()
            if graph_errors:
                raise ValueError(f"IR graph validation failed: {graph_errors}")
            
            report["steps"].append({
                "step": "ir_generation",
                "status": "success",
                "nodes": len(ir_graph.operations),
                "tensors": len(ir_graph.tensors)
            })

            # Step 5: Canonical parameter count from IR
            self._log("Step 5: Computing IR parameter count")
            ir_param_count = count_parameters_from_ir(ir_graph)

            # Cross-validate against template estimate
            template_param_estimate = solution.actual_params
            relative_diff = 0.0
            if template_param_estimate:
                relative_diff = abs(ir_param_count - template_param_estimate) / template_param_estimate
                if relative_diff > 0.01:
                    raise ValueError(
                        f"Parameter mismatch between template ({template_param_estimate:,}) "
                        f"and IR ({ir_param_count:,})"
                    )

            # Promote IR count to canonical value
            solution.actual_params = ir_param_count
            solution.dimensions["total_params"] = ir_param_count
            report["solution"]["actual_params"] = ir_param_count
            report["solution"]["param_source"] = "ir_graph"

            report["steps"].append({
                "step": "parameter_accounting",
                "status": "success",
                "parameters_ir": ir_param_count,
                "template_estimate": template_param_estimate,
                "relative_diff": relative_diff
            })
            
            # Step 6: Emit code
            self._log("Step 6: Emitting code")
            
            # Generate model name
            model_name = self._generate_model_name(spec, solution.dimensions)
            
            # Emit PyTorch code
            pt_files = self.pt_emitter.emit(
                ir_graph,
                model_name,
                output_dir / "model"
            )
            
            # Write files
            model_dir = output_dir / "model"
            model_dir.mkdir(exist_ok=True)
            
            for filename, content in pt_files.items():
                file_path = model_dir / filename
                file_path.write_text(content)
                report["files"].append({
                    "path": str(file_path.relative_to(output_dir)),
                    "type": "code",
                    "size": len(content)
                })
            
            # Step 7: Generate weights (placeholder)
            self._log("Step 7: Generating weights structure")
            
            # Create weights directory
            weights_dir = output_dir / "weights"
            weights_dir.mkdir(exist_ok=True)
            
            # Generate weight metadata
            weight_names = self.st_emitter.generate_weight_names(
                model_name,
                solution.dimensions.get('num_layers', 32)
            )
            
            # Create weight manifest
            weight_manifest = {
                "model_name": model_name,
                "total_parameters": solution.actual_params,
                "weights": weight_names,
                "dtype": spec.precision,
                "quantization": spec.quantize
            }
            
            manifest_path = weights_dir / "manifest.json"
            manifest_path.write_text(json.dumps(weight_manifest, indent=2))
            
            report["files"].append({
                "path": str(manifest_path.relative_to(output_dir)),
                "type": "manifest",
                "size": manifest_path.stat().st_size
            })
            
            # Step 8: Generate compilation report
            self._log("Step 8: Generating final report")
            
            # Save spec
            spec_path = output_dir / "spec.json"
            spec_path.write_text(json.dumps(spec.to_dict(), indent=2))
            
            # Save solution
            solution_path = output_dir / "solution.json"
            solution_path.write_text(json.dumps({
                "dimensions": solution.dimensions,
                "parameters": solution.actual_params
            }, indent=2))
            
            # Save IR graph
            ir_path = output_dir / "ir_graph.json"
            ir_path.write_text(ir_graph.to_json())
            
            # Update report
            report.update({
                "success": True,
                "model_name": model_name,
                "model_dir": str(model_dir.relative_to(output_dir)),
                "weights_dir": str(weights_dir.relative_to(output_dir)),
                "total_files": len(report["files"]),
                "compilation_time": "N/A"  # Would track actual time
            })
            
            report["steps"].append({
                "step": "finalization",
                "status": "success"
            })
            
            # Write final report
            report_path = output_dir / "compilation_report.json"
            report_path.write_text(json.dumps(report, indent=2))
            
            self._log(f"\nCompilation successful!")
            self._log(f"Model: {model_name}")
            self._log(f"Parameters: {solution.actual_params:,}")
            self._log(f"Output directory: {output_dir}")
            
            if solution.warnings:
                self._log("\nWarnings:")
                for warning in solution.warnings:
                    self._log(f"  â€¢ {warning}")
            
            return report
            
        except Exception as e:
            report.update({
                "success": False,
                "error": str(e),
                "error_type": type(e).__name__
            })
            
            report["steps"].append({
                "step": "error",
                "status": "failed",
                "error": str(e)
            })
            
            # Write error report
            error_report = output_dir / "error_report.json"
            error_report.write_text(json.dumps(report, indent=2))
            
            self._log(f"\nCompilation failed: {e}")
            raise
    
    def _generate_model_name(self, spec: LLM, dims: Dict[str, int]) -> str:
        """Generate meaningful model name"""
        # Extract key information
        template = spec.template.replace("_", "-")
        
        # Get parameter count in billions
        param_b = dims.get('total_params', 0) / 1e9
        
        # Round to nearest decimal
        if param_b < 1:
            param_str = f"{param_b * 1000:.0f}M"
        else:
            param_str = f"{param_b:.1f}B".replace(".", "")
        
        # Architecture features
        features = []
        if spec.attention.value != "gqa":
            features.append(spec.attention.value)
        if spec.norm.value != "rmsnorm":
            features.append(spec.norm.value)
        if spec.activation.value != "swiglu":
            features.append(spec.activation.value)
        
        # Build name
        name_parts = [f"llmc-{template}", param_str]
        if features:
            name_parts.append("-".join(features[:2]))
        
        return "-".join(name_parts).lower()
    
    def _log(self, message: str):
        """Log message if verbose"""
        if self.verbose:
            print(message)

def compile_spec(spec: LLM, 
                 output_dir: Path,
                 verbose: bool = False) -> Dict[str, Any]:
    """
    Compile LLM from specification.
    
    Args:
        spec: LLM specification
        output_dir: Output directory path
        verbose: Enable verbose logging
        
    Returns:
        Compilation report
    """
    compiler = LLMCompiler(verbose=verbose)
    return compiler.compile(spec, Path(output_dir))

def main():
    """Command-line interface"""
    import argparse
    
    parser = argparse.ArgumentParser(description="LLM Architecture Compiler")
    parser.add_argument("--spec", type=str, required=True,
                       help="Specification file (JSON)")
    parser.add_argument("--output", type=str, required=True,
                       help="Output directory")
    parser.add_argument("--verbose", action="store_true",
                       help="Verbose output")
    
    args = parser.parse_args()
    
    # Load spec
    spec_path = Path(args.spec)
    if not spec_path.exists():
        print(f"Error: Spec file not found: {spec_path}")
        sys.exit(1)
    
    spec_data = json.loads(spec_path.read_text())
    spec = LLM.from_dict(spec_data)
    
    # Compile
    try:
        report = compile_spec(spec, args.output, args.verbose)
        
        if report["success"]:
            print(f"\nâœ… Compilation successful!")
            print(f"ðŸ“ Output: {report['output_dir']}")
            print(f"ðŸ¤– Model: {report['model_name']}")
            print(f"ðŸ§® Parameters: {report['solution']['actual_params']:,}")
            sys.exit(0)
        else:
            print(f"\nâŒ Compilation failed: {report.get('error')}")
            sys.exit(1)
            
    except Exception as e:
        print(f"\nâŒ Compilation error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()

ðŸ“„ ./llm_compiler/templates/encoder_decoder.py
=============================
"""
Encoder-Decoder Transformer Template
====================================

Implements T5/BART-style encoder-decoder architectures.
"""

from typing import Dict, Any, List
from dataclasses import dataclass

from .base import ArchitectureTemplate, TemplateInfo, TemplateParameter
from ..ir.graph import IRGraph
from ..ir.builder import GraphBuilder
from ..solver.constraints import (
    ConstraintSystem, EqualityConstraint, 
    DivisibilityConstraint, RangeConstraint
)

class EncoderDecoderTemplate(ArchitectureTemplate):
    """Encoder-decoder transformer (T5/BART style)"""
    
    def __init__(self, version: str = "v1"):
        self.version = version
        
    @property
    def info(self) -> TemplateInfo:
        return TemplateInfo(
            name=f"encoder_decoder_{self.version}",
            description="Encoder-decoder transformer (T5/BART style)",
            version=self.version,
            parameters=[
                TemplateParameter.NUM_LAYERS,
                TemplateParameter.HIDDEN_SIZE,
                TemplateParameter.INTERMEDIATE_SIZE,
                TemplateParameter.NUM_HEADS,
                TemplateParameter.NUM_KV_HEADS,
                TemplateParameter.HEAD_DIM,
                TemplateParameter.VOCAB_SIZE,
                TemplateParameter.CONTEXT_LENGTH,
                "num_encoder_layers",
                "num_decoder_layers",
            ],
            required_parameters={
                TemplateParameter.VOCAB_SIZE,
                TemplateParameter.CONTEXT_LENGTH,
            },
            optional_parameters={
                TemplateParameter.NUM_LAYERS,
                TemplateParameter.HIDDEN_SIZE,
                TemplateParameter.INTERMEDIATE_SIZE,
                TemplateParameter.NUM_HEADS,
                TemplateParameter.NUM_KV_HEADS,
                TemplateParameter.HEAD_DIM,
                "num_encoder_layers",
                "num_decoder_layers",
            },
            default_constraints=[
                # Shared constraints
                DivisibilityConstraint(
                    var1=TemplateParameter.HIDDEN_SIZE.value,
                    var2=TemplateParameter.NUM_HEADS.value,
                    name="hidden_size_divisible_by_num_heads"
                ),
                # Encoder-decoder specific
                EqualityConstraint(
                    var1="num_layers",
                    var2="num_encoder_layers + num_decoder_layers",
                    name="total_layers_sum"
                ),
            ]
        )
    
    def create_constraint_system(self, spec: Dict[str, Any]) -> ConstraintSystem:
        """Create constraint system for encoder-decoder architecture"""
        system = ConstraintSystem()
        
        # Add default constraints
        for constraint in self.info.default_constraints:
            system.add_constraint(constraint)
        
        # Encoder-decoder typically uses same hidden size throughout
        system.add_constraint(EqualityConstraint(
            var1="encoder_hidden_size",
            var2=TemplateParameter.HIDDEN_SIZE.value,
            name="encoder_hidden_size_equals_main"
        ))
        
        system.add_constraint(EqualityConstraint(
            var1="decoder_hidden_size",
            var2=TemplateParameter.HIDDEN_SIZE.value,
            name="decoder_hidden_size_equals_main"
        ))
        
        # Add user constraints
        if 'explicit_dims' in spec:
            for param, value in spec['explicit_dims'].items():
                system.add_constraint(EqualityConstraint(
                    var1=param,
                    var2=str(value),
                    name=f"user_{param}"
                ))
        
        if 'target_params' in spec:
            system.add_constraint(EqualityConstraint(
                var1="total_params",
                var2=str(spec['target_params']),
                name="target_parameter_count"
            ))
        
        return system
    
    def calculate_parameters(self, dims: Dict[str, int]) -> int:
        """Calculate parameters for encoder-decoder transformer"""
        # Similar to decoder-only but with encoder and cross-attention
        hidden_size = dims[TemplateParameter.HIDDEN_SIZE.value]
        intermediate_size = dims[TemplateParameter.INTERMEDIATE_SIZE.value]
        vocab_size = dims[TemplateParameter.VOCAB_SIZE.value]
        num_heads = dims[TemplateParameter.NUM_HEADS.value]
        head_dim = dims.get(TemplateParameter.HEAD_DIM.value, hidden_size // num_heads)
        
        # Get layer counts
        num_encoder_layers = dims.get('num_encoder_layers', dims.get('num_layers', 12) // 2)
        num_decoder_layers = dims.get('num_decoder_layers', dims.get('num_layers', 12) // 2)
        
        # Encoder parameters (similar to decoder-only but without cross-attention)
        encoder_per_layer = self._calculate_encoder_layer_params(
            hidden_size, intermediate_size, num_heads, head_dim
        )
        
        # Decoder parameters (with cross-attention)
        decoder_per_layer = self._calculate_decoder_layer_params(
            hidden_size, intermediate_size, num_heads, head_dim
        )
        
        # Total
        total = (encoder_per_layer * num_encoder_layers +
                 decoder_per_layer * num_decoder_layers)
        
        # Embeddings (shared typically)
        total += vocab_size * hidden_size
        
        return total
    
    def _calculate_encoder_layer_params(self, hidden_size, intermediate_size, num_heads, head_dim):
        """Calculate parameters for one encoder layer"""
        params = 0
        
        # Self-attention
        params += 4 * hidden_size * hidden_size  # QKV + output
        params += hidden_size  # RMSNorm
        
        # MLP
        params += 2 * hidden_size * intermediate_size  # gate/up
        params += intermediate_size * hidden_size  # down
        params += hidden_size  # RMSNorm
        
        return params
    
    def _calculate_decoder_layer_params(self, hidden_size, intermediate_size, num_heads, head_dim):
        """Calculate parameters for one decoder layer"""
        params = self._calculate_encoder_layer_params(hidden_size, intermediate_size, num_heads, head_dim)
        
        # Add cross-attention
        params += 4 * hidden_size * hidden_size  # Cross-attention QKV + output
        params += hidden_size  # Additional RMSNorm for cross-attention
        
        return params
    
    def build_ir(self, dims: Dict[str, int], builder: GraphBuilder, spec: Dict[str, Any]) -> IRGraph:
        """Build encoder-decoder IR graph"""
        # Implementation similar to decoder-only but with encoder/decoder separation
        # This would be quite long - showing structure
        graph = IRGraph(name=f"encoder_decoder_{self.version}")
        
        # Build encoder
        # Build decoder with cross-attention
        # Connect them
        
        return graph
    
    def validate_spec(self, spec: Dict[str, Any]) -> List[str]:
        """Validate encoder-decoder spec"""
        errors = []
        
        if 'vocab_size' not in spec:
            errors.append("vocab_size is required")
        if 'context_length' not in spec:
            errors.append("context_length is required")
        
        return errors
ðŸ“„ ./llm_compiler/templates/decoder_only.py
=============================
"""
Decoder-Only Transformer Template
==================================

Implements GPT/LLaMA-style decoder-only architectures.
Supports MHA, MQA, GQA attention variants.
"""

import math
from typing import Dict, Any, List, Tuple, Optional
from dataclasses import dataclass

from .base import ArchitectureTemplate, TemplateInfo, TemplateParameter
from ..ir.graph import IRGraph, Tensor, Operation
from ..ir.builder import GraphBuilder
from ..ir.node import NodeType, Node
from ..solver.constraints import (
    Constraint, ConstraintSystem, EqualityConstraint, 
    DivisibilityConstraint, RangeConstraint, LinearConstraint
)
from ..utils.math_utils import round_to_multiple, find_divisors

class DecoderOnlyTemplate(ArchitectureTemplate):
    """Decoder-only transformer (GPT/LLaMA class)"""
    
    def __init__(self, version: str = "v1"):
        self.version = version
        
    @property
    def info(self) -> TemplateInfo:
        return TemplateInfo(
            name=f"decoder_only_{self.version}",
            description="Decoder-only transformer (GPT/LLaMA style)",
            version=self.version,
            parameters=[
                TemplateParameter.NUM_LAYERS,
                TemplateParameter.HIDDEN_SIZE,
                TemplateParameter.INTERMEDIATE_SIZE,
                TemplateParameter.NUM_HEADS,
                TemplateParameter.NUM_KV_HEADS,
                TemplateParameter.HEAD_DIM,
                TemplateParameter.VOCAB_SIZE,
                TemplateParameter.CONTEXT_LENGTH,
            ],
            required_parameters={
                TemplateParameter.VOCAB_SIZE,
                TemplateParameter.CONTEXT_LENGTH,
            },
            optional_parameters={
                TemplateParameter.NUM_LAYERS,
                TemplateParameter.HIDDEN_SIZE,
                TemplateParameter.INTERMEDIATE_SIZE,
                TemplateParameter.NUM_HEADS,
                TemplateParameter.NUM_KV_HEADS,
                TemplateParameter.HEAD_DIM,
            },
            default_constraints=[
                # Head dimension constraints
                DivisibilityConstraint(
                    var1=TemplateParameter.HIDDEN_SIZE.value,
                    var2=TemplateParameter.NUM_HEADS.value,
                    name="hidden_size_divisible_by_num_heads"
                ),
                DivisibilityConstraint(
                    var1=TemplateParameter.HIDDEN_SIZE.value,
                    var2=TemplateParameter.NUM_KV_HEADS.value,
                    name="hidden_size_divisible_by_num_kv_heads"
                ),
                # KV heads must divide query heads for GQA
                DivisibilityConstraint(
                    var1=TemplateParameter.NUM_HEADS.value,
                    var2=TemplateParameter.NUM_KV_HEADS.value,
                    name="num_heads_divisible_by_num_kv_heads"
                ),
                # Intermediate size typical expansion
                EqualityConstraint(
                    var1=TemplateParameter.INTERMEDIATE_SIZE.value,
                    var2=f"2.6875 * {TemplateParameter.HIDDEN_SIZE.value}",
                    name="swiglu_intermediate_size"
                ),
            ]
        )
    
    def create_constraint_system(self, spec: Dict[str, Any]) -> ConstraintSystem:
        """
        Create constraint system for decoder-only architecture.
        
        Handles attention type-specific constraints.
        """
        system = ConstraintSystem()
        
        # Add default constraints
        for constraint in self.info.default_constraints:
            system.add_constraint(constraint)
        
        # Add attention-specific constraints
        attention_type = spec.get('attention', 'gqa')
        
        if attention_type == 'mha':
            # Multi-head: num_kv_heads == num_heads
            system.add_constraint(EqualityConstraint(
                var1=TemplateParameter.NUM_KV_HEADS.value,
                var2=TemplateParameter.NUM_HEADS.value,
                name="mha_kv_equals_q"
            ))
        elif attention_type == 'mqa':
            # Multi-query: num_kv_heads == 1
            system.add_constraint(EqualityConstraint(
                var1=TemplateParameter.NUM_KV_HEADS.value,
                var2="1",
                name="mqa_single_kv_head"
            ))
        elif attention_type == 'gqa':
            # Grouped-query: already handled by divisibility constraint
            pass
        
        # Add user-specified constraints if any
        if 'explicit_dims' in spec:
            for param, value in spec['explicit_dims'].items():
                system.add_constraint(EqualityConstraint(
                    var1=param,
                    var2=str(value),
                    name=f"user_{param}"
                ))
        
        # Add parameter count constraint if target specified
        if 'target_params' in spec:
            system.add_constraint(EqualityConstraint(
                var1="total_params",
                var2=str(spec['target_params']),
                name="target_parameter_count"
            ))
        
        # Add activation-specific constraints
        if spec.get('activation') == 'swiglu':
            # SwiGLU intermediate size formula
            system.add_constraint(EqualityConstraint(
                var1=TemplateParameter.INTERMEDIATE_SIZE.value,
                var2=f"2 * round({TemplateParameter.HIDDEN_SIZE.value} * 8/3 / 32) * 32",
                name="swiglu_exact_intermediate"
            ))
        
        return system
    
    def calculate_parameters(self, dims: Dict[str, int]) -> int:
        """
        Calculate exact parameter count for decoder-only transformer.
        
        Formula based on LLaMA/GPT architecture:
        - Embedding: vocab_size * hidden_size
        - Output layer: hidden_size * vocab_size (if not tied)
        - Per layer:
          â€¢ Attention QKV: 3 * hidden_size * hidden_size
          â€¢ Attention output: hidden_size * hidden_size
          â€¢ Attention norm: hidden_size * 2
          â€¢ MLP gate/up: 2 * hidden_size * intermediate_size
          â€¢ MLP down: intermediate_size * hidden_size
          â€¢ MLP norm: hidden_size * 2
        """
        n_layers = dims[TemplateParameter.NUM_LAYERS.value]
        hidden_size = dims[TemplateParameter.HIDDEN_SIZE.value]
        intermediate_size = dims[TemplateParameter.INTERMEDIATE_SIZE.value]
        vocab_size = dims[TemplateParameter.VOCAB_SIZE.value]
        num_heads = dims[TemplateParameter.NUM_HEADS.value]
        num_kv_heads = dims.get(TemplateParameter.NUM_KV_HEADS.value, num_heads)
        head_dim = dims.get(TemplateParameter.HEAD_DIM.value, hidden_size // num_heads)
        
        # Calculate per-layer parameters
        per_layer = 0
        
        # Attention QKV projections
        # In grouped-query attention, KV heads are fewer
        q_size = num_heads * head_dim
        k_size = num_kv_heads * head_dim
        v_size = num_kv_heads * head_dim
        
        per_layer += hidden_size * (q_size + k_size + v_size)  # QKV weights
        per_layer += (q_size + k_size + v_size)  # QKV biases (if any)
        
        # Attention output projection
        per_layer += (num_heads * head_dim) * hidden_size  # Output weights
        per_layer += hidden_size  # Output bias (if any)
        
        # Attention normalization (RMSNorm)
        per_layer += hidden_size  # RMSNorm scale (no bias)
        
        # MLP (SwiGLU)
        # gate_proj + up_proj (both hidden_size -> intermediate_size)
        per_layer += 2 * hidden_size * intermediate_size
        per_layer += 2 * intermediate_size  # biases
        
        # down_proj (intermediate_size -> hidden_size)
        per_layer += intermediate_size * hidden_size
        per_layer += hidden_size  # bias
        
        # MLP normalization
        per_layer += hidden_size  # RMSNorm scale
        
        # Total layers
        total = per_layer * n_layers
        
        # Embeddings
        total += vocab_size * hidden_size  # token embeddings
        
        # Output layer (if not tied)
        tie_embeddings = True  # Default for decoder-only
        if not tie_embeddings:
            total += hidden_size * vocab_size
        
        return total
    
    def build_ir(self, 
                 dims: Dict[str, int],
                 builder: GraphBuilder,
                 spec: Dict[str, Any]) -> IRGraph:
        """
        Build decoder-only transformer IR graph.
        """
        # Extract dimensions
        n_layers = dims[TemplateParameter.NUM_LAYERS.value]
        hidden_size = dims[TemplateParameter.HIDDEN_SIZE.value]
        intermediate_size = dims[TemplateParameter.INTERMEDIATE_SIZE.value]
        vocab_size = dims[TemplateParameter.VOCAB_SIZE.value]
        context_length = dims[TemplateParameter.CONTEXT_LENGTH.value]
        num_heads = dims[TemplateParameter.NUM_HEADS.value]
        num_kv_heads = dims.get(TemplateParameter.NUM_KV_HEADS.value, num_heads)
        head_dim = dims.get(TemplateParameter.HEAD_DIM.value, hidden_size // num_heads)
        
        # Start building graph
        graph = IRGraph(name=f"decoder_only_{self.version}")
        
        # Input token IDs
        tokens = builder.create_input(
            name="input_ids",
            shape=[-1, context_length],  # batch size dynamic
            dtype="int32"
        )
        
        # Token embeddings
        embeddings = builder.create_embedding(
            name="token_embeddings",
            input=tokens,
            vocab_size=vocab_size,
            embedding_dim=hidden_size
        )
        
        # Positional embeddings
        if spec.get('positional_encoding') == 'rope':
            pos_emb = builder.create_rope(
                name="rope_positional",
                input=embeddings,
                dim=head_dim,
                theta=spec.get('rope_theta', 10000.0),
                scaling_factor=spec.get('rope_scaling_factor')
            )
            current = pos_emb
        elif spec.get('positional_encoding') == 'alibi':
            pos_emb = builder.create_alibi(
                name="alibi_bias",
                num_heads=num_heads,
                max_bias=spec.get('alibi_max_bias', 8.0)
            )
            current = embeddings
        else:
            current = embeddings
        
        # Initial normalization
        if spec.get('norm') == 'rmsnorm':
            current = builder.create_rmsnorm(
                name="input_norm",
                input=current,
                dim=hidden_size
            )
        
        # Build decoder layers
        for layer_idx in range(n_layers):
            layer_prefix = f"layer_{layer_idx}"
            
            # Attention residual path
            attn_norm = builder.create_rmsnorm(
                name=f"{layer_prefix}_attn_norm",
                input=current,
                dim=hidden_size
            )
            
            # Attention
            if spec.get('attention') in ['mha', 'gqa', 'mqa']:
                # QKV projections
                q_proj = builder.create_linear(
                    name=f"{layer_prefix}_q_proj",
                    input=attn_norm,
                    in_features=hidden_size,
                    out_features=num_heads * head_dim
                )
                
                k_proj = builder.create_linear(
                    name=f"{layer_prefix}_k_proj",
                    input=attn_norm,
                    in_features=hidden_size,
                    out_features=num_kv_heads * head_dim
                )
                
                v_proj = builder.create_linear(
                    name=f"{layer_prefix}_v_proj",
                    input=attn_norm,
                    in_features=hidden_size,
                    out_features=num_kv_heads * head_dim
                )
                
                # Multi-head attention
                attn_output = builder.create_multi_head_attention(
                    name=f"{layer_prefix}_attn",
                    query=q_proj,
                    key=k_proj,
                    value=v_proj,
                    num_heads=num_heads,
                    num_kv_heads=num_kv_heads,
                    head_dim=head_dim,
                    attention_type=spec.get('attention', 'gqa'),
                    use_alibi=(spec.get('positional_encoding') == 'alibi')
                )
            else:
                raise ValueError(f"Unsupported attention type: {spec.get('attention')}")
            
            # Attention output projection
            attn_out_proj = builder.create_linear(
                name=f"{layer_prefix}_attn_out_proj",
                input=attn_output,
                in_features=num_heads * head_dim,
                out_features=hidden_size
            )
            
            # First residual connection
            attn_residual = builder.create_add(
                name=f"{layer_prefix}_attn_residual",
                a=current,
                b=attn_out_proj
            )
            
            # MLP path
            mlp_norm = builder.create_rmsnorm(
                name=f"{layer_prefix}_mlp_norm",
                input=attn_residual,
                dim=hidden_size
            )
            
            # SwiGLU MLP
            if spec.get('activation') == 'swiglu':
                # Gate projection
                gate_proj = builder.create_linear(
                    name=f"{layer_prefix}_gate_proj",
                    input=mlp_norm,
                    in_features=hidden_size,
                    out_features=intermediate_size
                )
                
                # Up projection
                up_proj = builder.create_linear(
                    name=f"{layer_prefix}_up_proj",
                    input=mlp_norm,
                    in_features=hidden_size,
                    out_features=intermediate_size
                )
                
                # SwiGLU activation
                gate_act = builder.create_swiglu(
                    name=f"{layer_prefix}_swiglu",
                    gate=gate_proj,
                    up=up_proj
                )
            else:
                # Standard MLP
                fc1 = builder.create_linear(
                    name=f"{layer_prefix}_fc1",
                    input=mlp_norm,
                    in_features=hidden_size,
                    out_features=intermediate_size
                )
                
                gate_act = builder.create_activation(
                    name=f"{layer_prefix}_activation",
                    input=fc1,
                    activation_type=spec.get('activation', 'silu')
                )
            
            # Down projection
            down_proj = builder.create_linear(
                name=f"{layer_prefix}_down_proj",
                input=gate_act,
                in_features=intermediate_size,
                out_features=hidden_size
            )
            
            # Second residual connection
            current = builder.create_add(
                name=f"{layer_prefix}_mlp_residual",
                a=attn_residual,
                b=down_proj
            )
        
        # Final normalization
        if spec.get('norm') == 'rmsnorm':
            current = builder.create_rmsnorm(
                name="output_norm",
                input=current,
                dim=hidden_size
            )
        
        # Output projection (tied or separate)
        if spec.get('tie_word_embeddings', True):
            # Use embedding weights for output
            output = builder.create_linear(
                name="output_projection",
                input=current,
                in_features=hidden_size,
                out_features=vocab_size,
                use_bias=False,
                tie_weight="token_embeddings.weight"  # Reference embedding weights
            )
        else:
            # Separate output weights
            output = builder.create_linear(
                name="output_projection",
                input=current,
                in_features=hidden_size,
                out_features=vocab_size,
                use_bias=False
            )
        
        # Set as output
        builder.set_output(output, name="logits")
        
        return graph
    
    def validate_spec(self, spec: Dict[str, Any]) -> List[str]:
        """Validate specification against template requirements"""
        errors = []
        
        # Check required parameters
        if 'vocab_size' not in spec:
            errors.append("vocab_size is required")
        if 'context_length' not in spec:
            errors.append("context_length is required")
        
        # Check attention-specific requirements
        attention = spec.get('attention', 'gqa')
        if attention == 'sliding_window':
            if 'sliding_window_size' not in spec:
                errors.append("sliding_window_size required for sliding_window attention")
        
        # Check positional encoding compatibility
        pos_enc = spec.get('positional_encoding', 'rope')
        if pos_enc == 'rope':
            if attention in ['alibi', 'relative']:
                errors.append(f"rope positional encoding incompatible with {attention} attention")
        
        # Check activation function
        activation = spec.get('activation', 'swiglu')
        if activation not in ['gelu', 'relu', 'silu', 'swiglu']:
            errors.append(f"Unsupported activation: {activation}")
        
        return errors
ðŸ“„ ./llm_compiler/templates/registry.py
=============================
"""
Template Registry
================

Manages available architecture templates.
Central registry for all template implementations.
"""

from typing import Dict, Type, List
import inspect

from .base import ArchitectureTemplate
from .decoder_only import DecoderOnlyTemplate
from .encoder_decoder import EncoderDecoderTemplate

class TemplateRegistry:
    """Registry for all architecture templates"""
    
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._templates = {}
            cls._instance._initialize()
        return cls._instance
    
    def _initialize(self):
        """Initialize with built-in templates"""
        self.register(DecoderOnlyTemplate())
        self.register(DecoderOnlyTemplate(version="v2"))
        self.register(EncoderDecoderTemplate())
    
    def register(self, template: ArchitectureTemplate):
        """Register a template"""
        self._templates[template.info.name] = template
    
    def get(self, name: str) -> ArchitectureTemplate:
        """Get template by name"""
        if name not in self._templates:
            raise ValueError(f"Template not found: {name}. "
                           f"Available: {list(self._templates.keys())}")
        return self._templates[name]
    
    def list(self) -> List[str]:
        """List all registered template names"""
        return list(self._templates.keys())
    
    def list_with_info(self) -> Dict[str, dict]:
        """List templates with their metadata"""
        return {
            name: {
                'description': template.info.description,
                'version': template.info.version,
                'parameters': [p.value for p in template.info.parameters],
                'required': [p.value for p in template.info.required_parameters],
            }
            for name, template in self._templates.items()
        }

# Global registry instance
registry = TemplateRegistry()

def get_template(name: str) -> ArchitectureTemplate:
    """Get template from global registry"""
    return registry.get(name)

def list_templates() -> List[str]:
    """List all available templates"""
    return registry.list()

def list_templates_with_info() -> Dict[str, dict]:
    """List templates with metadata"""
    return registry.list_with_info()
ðŸ“„ ./llm_compiler/templates/base.py
=============================
"""
Base Template Class
===================

Defines the interface that all architecture templates must implement.
Templates are responsible for:
1. Declaring their degrees of freedom
2. Defining parameter calculation formulas
3. Generating IR graphs
4. Enforcing architecture-specific constraints
"""

from abc import ABC, abstractmethod
from typing import Dict, Any, List, Set, Tuple, Optional
from dataclasses import dataclass
from enum import Enum

from ..ir.graph import IRGraph, Tensor
from ..ir.builder import GraphBuilder
from ..solver.constraints import Constraint, ConstraintSystem

class TemplateParameter(Enum):
    """Standard template parameters"""
    NUM_LAYERS = "num_layers"
    HIDDEN_SIZE = "hidden_size"
    INTERMEDIATE_SIZE = "intermediate_size"
    NUM_HEADS = "num_heads"
    NUM_KV_HEADS = "num_kv_heads"
    HEAD_DIM = "head_dim"
    VOCAB_SIZE = "vocab_size"
    CONTEXT_LENGTH = "context_length"

@dataclass
class TemplateInfo:
    """Metadata about a template"""
    name: str
    description: str
    version: str
    parameters: List[TemplateParameter]
    required_parameters: Set[TemplateParameter]
    optional_parameters: Set[TemplateParameter]
    default_constraints: List[Constraint]

class ArchitectureTemplate(ABC):
    """
    Abstract base class for all architecture templates.
    
    Templates must be deterministic and explicit - they cannot make
    arbitrary choices on behalf of the user.
    """
    
    @property
    @abstractmethod
    def info(self) -> TemplateInfo:
        """Return template metadata"""
        pass
    
    @abstractmethod
    def create_constraint_system(self, spec: Dict[str, Any]) -> ConstraintSystem:
        """
        Create constraint system for this template.
        
        Args:
            spec: User specification with some parameters fixed
            
        Returns:
            ConstraintSystem with all architectural constraints
        """
        pass
    
    @abstractmethod
    def calculate_parameters(self, dims: Dict[str, int]) -> int:
        """
        Calculate total parameter count for given dimensions.
        
        Must match exactly what the generated model will have.
        
        Args:
            dims: Complete dimension dictionary
            
        Returns:
            Total number of trainable parameters
        """
        pass
    
    @abstractmethod
    def build_ir(self, 
                 dims: Dict[str, int],
                 builder: GraphBuilder,
                 spec: Dict[str, Any]) -> IRGraph:
        """
        Build the IR graph for this architecture.
        
        Args:
            dims: Solved dimensions
            builder: Graph builder helper
            spec: Complete specification
            
        Returns:
            Complete IR graph
        """
        pass
    
    @abstractmethod
    def validate_spec(self, spec: Dict[str, Any]) -> List[str]:
        """
        Validate specification against template requirements.
        
        Args:
            spec: User specification
            
        Returns:
            List of error messages (empty if valid)
        """
        pass
    
    def get_parameter_bounds(self) -> Dict[str, Tuple[Optional[int], Optional[int]]]:
        """
        Get reasonable bounds for each parameter.
        
        Returns:
            Dict mapping parameter name to (min, max) bounds
        """
        return {
            TemplateParameter.NUM_LAYERS.value: (1, 1000),
            TemplateParameter.HIDDEN_SIZE.value: (128, 65536),
            TemplateParameter.INTERMEDIATE_SIZE.value: (128, 262144),
            TemplateParameter.NUM_HEADS.value: (1, 256),
            TemplateParameter.NUM_KV_HEADS.value: (1, 256),
            TemplateParameter.HEAD_DIM.value: (32, 256),
            TemplateParameter.VOCAB_SIZE.value: (32, 1000000),
            TemplateParameter.CONTEXT_LENGTH.value: (1, 1000000),
        }
ðŸ“„ ./llm_compiler/emitters/pytorch.py
=============================
"""
PyTorch Emitter
===============

Emits PyTorch modules that *execute directly from the IR*.
The generated model:
- Parses the frozen IR JSON embedded in the emitted file
- Instantiates nn.Modules only for parameter-carrying IR nodes
- Executes operations in topological order without re-deciding structure
"""

from __future__ import annotations
from typing import Dict, Any, List
import json
import textwrap
from pathlib import Path

from ..ir.graph import IRGraph, NodeType


class PyTorchEmitter:
    """IR-driven PyTorch emitter."""

    def __init__(self, precision: str = "float32"):
        self.precision = precision
        self.torch_dtype_map = {
            "float32": "torch.float32",
            "float16": "torch.float16",
            "bfloat16": "torch.bfloat16",
            "int32": "torch.int32",
            "int64": "torch.int64",
        }

    # ------------------------------------------------------------------ public
    def emit(self, graph: IRGraph, model_name: str, output_dir: Path) -> Dict[str, str]:
        files: Dict[str, str] = {}

        files[f"{model_name}.py"] = self._emit_model_file(graph, model_name)
        files["config.py"] = self._emit_config(graph)
        files["__init__.py"] = self._emit_init_file(model_name)
        files["setup.py"] = self._emit_setup_file(model_name)
        files[f"test_{model_name}.py"] = self._emit_test_file(model_name)
        return files

    # ----------------------------------------------------------------- emitters
    def _emit_model_file(self, graph: IRGraph, model_name: str) -> str:
        """Emit the IR-driven PyTorch model."""
        graph_blob = graph.to_json()
        dtype_literal = self.torch_dtype_map.get(self.precision, "torch.float32")

        model_code = textwrap.dedent(
            f'''
            import json
            from typing import Dict, Any, Optional
            import torch
            import torch.nn as nn
            import torch.nn.functional as F

            from .config import ModelConfig

            IR_JSON = r"""{graph_blob}"""
            IR_DEF = json.loads(IR_JSON)


            # ------------------------------------------------------------- helpers
            def _topological_sort(ir: Dict[str, Any]):
                ops = ir["operations"]
                tensors = ir["tensors"]
                indegree = {{name: 0 for name in ops}}
                for op_name, op in ops.items():
                    for inp in op["inputs"]:
                        producer = tensors.get(inp, {{}}).get("node")
                        if producer and producer in indegree:
                            indegree[op_name] += 1
                ready = [name for name, deg in indegree.items() if deg == 0]
                order = []
                while ready:
                    current = ready.pop(0)
                    order.append(current)
                    for out in ops[current]["outputs"]:
                        for consumer in tensors.get(out, {{}}).get("consumers", []):
                            if consumer in indegree:
                                indegree[consumer] -= 1
                                if indegree[consumer] == 0:
                                    ready.append(consumer)
                if len(order) != len(ops):
                    # Fallback to insertion order if a cycle slipped in
                    return list(ops.keys())
                return order


            class RMSNorm(nn.Module):
                def __init__(self, dim: int, eps: float = 1e-6):
                    super().__init__()
                    self.eps = eps
                    self.weight = nn.Parameter(torch.ones(dim))

                def forward(self, x: torch.Tensor) -> torch.Tensor:
                    norm_x = x.pow(2).mean(-1, keepdim=True)
                    x_normed = x * torch.rsqrt(norm_x + self.eps)
                    return self.weight * x_normed


            class AttentionOp(nn.Module):
                def __init__(self, num_heads: int, num_kv_heads: int, head_dim: int, dropout: float = 0.0):
                    super().__init__()
                    self.num_heads = num_heads
                    self.num_kv_heads = num_kv_heads
                    self.head_dim = head_dim
                    self.dropout = dropout
                    self.num_heads_per_kv = max(1, num_heads // num_kv_heads)

                def forward(self, q, k, v, attention_mask=None):
                    bsz, seq_len, _ = q.shape
                    q = q.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
                    k = k.view(bsz, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)
                    v = v.view(bsz, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)

                    if self.num_kv_heads != self.num_heads:
                        k = k.repeat_interleave(self.num_heads_per_kv, dim=1)
                        v = v.repeat_interleave(self.num_heads_per_kv, dim=1)

                    attn = F.scaled_dot_product_attention(
                        q, k, v, attn_mask=attention_mask, dropout_p=self.dropout
                    )
                    attn = attn.transpose(1, 2).reshape(bsz, seq_len, self.num_heads * self.head_dim)
                    return attn


            def _apply_rope(x: torch.Tensor, theta: float = 10000.0):
                if x.size(-1) % 2 != 0:
                    return x
                half = x.size(-1) // 2
                freqs = torch.arange(half, device=x.device, dtype=x.dtype)
                freqs = theta ** (-freqs / half)
                positions = torch.arange(x.size(1), device=x.device, dtype=x.dtype)
                angles = torch.einsum("i,j->ij", positions, freqs)
                sin, cos = angles.sin(), angles.cos()
                x1, x2 = x[..., :half], x[..., half:]
                rotated = torch.stack([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)
                return rotated.reshape_as(x)


            def _activation(kind: str, tensor: torch.Tensor):
                if kind == "relu":
                    return F.relu(tensor)
                if kind == "gelu":
                    return F.gelu(tensor)
                if kind == "silu":
                    return F.silu(tensor)
                if kind == "swiglu":
                    # Caller supplies gate/up split; we only keep hook
                    return tensor
                raise ValueError(f"Unsupported activation: {{kind}}")


            # -------------------------------------------------------------- model
            class {model_name}(nn.Module):
                def __init__(self, config: Optional[ModelConfig] = None):
                    super().__init__()
                    self.ir = IR_DEF
                    self.order = _topological_sort(IR_DEF)
                    self.config = config or ModelConfig()
                    self.modules_by_op = nn.ModuleDict()

                    for name, op in self.ir["operations"].items():
                        op_type = op["type"]
                        attrs = op.get("attributes", {{}})
                        if op_type == "embedding":
                            self.modules_by_op[name] = nn.Embedding(
                                attrs["vocab_size"], attrs["embedding_dim"]
                            )
                        elif op_type == "linear":
                            self.modules_by_op[name] = nn.Linear(
                                attrs["in_features"],
                                attrs["out_features"],
                                bias=attrs.get("use_bias", True),
                            )
                        elif op_type == "rmsnorm":
                            self.modules_by_op[name] = RMSNorm(
                                attrs["normalized_shape"], eps=attrs.get("eps", 1e-6)
                            )
                        elif op_type == "layernorm":
                            self.modules_by_op[name] = nn.LayerNorm(
                                attrs["normalized_shape"], eps=attrs.get("eps", 1e-5)
                            )
                        elif op_type == "multi_head_attention":
                            self.modules_by_op[name] = AttentionOp(
                                attrs["num_heads"],
                                attrs.get("num_kv_heads", attrs["num_heads"]),
                                attrs["head_dim"],
                                dropout=self.config.attention_dropout,
                            )
                        # Parameter-free ops are executed directly in forward

                    # Handle tied weights declared in the IR
                    for name, op in self.ir["operations"].items():
                        attrs = op.get("attributes", {{}})
                        tie_target = attrs.get("tie_weight")
                        if tie_target and name in self.modules_by_op:
                            target_module = tie_target.split(".")[0]
                            if target_module in self.modules_by_op:
                                self.modules_by_op[name].weight = self.modules_by_op[target_module].weight

                    # Move to configured dtype
                    self.to(dtype={dtype_literal})

                def forward(self, **inputs):
                    values: Dict[str, torch.Tensor] = {{}}

                    # Bind graph inputs
                    for required in self.ir["inputs"]:
                        if required not in inputs:
                            raise ValueError(f"Missing required input '{{required}}'")
                        values[required] = inputs[required]

                    attention_mask = inputs.get("attention_mask")

                    for op_name in self.order:
                        op = self.ir["operations"][op_name]
                        op_type = op["type"]
                        attrs = op.get("attributes", {{}})
                        args = [values[i] for i in op.get("inputs", [])]

                        if op_type == "embedding":
                            out = self.modules_by_op[op_name](args[0])
                        elif op_type == "linear":
                            out = self.modules_by_op[op_name](args[0])
                        elif op_type == "rmsnorm" or op_type == "layernorm":
                            out = self.modules_by_op[op_name](args[0])
                        elif op_type == "add":
                            out = args[0] + args[1]
                        elif op_type == "mul":
                            out = args[0] * args[1]
                        elif op_type == "activation":
                            out = _activation(attrs.get("activation", "silu"), args[0])
                        elif op_type == "swiglu":
                            out = F.silu(args[0]) * args[1]
                        elif op_type == "multi_head_attention":
                            out = self.modules_by_op[op_name](args[0], args[1], args[2], attention_mask)
                        elif op_type == "rope":
                            out = _apply_rope(args[0], theta=attrs.get("theta", 10000.0))
                        elif op_type == "softmax":
                            out = F.softmax(args[0], dim=attrs.get("dim", -1))
                        else:
                            raise RuntimeError(f"Unsupported IR op type: {{op_type}}")

                        # Assume single output per op
                        out_name = op["outputs"][0]
                        values[out_name] = out

                    # Collect graph outputs
                    outputs = {{name: values[name] for name in self.ir["outputs"]}}
                    if len(outputs) == 1:
                        return next(iter(outputs.values()))
                    return outputs
            '''
        )

        return model_code

    def _derive_config_defaults(self, graph: IRGraph) -> Dict[str, Any]:
        dims: Dict[str, Any] = {}
        # Use first embedding as authoritative vocab/hidden sizes
        for op in graph.operations.values():
            if op.node_type == NodeType.EMBEDDING:
                dims["vocab_size"] = op.attributes.get("vocab_size", 50000)
                dims["hidden_size"] = op.attributes.get("embedding_dim", 4096)
                break

        # Heads / head_dim from first attention op
        for op in graph.operations.values():
            if op.node_type == NodeType.MULTI_HEAD_ATTENTION:
                dims["num_attention_heads"] = op.attributes.get("num_heads", 32)
                dims["num_key_value_heads"] = op.attributes.get("num_kv_heads", dims["num_attention_heads"])
                dims["head_dim"] = op.attributes.get("head_dim", 128)
                break

        # Intermediate size from first gate/up projection
        for op in graph.operations.values():
            if op.node_type == NodeType.LINEAR and "up_proj" in op.name:
                dims["intermediate_size"] = op.attributes.get("out_features", 11008)
                break

        # Layers
        dims["num_hidden_layers"] = sum(1 for op in graph.operations.values() if op.node_type == NodeType.MULTI_HEAD_ATTENTION)

        # Context length from input tensor shapes
        for tensor in graph.tensors.values():
            if tensor.name in graph.inputs and tensor.shape:
                if len(tensor.shape) >= 2 and tensor.shape[1] not in (-1, None):
                    dims["context_length"] = tensor.shape[1]
                    break
        dims.setdefault("context_length", 8192)

        return dims

    def _emit_config(self, graph: IRGraph) -> str:
        dims = self._derive_config_defaults(graph)
        return textwrap.dedent(
            f"""
            from dataclasses import dataclass
            from typing import Optional


            @dataclass
            class ModelConfig:
                vocab_size: int = {dims.get('vocab_size', 50000)}
                hidden_size: int = {dims.get('hidden_size', 4096)}
                intermediate_size: int = {dims.get('intermediate_size', 11008)}
                num_hidden_layers: int = {dims.get('num_hidden_layers', 32)}
                num_attention_heads: int = {dims.get('num_attention_heads', 32)}
                num_key_value_heads: int = {dims.get('num_key_value_heads', dims.get('num_attention_heads', 32))}
                head_dim: int = {dims.get('head_dim', 128)}
                max_position_embeddings: int = {dims.get('context_length', 8192)}

                attention_dropout: float = 0.0
                hidden_dropout: float = 0.0
                torch_dtype: str = "{self.precision}"

                def to_dict(self):
                    return self.__dict__.copy()
            """
        )

    def _emit_init_file(self, model_name: str) -> str:
        return textwrap.dedent(
            f"""
            from .config import ModelConfig
            from .{model_name} import {model_name}

            __all__ = [
                "ModelConfig",
                "{model_name}",
            ]

            __version__ = "1.0.0"
            """
        )

    def _emit_setup_file(self, model_name: str) -> str:
        return textwrap.dedent(
            f"""
            from setuptools import setup, find_packages

            setup(
                name="{model_name.lower()}",
                version="1.0.0",
                author="LLM Compiler",
                description="Generated LLM model",
                packages=find_packages(),
                python_requires=">=3.8",
                install_requires=[
                    "torch>=2.0.0",
                ],
            )
            """
        )

    def _emit_test_file(self, model_name: str) -> str:
        return textwrap.dedent(
            f"""
            import torch
            from .config import ModelConfig
            from .{model_name} import {model_name}


            def test_model():
                config = ModelConfig()
                model = {model_name}(config)
                batch_size = 2
                seq_len = min(16, config.max_position_embeddings)

                input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))
                outputs = model(input_ids=input_ids)
                if isinstance(outputs, dict):
                    logits = list(outputs.values())[0]
                else:
                    logits = outputs
                assert logits.shape[:2] == (batch_size, seq_len)
                print("Model forward succeeded; output shape", logits.shape)


            if __name__ == "__main__":
                test_model()
            """
        )

ðŸ“„ ./llm_compiler/emitters/safetensors.py
=============================
"""
Safetensors Emitter
===================

Emits model weights in safetensors format.
Generates both weights and metadata.
"""

from typing import Dict, List, Any, Optional
import json
import struct
import numpy as np
from pathlib import Path

class SafetensorsEmitter:
    """Emits weights in safetensors format"""
    
    def __init__(self):
        self.header_size = 8  # bytes for header length
        
    def emit(self,
             weights: Dict[str, np.ndarray],
             metadata: Dict[str, Any],
             output_path: Path) -> Dict[str, Any]:
        """
        Emit weights in safetensors format.
        
        Args:
            weights: Dictionary of tensor name -> numpy array
            metadata: Model metadata
            output_path: Output file path
            
        Returns:
            Dictionary with emission info
        """
        # Prepare tensors
        tensor_data = {}
        offsets = {}
        
        current_offset = 0
        
        # Calculate offsets
        for name, tensor in weights.items():
            # Convert to contiguous array
            tensor = np.ascontiguousarray(tensor)
            
            # Store data
            tensor_data[name] = tensor
            
            # Calculate offset and size
            size = tensor.nbytes
            dtype = self._numpy_to_safetensors_dtype(tensor.dtype)
            shape = list(tensor.shape)
            
            offsets[name] = {
                "dtype": dtype,
                "shape": shape,
                "data_offsets": [current_offset, current_offset + size]
            }
            
            current_offset += size
        
        # Create header
        header = {
            "__metadata__": metadata
        }
        header.update(offsets)
        
        # Serialize header
        header_json = json.dumps(header).encode('utf-8')
        header_length = len(header_json)
        
        # Calculate padding
        total_header_size = self.header_size + header_length
        pad = 8 - (total_header_size % 8)
        if pad == 8:
            pad = 0
        
        # Write file
        with open(output_path, 'wb') as f:
            # Write header length
            f.write(struct.pack('<Q', header_length))
            
            # Write header
            f.write(header_json)
            
            # Write padding
            if pad > 0:
                f.write(b'\x00' * pad)
            
            # Write tensor data
            for name in weights.keys():
                tensor = tensor_data[name]
                f.write(tensor.tobytes())
        
        # Return info
        return {
            "file_path": str(output_path),
            "file_size": output_path.stat().st_size,
            "num_tensors": len(weights),
            "total_bytes": current_offset,
            "header_size": total_header_size + pad,
            "metadata": metadata
        }
    
    def _numpy_to_safetensors_dtype(self, dtype: np.dtype) -> str:
        """Convert numpy dtype to safetensors dtype string"""
        mapping = {
            np.float32: "F32",
            np.float16: "F16",
            np.bfloat16: "BF16",
            np.int32: "I32",
            np.int64: "I64",
            np.uint8: "U8",
            np.bool_: "BOOL",
        }
        
        for np_type, st_type in mapping.items():
            if np.issubdtype(dtype, np_type):
                return st_type
        
        raise ValueError(f"Unsupported dtype: {dtype}")
    
    def create_metadata(self,
                       model_name: str,
                       config: Dict[str, Any],
                       parameter_count: int) -> Dict[str, Any]:
        """Create safetensors metadata"""
        return {
            "model_name": model_name,
            "format": "pytorch",
            "architecture": config.get('template', 'unknown'),
            "vocab_size": config.get('vocab_size', 0),
            "hidden_size": config.get('hidden_size', 0),
            "num_layers": config.get('num_layers', 0),
            "num_attention_heads": config.get('num_attention_heads', 0),
            "num_key_value_heads": config.get('num_key_value_heads', 0),
            "head_dim": config.get('head_dim', 0),
            "intermediate_size": config.get('intermediate_size', 0),
            "max_position_embeddings": config.get('context_length', 0),
            "rope_theta": config.get('rope_theta', 10000.0),
            "attention_type": config.get('attention_type', 'gqa'),
            "norm_type": config.get('norm', 'rmsnorm'),
            "activation": config.get('activation', 'swiglu'),
            "parameter_count": parameter_count,
            "generator": "llm_compiler",
            "version": "1.0.0"
        }
    
    def generate_weight_names(self, 
                             model_name: str,
                             num_layers: int) -> List[str]:
        """Generate weight names for model"""
        names = []
        
        # Embeddings
        names.append(f"{model_name}.embed_tokens.weight")
        
        # Layers
        for i in range(num_layers):
            prefix = f"{model_name}.layers.{i}"
            
            # Attention
            names.append(f"{prefix}.self_attn.q_proj.weight")
            names.append(f"{prefix}.self_attn.k_proj.weight")
            names.append(f"{prefix}.self_attn.v_proj.weight")
            names.append(f"{prefix}.self_attn.o_proj.weight")
            
            # Norms
            names.append(f"{prefix}.input_layernorm.weight")
            names.append(f"{prefix}.post_attention_layernorm.weight")
            
            # MLP
            names.append(f"{prefix}.mlp.w1.weight")
            names.append(f"{prefix}.mlp.w2.weight")
            names.append(f"{prefix}.mlp.w3.weight")
        
        # Final norm
        names.append(f"{model_name}.norm.weight")
        
        # LM head (if not tied)
        names.append(f"{model_name}.lm_head.weight")
        
        return names                                                                                 
apple@Apples-MacBook-Air alwork % 
ðŸ“„ ./llm_compiler/tokenizers/__init__.py
=============================
from .base import (
    BaseTokenizer,
    ToyBPETokenizer,
    ToyUnigramTokenizer,
    TokenizerTrainingStats,
)

__all__ = [
    "BaseTokenizer",
    "ToyBPETokenizer",
    "ToyUnigramTokenizer",
    "TokenizerTrainingStats",
]

ðŸ“„ ./llm_compiler/tokenizers/base.py
=============================
"""
Tokenizer base classes
======================

Lightweight implementations to keep tokenizer definition, vocab size,
and corpus statistics bound together. These implementations are minimal
but enforce the invariants needed by the compiler/emitters.
"""

from __future__ import annotations
from dataclasses import dataclass, field
from typing import Dict, List, Iterable


@dataclass
class TokenizerTrainingStats:
    """Corpus statistics captured during tokenizer training."""
    token_freqs: Dict[str, int]
    total_tokens: int
    unique_tokens: int

    @classmethod
    def from_counts(cls, counts: Dict[str, int]) -> "TokenizerTrainingStats":
        total = sum(counts.values())
        return cls(
            token_freqs=dict(counts),
            total_tokens=total,
            unique_tokens=len(counts),
        )


class BaseTokenizer:
    """Minimal tokenizer contract used by the compiler."""

    def __init__(self, vocab_size: int, unk_token: str = "<unk>", pad_token: str = "<pad>"):
        self.vocab_size = vocab_size
        self.unk_token = unk_token
        self.pad_token = pad_token
        self.token_to_id: Dict[str, int] = {}
        self.id_to_token: List[str] = []
        self.training_stats: TokenizerTrainingStats | None = None

    # --- binding / validation -------------------------------------------------
    def bind_training_stats(self, stats: TokenizerTrainingStats) -> None:
        """Attach corpus statistics and ensure the vocab respects frequencies."""
        self.training_stats = stats
        if stats.unique_tokens < self.vocab_size:
            raise ValueError(
                f"Requested vocab_size={self.vocab_size} exceeds unique tokens in corpus ({stats.unique_tokens})"
            )

    def enforce_vocab_size(self):
        if len(self.id_to_token) != self.vocab_size:
            raise ValueError(
                f"Tokenizer vocab size mismatch: expected {self.vocab_size}, got {len(self.id_to_token)}"
            )

    # --- interface -----------------------------------------------------------
    def encode(self, text: str) -> List[int]:
        raise NotImplementedError

    def decode(self, ids: Iterable[int]) -> str:
        raise NotImplementedError

    def train(self, stats: TokenizerTrainingStats):
        """Train tokenizer using provided corpus stats."""
        raise NotImplementedError


class ToyBPETokenizer(BaseTokenizer):
    """
    Minimal stand-in BPE tokenizer.
    Not production-ready but enforces vocab/statistics coupling.
    """

    def train(self, stats: TokenizerTrainingStats):
        self.bind_training_stats(stats)
        # Build vocab by most frequent tokens first
        sorted_tokens = sorted(stats.token_freqs.items(), key=lambda kv: kv[1], reverse=True)
        top_tokens = [tok for tok, _ in sorted_tokens[: self.vocab_size - 2]]
        # Reserve UNK and PAD
        vocab = [self.pad_token, self.unk_token] + top_tokens
        self.id_to_token = vocab[: self.vocab_size]
        self.token_to_id = {tok: idx for idx, tok in enumerate(self.id_to_token)}
        self.enforce_vocab_size()
        return self

    def encode(self, text: str) -> List[int]:
        # Very naive whitespace tokenization for placeholder behavior
        tokens = text.split()
        ids = []
        for tok in tokens:
            ids.append(self.token_to_id.get(tok, self.token_to_id.get(self.unk_token, 1)))
        return ids

    def decode(self, ids: Iterable[int]) -> str:
        return " ".join(self.id_to_token[i] if 0 <= i < len(self.id_to_token) else self.unk_token for i in ids)


class ToyUnigramTokenizer(BaseTokenizer):
    """Simple unigram tokenizer with frequency-aware vocab selection."""

    def train(self, stats: TokenizerTrainingStats):
        self.bind_training_stats(stats)
        sorted_tokens = sorted(stats.token_freqs.items(), key=lambda kv: kv[1], reverse=True)
        top_tokens = [tok for tok, _ in sorted_tokens[: self.vocab_size - 2]]
        vocab = [self.pad_token, self.unk_token] + top_tokens
        self.id_to_token = vocab[: self.vocab_size]
        self.token_to_id = {tok: idx for idx, tok in enumerate(self.id_to_token)}
        self.enforce_vocab_size()
        return self

    def encode(self, text: str) -> List[int]:
        tokens = text.split()
        return [self.token_to_id.get(tok, self.token_to_id.get(self.unk_token, 1)) for tok in tokens]

    def decode(self, ids: Iterable[int]) -> str:
        return " ".join(self.id_to_token[i] if 0 <= i < len(self.id_to_token) else self.unk_token for i in ids)

ðŸ“„ ./llm_compiler/ir/graph.py
=============================
"""
Intermediate Representation (IR)
================================

Framework-agnostic graph representation of LLM architectures.
Explicit tensors, shapes, and connections.
"""

from __future__ import annotations
from typing import Dict, List, Tuple, Optional, Any, Set
from dataclasses import dataclass, field
from enum import Enum
import json

class TensorDType(Enum):
    """Tensor data types"""
    FLOAT32 = "float32"
    FLOAT16 = "float16"
    BFLOAT16 = "bfloat16"
    INT32 = "int32"
    INT64 = "int64"
    BOOL = "bool"

class NodeType(Enum):
    """IR node types"""
    INPUT = "input"
    OUTPUT = "output"
    CONSTANT = "constant"
    
    # Operations
    LINEAR = "linear"
    EMBEDDING = "embedding"
    LAYERNORM = "layernorm"
    RMSNORM = "rmsnorm"
    ATTENTION = "attention"
    MULTI_HEAD_ATTENTION = "multi_head_attention"
    ROPE = "rope"
    ALIBI = "alibi"
    SWIGLU = "swiglu"
    ACTIVATION = "activation"
    ADD = "add"
    MUL = "mul"
    MATMUL = "matmul"
    TRANSPOSE = "transpose"
    RESHAPE = "reshape"
    CONCAT = "concat"
    SPLIT = "split"
    SLICE = "slice"
    GATHER = "gather"
    SOFTMAX = "softmax"
    DROPOUT = "dropout"
    
    # Control flow
    LOOP = "loop"
    CONDITIONAL = "conditional"

@dataclass
class Tensor:
    """IR Tensor representation"""
    name: str
    shape: List[Any]  # Can contain -1 for dynamic dimensions
    dtype: TensorDType
    node: Optional[str] = None  # Producing node
    consumers: List[str] = field(default_factory=list)  # Consuming nodes
    
    def __str__(self) -> str:
        shape_str = "[" + ", ".join(str(s) for s in self.shape) + "]"
        return f"{self.name}: {shape_str} {self.dtype.value}"
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "shape": self.shape,
            "dtype": self.dtype.value,
            "node": self.node,
            "consumers": self.consumers
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Tensor':
        return cls(
            name=data["name"],
            shape=data["shape"],
            dtype=TensorDType(data["dtype"]),
            node=data.get("node"),
            consumers=data.get("consumers", [])
        )

@dataclass
class Operation:
    """IR Operation node"""
    name: str
    node_type: NodeType
    inputs: List[str]  # Input tensor names
    outputs: List[str]  # Output tensor names
    attributes: Dict[str, Any] = field(default_factory=dict)
    
    def __str__(self) -> str:
        inputs_str = ", ".join(self.inputs)
        outputs_str = ", ".join(self.outputs)
        attrs_str = ""
        if self.attributes:
            attrs_str = " " + " ".join(f"{k}={v}" for k, v in self.attributes.items())
        return f"{self.name}: {self.node_type.value}({inputs_str}) -> {outputs_str}{attrs_str}"
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "type": self.node_type.value,
            "inputs": self.inputs,
            "outputs": self.outputs,
            "attributes": self.attributes
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Operation':
        return cls(
            name=data["name"],
            node_type=NodeType(data["type"]),
            inputs=data["inputs"],
            outputs=data["outputs"],
            attributes=data.get("attributes", {})
        )

class IRGraph:
    """Complete IR Graph"""
    
    def __init__(self, name: str):
        self.name = name
        self.tensors: Dict[str, Tensor] = {}
        self.operations: Dict[str, Operation] = {}
        self.inputs: List[str] = []
        self.outputs: List[str] = []
    
    def add_tensor(self, tensor: Tensor):
        """Add tensor to graph"""
        if tensor.name in self.tensors:
            raise ValueError(f"Tensor {tensor.name} already exists")
        self.tensors[tensor.name] = tensor
    
    def add_operation(self, operation: Operation):
        """Add operation to graph"""
        if operation.name in self.operations:
            raise ValueError(f"Operation {operation.name} already exists")
        
        # Update tensor references
        for output in operation.outputs:
            if output in self.tensors:
                self.tensors[output].node = operation.name
            else:
                # Create output tensor
                self.tensors[output] = Tensor(
                    name=output,
                    shape=[],  # Unknown shape
                    dtype=TensorDType.FLOAT32,  # Default
                    node=operation.name
                )
        
        # Update input tensor consumers
        for input_name in operation.inputs:
            if input_name in self.tensors:
                self.tensors[input_name].consumers.append(operation.name)
            else:
                # Create input tensor if it doesn't exist
                self.tensors[input_name] = Tensor(
                    name=input_name,
                    shape=[],  # Unknown shape
                    dtype=TensorDType.FLOAT32,  # Default
                    consumers=[operation.name]
                )
        
        self.operations[operation.name] = operation
    
    def add_input(self, tensor_name: str):
        """Mark tensor as graph input"""
        if tensor_name not in self.inputs:
            self.inputs.append(tensor_name)
    
    def add_output(self, tensor_name: str):
        """Mark tensor as graph output"""
        if tensor_name not in self.outputs:
            self.outputs.append(tensor_name)
    
    def validate(self) -> List[str]:
        """Validate graph consistency"""
        errors = []
        
        # Check all tensors have producers or are inputs
        for tensor_name, tensor in self.tensors.items():
            if tensor.node is None and tensor_name not in self.inputs:
                errors.append(f"Tensor {tensor_name} has no producer and is not an input")
            
            # Check consumers exist
            for consumer in tensor.consumers:
                if consumer not in self.operations:
                    errors.append(f"Tensor {tensor_name} references non-existent consumer {consumer}")
        
        # Check operations reference valid tensors
        for op_name, operation in self.operations.items():
            for input_name in operation.inputs:
                if input_name not in self.tensors:
                    errors.append(f"Operation {op_name} references non-existent input {input_name}")
            
            for output_name in operation.outputs:
                if output_name not in self.tensors:
                    errors.append(f"Operation {op_name} references non-existent output {output_name}")
        
        # Check outputs exist
        for output_name in self.outputs:
            if output_name not in self.tensors:
                errors.append(f"Output {output_name} does not exist")

        # Semantic validation (shapes/dtypes)
        errors.extend(self.validate_semantics())

        return errors

    def _topological_order(self) -> List[str]:
        """Return operation names in topological order"""
        indegree = {name: 0 for name in self.operations}
        for op in self.operations.values():
            for inp in op.inputs:
                producer = self.tensors.get(inp, Tensor("", [], TensorDType.FLOAT32)).node
                if producer and producer in indegree:
                    indegree[op.name] += 1
        ready = [name for name, deg in indegree.items() if deg == 0]
        order = []
        while ready:
            current = ready.pop(0)
            order.append(current)
            op = self.operations[current]
            for out in op.outputs:
                for consumer in self.tensors.get(out, Tensor("", [], TensorDType.FLOAT32)).consumers:
                    if consumer in indegree:
                        indegree[consumer] -= 1
                        if indegree[consumer] == 0:
                            ready.append(consumer)
        # Fallback to insertion order if cycle detected
        if len(order) != len(self.operations):
            return list(self.operations.keys())
        return order

    def validate_semantics(self) -> List[str]:
        """Validate shapes/dtypes for common operations"""
        errors: List[str] = []
        tensor_shapes = {name: tensor.shape for name, tensor in self.tensors.items()}
        tensor_dtypes = {name: tensor.dtype for name, tensor in self.tensors.items()}

        def _rank(shape):
            return len(shape) if shape is not None else 0

        def _dim_equal(a, b):
            return a == b or a == -1 or b == -1

        def _propagate_output(op_name, output_name, shape, dtype=TensorDType.FLOAT32):
            if shape is not None:
                tensor_shapes[output_name] = list(shape)
            tensor_dtypes[output_name] = dtype

        for op_name in self._topological_order():
            op = self.operations[op_name]
            try:
                if op.node_type == NodeType.LINEAR:
                    in_tensor = op.inputs[0]
                    in_shape = tensor_shapes.get(in_tensor)
                    in_features = op.attributes.get("in_features")
                    out_features = op.attributes.get("out_features")
                    if in_shape and in_features is not None and len(in_shape) > 0:
                        if in_shape[-1] != in_features:
                            errors.append(
                                f"{op_name}: input last dim {in_shape[-1]} "
                                f"does not match linear in_features {in_features}"
                            )
                    if out_features is not None:
                        prefix = in_shape[:-1] if in_shape else [-1]
                        _propagate_output(op_name, op.outputs[0], prefix + [out_features])

                elif op.node_type in (NodeType.RMSNORM, NodeType.LAYERNORM):
                    src = op.inputs[0]
                    in_shape = tensor_shapes.get(src)
                    normalized_shape = op.attributes.get("normalized_shape")
                    if in_shape and normalized_shape is not None:
                        if in_shape[-1] != normalized_shape:
                            errors.append(
                                f"{op_name}: normalized_shape {normalized_shape} "
                                f"does not match input hidden dim {in_shape[-1]}"
                            )
                    _propagate_output(op_name, op.outputs[0], in_shape)

                elif op.node_type == NodeType.EMBEDDING:
                    input_name = op.inputs[0]
                    in_shape = tensor_shapes.get(input_name, [])
                    vocab = op.attributes.get("vocab_size")
                    if vocab is None:
                        errors.append(f"{op_name}: missing vocab_size attribute")
                    _propagate_output(op_name, op.outputs[0], in_shape + [op.attributes.get("embedding_dim", 0)])

                elif op.node_type == NodeType.MULTI_HEAD_ATTENTION:
                    q_name, k_name, v_name = op.inputs[:3]
                    q_shape = tensor_shapes.get(q_name)
                    k_shape = tensor_shapes.get(k_name)
                    v_shape = tensor_shapes.get(v_name)
                    num_heads = op.attributes.get("num_heads")
                    num_kv_heads = op.attributes.get("num_kv_heads", num_heads)
                    head_dim = op.attributes.get("head_dim")
                    if q_shape and num_heads and head_dim and q_shape[-1] != num_heads * head_dim:
                        errors.append(
                            f"{op_name}: query hidden dim {q_shape[-1]} != num_heads*head_dim ({num_heads*head_dim})"
                        )
                    if k_shape and num_kv_heads and head_dim and k_shape[-1] != num_kv_heads * head_dim:
                        errors.append(
                            f"{op_name}: key hidden dim {k_shape[-1]} != num_kv_heads*head_dim ({num_kv_heads*head_dim})"
                        )
                    if v_shape and num_kv_heads and head_dim and v_shape[-1] != num_kv_heads * head_dim:
                        errors.append(
                            f"{op_name}: value hidden dim {v_shape[-1]} != num_kv_heads*head_dim ({num_kv_heads*head_dim})"
                        )
                    # Batch/seq consistency
                    if q_shape and k_shape and _rank(q_shape) >= 2 and _rank(k_shape) >= 2:
                        if not _dim_equal(q_shape[0], k_shape[0]):
                            errors.append(f"{op_name}: batch mismatch q({q_shape[0]}) vs k({k_shape[0]})")
                    if q_shape and v_shape and _rank(q_shape) >= 2 and _rank(v_shape) >= 2:
                        if not _dim_equal(q_shape[0], v_shape[0]):
                            errors.append(f"{op_name}: batch mismatch q({q_shape[0]}) vs v({v_shape[0]})")
                    # Optional attention mask as 4th input
                    if len(op.inputs) >= 4:
                        mask_shape = tensor_shapes.get(op.inputs[3])
                        if mask_shape and _rank(mask_shape) >= 4 and q_shape:
                            if not _dim_equal(mask_shape[0], q_shape[0]):
                                errors.append(f"{op_name}: attention mask batch {mask_shape[0]} != query batch {q_shape[0]}")
                            if not _dim_equal(mask_shape[-1], k_shape[1] if k_shape and _rank(k_shape) >= 2 else -1):
                                errors.append(f"{op_name}: attention mask length {mask_shape[-1]} incompatible with key seq")
                    # Output shape follows query
                    if q_shape:
                        _propagate_output(op_name, op.outputs[0], q_shape)

                elif op.node_type in (NodeType.ADD, NodeType.MUL):
                    a_shape = tensor_shapes.get(op.inputs[0])
                    b_shape = tensor_shapes.get(op.inputs[1])
                    if a_shape and b_shape and a_shape != b_shape:
                        errors.append(f"{op_name}: operand shape mismatch {a_shape} vs {b_shape}")
                    _propagate_output(op_name, op.outputs[0], a_shape or b_shape or [])

                elif op.node_type == NodeType.SWIGLU:
                    gate_shape = tensor_shapes.get(op.inputs[0])
                    up_shape = tensor_shapes.get(op.inputs[1])
                    if gate_shape and up_shape and gate_shape != up_shape:
                        errors.append(f"{op_name}: gate/up shapes differ {gate_shape} vs {up_shape}")
                    _propagate_output(op_name, op.outputs[0], gate_shape or up_shape or [])

                elif op.node_type == NodeType.ACTIVATION:
                    src = op.inputs[0]
                    _propagate_output(op_name, op.outputs[0], tensor_shapes.get(src, []), tensor_dtypes.get(src, TensorDType.FLOAT32))

                elif op.node_type == NodeType.MATMUL:
                    a_shape = tensor_shapes.get(op.inputs[0], [])
                    b_shape = tensor_shapes.get(op.inputs[1], [])
                    if _rank(a_shape) >= 2 and _rank(b_shape) >= 2:
                        if not _dim_equal(a_shape[-1], b_shape[-2]):
                            errors.append(f"{op_name}: matmul dim mismatch {a_shape[-1]} vs {b_shape[-2]}")
                        out_shape = a_shape[:-1] + [b_shape[-1]]
                        _propagate_output(op_name, op.outputs[0], out_shape)
                    else:
                        _propagate_output(op_name, op.outputs[0], [])
                else:
                    # Default: propagate first input shape if available
                    if op.inputs:
                        _propagate_output(op_name, op.outputs[0], tensor_shapes.get(op.inputs[0], []), tensor_dtypes.get(op.inputs[0], TensorDType.FLOAT32))
            except Exception as exc:
                errors.append(f"{op_name}: semantic validation error {exc}")

        return errors
    
    def get_parameter_count(self) -> int:
        """Estimate parameter count from graph"""
        from ..utils.parameters import count_parameters_from_ir
        return count_parameters_from_ir(self)
    
    def to_dict(self) -> Dict[str, Any]:
        """Serialize graph to dictionary"""
        return {
            "name": self.name,
            "tensors": {name: tensor.to_dict() for name, tensor in self.tensors.items()},
            "operations": {name: op.to_dict() for name, op in self.operations.items()},
            "inputs": self.inputs,
            "outputs": self.outputs
        }
    
    def to_json(self) -> str:
        """Serialize graph to JSON"""
        return json.dumps(self.to_dict(), indent=2)
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'IRGraph':
        """Deserialize graph from dictionary"""
        graph = cls(data["name"])
        
        # Load tensors
        for name, tensor_data in data.get("tensors", {}).items():
            graph.tensors[name] = Tensor.from_dict(tensor_data)
        
        # Load operations
        for name, op_data in data.get("operations", {}).items():
            graph.operations[name] = Operation.from_dict(op_data)
        
        graph.inputs = data.get("inputs", [])
        graph.outputs = data.get("outputs", [])
        
        return graph
    
    @classmethod
    def from_json(cls, json_str: str) -> 'IRGraph':
        """Deserialize graph from JSON"""
        data = json.loads(json_str)
        return cls.from_dict(data)
    
    def __str__(self) -> str:
        lines = [f"IRGraph: {self.name}"]
        lines.append(f"  Tensors: {len(self.tensors)}")
        lines.append(f"  Operations: {len(self.operations)}")
        lines.append(f"  Inputs: {self.inputs}")
        lines.append(f"  Outputs: {self.outputs}")
        
        # Show operations
        lines.append("\nOperations:")
        for op in self.operations.values():
            lines.append(f"  {op}")
        
        # Show tensors
        lines.append("\nTensors:")
        for tensor in self.tensors.values():
            lines.append(f"  {tensor}")
        
        return "\n".join(lines)

ðŸ“„ ./llm_compiler/ir/builder.py
=============================
"""
IR Graph Builder
================

Helper for building IR graphs from templates.
Provides high-level operations that map to IR nodes.
"""

from typing import Dict, List, Tuple, Optional, Any, Union
from .graph import IRGraph, Tensor, Operation, NodeType, TensorDType

class GraphBuilder:
    """Builder for IR graphs"""
    
    def __init__(self, graph: IRGraph = None):
        self.graph = graph or IRGraph("unnamed")
        self._tensor_counter = 0
        self._op_counter = 0
    
    def _new_name(self, prefix: str) -> str:
        """Generate unique name"""
        self._tensor_counter += 1
        return f"{prefix}_{self._tensor_counter}"
    
    def _new_op_name(self, prefix: str) -> str:
        """Generate unique operation name"""
        self._op_counter += 1
        return f"{prefix}_{self._op_counter}"
    
    def create_input(self, 
                     name: str,
                     shape: List[Any],
                     dtype: Union[str, TensorDType] = "float32") -> str:
        """Create input tensor"""
        if isinstance(dtype, str):
            dtype = TensorDType(dtype)
        
        tensor = Tensor(
            name=name,
            shape=shape,
            dtype=dtype
        )
        
        self.graph.add_tensor(tensor)
        self.graph.add_input(name)
        return name
    
    def create_constant(self,
                        name: str,
                        value: Any,
                        shape: List[int],
                        dtype: Union[str, TensorDType] = "float32") -> str:
        """Create constant tensor"""
        if isinstance(dtype, str):
            dtype = TensorDType(dtype)
        
        tensor = Tensor(
            name=name,
            shape=shape,
            dtype=dtype
        )
        
        op = Operation(
            name=self._new_op_name("constant"),
            node_type=NodeType.CONSTANT,
            inputs=[],
            outputs=[name],
            attributes={
                "value": value,
                "shape": shape,
                "dtype": dtype.value
            }
        )
        
        self.graph.add_tensor(tensor)
        self.graph.add_operation(op)
        return name
    
    def create_embedding(self,
                        name: str,
                        input: str,
                        vocab_size: int,
                        embedding_dim: int) -> str:
        """Create embedding operation"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.EMBEDDING,
            inputs=[input],
            outputs=[output],
            attributes={
                "vocab_size": vocab_size,
                "embedding_dim": embedding_dim
            }
        )
        
        self.graph.add_operation(op)
        # Record tensor shape/dtype for downstream semantic validation
        self.graph.tensors[output].shape = [ -1, -1, embedding_dim ]
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_linear(self,
                     name: str,
                     input: str,
                     in_features: int,
                     out_features: int,
                     use_bias: bool = True,
                     tie_weight: Optional[str] = None) -> str:
        """Create linear (fully connected) layer"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.LINEAR,
            inputs=[input],
            outputs=[output],
            attributes={
                "in_features": in_features,
                "out_features": out_features,
                "use_bias": use_bias,
                "tie_weight": tie_weight
            }
        )
        
        self.graph.add_operation(op)
        # Assume last dimension equals out_features; batch/sequence preserved
        input_shape = self.graph.tensors.get(input, Tensor("", [], TensorDType.FLOAT32)).shape
        prefix = input_shape[:-1] if input_shape else [-1]
        self.graph.tensors[output].shape = prefix + [out_features]
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_rmsnorm(self,
                      name: str,
                      input: str,
                      dim: int) -> str:
        """Create RMSNorm operation"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.RMSNORM,
            inputs=[input],
            outputs=[output],
            attributes={
                "normalized_shape": dim,
                "eps": 1e-6
            }
        )
        
        self.graph.add_operation(op)
        # RMSNorm preserves shape
        self.graph.tensors[output].shape = list(self.graph.tensors.get(input, Tensor("", [], TensorDType.FLOAT32)).shape)
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_layernorm(self,
                        name: str,
                        input: str,
                        dim: int,
                        eps: float = 1e-5) -> str:
        """Create LayerNorm operation"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.LAYERNORM,
            inputs=[input],
            outputs=[output],
            attributes={
                "normalized_shape": dim,
                "eps": eps
            }
        )
        
        self.graph.add_operation(op)
        self.graph.tensors[output].shape = list(self.graph.tensors.get(input, Tensor("", [], TensorDType.FLOAT32)).shape)
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_rope(self,
                   name: str,
                   input: str,
                   dim: int,
                   theta: float = 10000.0,
                   scaling_factor: Optional[float] = None) -> str:
        """Create RoPE (Rotary Positional Embedding)"""
        output = self._new_name(f"{name}_out")
        
        attrs = {
            "dim": dim,
            "theta": theta
        }
        if scaling_factor is not None:
            attrs["scaling_factor"] = scaling_factor
        
        op = Operation(
            name=name,
            node_type=NodeType.ROPE,
            inputs=[input],
            outputs=[output],
            attributes=attrs
        )
        
        self.graph.add_operation(op)
        # RoPE preserves shape
        self.graph.tensors[output].shape = list(self.graph.tensors.get(input, Tensor("", [], TensorDType.FLOAT32)).shape)
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_alibi(self,
                    name: str,
                    num_heads: int,
                    max_bias: float = 8.0) -> str:
        """Create ALiBi (Attention with Linear Biases)"""
        output = self._new_name(f"{name}_bias")
        
        op = Operation(
            name=name,
            node_type=NodeType.ALIBI,
            inputs=[],
            outputs=[output],
            attributes={
                "num_heads": num_heads,
                "max_bias": max_bias
            }
        )
        
        self.graph.add_operation(op)
        # ALiBi bias broadcast shape
        self.graph.tensors[output].shape = [1, num_heads, 1, 1]
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_multi_head_attention(self,
                                   name: str,
                                   query: str,
                                   key: str,
                                   value: str,
                                   num_heads: int,
                                   num_kv_heads: int,
                                   head_dim: int,
                                   attention_type: str = "gqa",
                                   use_alibi: bool = False) -> str:
        """Create multi-head attention operation"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.MULTI_HEAD_ATTENTION,
            inputs=[query, key, value],
            outputs=[output],
            attributes={
                "num_heads": num_heads,
                "num_kv_heads": num_kv_heads,
                "head_dim": head_dim,
                "attention_type": attention_type,
                "use_alibi": use_alibi
            }
        )
        
        self.graph.add_operation(op)
        # Output shape matches query with hidden dim = num_heads * head_dim
        q_shape = self.graph.tensors.get(query, Tensor("", [], TensorDType.FLOAT32)).shape
        prefix = q_shape[:-1] if q_shape else [-1, -1]
        self.graph.tensors[output].shape = prefix + [num_heads * head_dim]
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_swiglu(self,
                     name: str,
                     gate: str,
                     up: str) -> str:
        """Create SwiGLU activation"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.SWIGLU,
            inputs=[gate, up],
            outputs=[output],
            attributes={}
        )
        
        self.graph.add_operation(op)
        # Output shape follows gate/up input shape
        self.graph.tensors[output].shape = list(self.graph.tensors.get(gate, Tensor("", [], TensorDType.FLOAT32)).shape)
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_activation(self,
                         name: str,
                         input: str,
                         activation_type: str) -> str:
        """Create activation function"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.ACTIVATION,
            inputs=[input],
            outputs=[output],
            attributes={
                "activation": activation_type
            }
        )
        
        self.graph.add_operation(op)
        self.graph.tensors[output].shape = list(self.graph.tensors.get(input, Tensor("", [], TensorDType.FLOAT32)).shape)
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_add(self,
                  name: str,
                  a: str,
                  b: str) -> str:
        """Create element-wise addition"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.ADD,
            inputs=[a, b],
            outputs=[output],
            attributes={}
        )
        
        self.graph.add_operation(op)
        # Addition preserves shape (assume broadcast-compatible)
        self.graph.tensors[output].shape = list(self.graph.tensors.get(a, Tensor("", [], TensorDType.FLOAT32)).shape)
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_mul(self,
                  name: str,
                  a: str,
                  b: str) -> str:
        """Create element-wise multiplication"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.MUL,
            inputs=[a, b],
            outputs=[output],
            attributes={}
        )
        
        self.graph.add_operation(op)
        self.graph.tensors[output].shape = list(self.graph.tensors.get(a, Tensor("", [], TensorDType.FLOAT32)).shape)
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_matmul(self,
                     name: str,
                     a: str,
                     b: str,
                     transpose_a: bool = False,
                     transpose_b: bool = False) -> str:
        """Create matrix multiplication"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.MATMUL,
            inputs=[a, b],
            outputs=[output],
            attributes={
                "transpose_a": transpose_a,
                "transpose_b": transpose_b
            }
        )
        
        self.graph.add_operation(op)
        self.graph.tensors[output].shape = []
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_softmax(self,
                      name: str,
                      input: str,
                      dim: int = -1) -> str:
        """Create softmax operation"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.SOFTMAX,
            inputs=[input],
            outputs=[output],
            attributes={
                "dim": dim
            }
        )
        
        self.graph.add_operation(op)
        self.graph.tensors[output].shape = list(self.graph.tensors.get(input, Tensor("", [], TensorDType.FLOAT32)).shape)
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def create_dropout(self,
                      name: str,
                      input: str,
                      rate: float = 0.1) -> str:
        """Create dropout operation"""
        output = self._new_name(f"{name}_out")
        
        op = Operation(
            name=name,
            node_type=NodeType.DROPOUT,
            inputs=[input],
            outputs=[output],
            attributes={
                "rate": rate
            }
        )
        
        self.graph.add_operation(op)
        self.graph.tensors[output].shape = list(self.graph.tensors.get(input, Tensor("", [], TensorDType.FLOAT32)).shape)
        self.graph.tensors[output].dtype = TensorDType.FLOAT32
        return output
    
    def set_output(self, tensor: str, name: str = None):
        """Mark tensor as graph output"""
        self.graph.add_output(tensor)
    
    def get_graph(self) -> IRGraph:
        """Get built graph"""
        return self.graph

ðŸ“„ ./llm_compiler/datasets/__init__.py
=============================
from .spec import DatasetSpec

__all__ = ["DatasetSpec"]

ðŸ“„ ./llm_compiler/datasets/spec.py
=============================
"""
Dataset Specification
=====================

Lightweight declaration for datasets used during tokenizer training and
model specification. Couples corpus statistics (token counts/frequencies)
to the tokenizer and ultimately to the architecture spec.
"""

from __future__ import annotations
from dataclasses import dataclass, asdict
from typing import Optional, Dict, Any

from ..tokenizers.base import TokenizerTrainingStats


@dataclass
class DatasetSpec:
    name: str
    split: str = "train"
    tokenizer_stats: Optional[TokenizerTrainingStats] = None
    path: Optional[str] = None  # optional local path/URI hint
    meta: Dict[str, Any] = None

    def to_dict(self) -> Dict[str, Any]:
        data = asdict(self)
        if self.tokenizer_stats:
            data["tokenizer_stats"] = {
                "token_freqs": dict(self.tokenizer_stats.token_freqs),
                "total_tokens": self.tokenizer_stats.total_tokens,
                "unique_tokens": self.tokenizer_stats.unique_tokens,
            }
        return data

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "DatasetSpec":
        stats = data.get("tokenizer_stats")
        tokenizer_stats = None
        if stats:
            tokenizer_stats = TokenizerTrainingStats(
                token_freqs=stats.get("token_freqs", {}),
                total_tokens=stats.get("total_tokens", 0),
                unique_tokens=stats.get("unique_tokens", len(stats.get("token_freqs", {}))),
            )
        return cls(
            name=data["name"],
            split=data.get("split", "train"),
            tokenizer_stats=tokenizer_stats,
            path=data.get("path"),
            meta=data.get("meta"),
        )

ðŸ“„ ./llm_compiler/__init__.py
=============================
"""
LLM Architecture Compiler
=========================

A deterministic, explicit system for defining, generating, and emitting
Large Language Models from declarative specifications.

Philosophy:
- Architecture-first, not checkpoint-first
- Templates, not ad-hoc classes
- Constraint solving instead of guessing
- Compiler mindset, not framework mindset
- Everything reproducible from spec alone

Key Components:
1. Templates: Canonical architecture definitions
2. Specification: Single declarative model spec
3. Solver: Architecture constraint solver
4. IR: Framework-agnostic intermediate representation
5. Emitters: Backend-specific code generation
"""

__version__ = "1.0.0"
__all__ = ['LLM', 'compile_spec', 'validate_spec', 'list_templates', 'DatasetSpec']

from .spec import LLM
from .compile import compile_spec
from .templates.registry import list_templates
from .utils.validation import validate_spec
from .datasets.spec import DatasetSpec

ðŸ“„ ./llm_compiler/spec.py
=============================
"""
Specification Layer
===================

Defines the declarative interface for specifying LLM architectures.
All model definitions must be complete and explicit - no magic defaults.
"""

from dataclasses import dataclass, field
from typing import Dict, Any, Optional, Union, List
from enum import Enum

class NormType(Enum):
    """Normalization types"""
    LAYERNORM = "layernorm"
    RMSNORM = "rmsnorm"
    SCALENORM = "scalenorm"
    NO_NORM = "no_norm"

class ActivationType(Enum):
    """Activation functions"""
    GELU = "gelu"
    RELU = "relu"
    SILU = "silu"
    SWIGLU = "swiglu"
    GELLU = "gelu"
    RELU_SQUARED = "relu_squared"

class AttentionType(Enum):
    """Attention variants"""
    MHA = "mha"  # Multi-head attention
    MQA = "mqa"  # Multi-query attention
    GQA = "gqa"  # Grouped-query attention
    FLASH = "flash"  # Flash attention compatible
    SLIDING_WINDOW = "sliding_window"
    GLOBAL_LOCAL = "global_local"

class PositionalEncodingType(Enum):
    """Positional encoding methods"""
    ROPE = "rope"
    ALIBI = "alibi"
    SINUSOIDAL = "sinusoidal"
    RELATIVE = "relative"
    NONE = "none"

class TokenizerType(Enum):
    """Tokenizer types"""
    UNIGRAM = "unigram"
    BPE = "bpe"
    SENTENCEPIECE = "sentencepiece"
    WORDPIECE = "wordpiece"
    CHAR = "char"

class WeightFormat(Enum):
    """Weight storage formats"""
    SAFETENSORS = "safetensors"
    PYTORCH = "pytorch"
    NUMPY = "numpy"
    GGUF = "gguf"

class BackendTarget(Enum):
    """Backend compilation targets"""
    PYTORCH_TRAINING = "pytorch_training"
    PYTORCH_INFERENCE = "pytorch_inference"
    TORCHSCRIPT = "torchscript"
    ONNX = "onnx"
    JAX = "jax"

@dataclass
class LLM:
    """
    Complete LLM specification.
    
    All fields must be explicitly set - no automatic defaults.
    The system will validate and solve for missing dimensions.
    """
    # Template selection
    template: str
    
    # Size specification (choose one)
    target_params: Optional[int] = None
    explicit_dims: Optional[Dict[str, int]] = None
    
    # Core dimensions
    vocab_size: int
    context_length: int
    
    # Architecture choices
    attention: AttentionType = AttentionType.GQA
    norm: NormType = NormType.RMSNORM
    activation: ActivationType = ActivationType.SWIGLU
    positional_encoding: PositionalEncodingType = PositionalEncodingType.ROPE
    
    # Tokenizer
    tokenizer: TokenizerType = TokenizerType.UNIGRAM
    
    # Output format
    weight_format: WeightFormat = WeightFormat.SAFETENSORS
    backend: BackendTarget = BackendTarget.PYTORCH_TRAINING
    
    # Optional advanced settings
    rope_theta: float = 10000.0
    rope_scaling_factor: Optional[float] = None
    alibi_max_bias: float = 8.0
    sliding_window_size: Optional[int] = None
    tie_word_embeddings: bool = True
    gradient_checkpointing: bool = False
    precision: str = "float32"
    
    # Quantization (optional)
    quantize: Optional[str] = None
    quantize_bits: int = 16
    
    # Validation flag
    _validated: bool = field(default=False, init=False, repr=False)
    
    def __post_init__(self):
        """Validate basic constraints after initialization"""
        if self.target_params is None and self.explicit_dims is None:
            raise ValueError("Must specify either target_params or explicit_dims")
        
        if self.target_params is not None and self.explicit_dims is not None:
            raise ValueError("Cannot specify both target_params and explicit_dims")
        
        if self.vocab_size <= 0:
            raise ValueError(f"vocab_size must be positive, got {self.vocab_size}")
        
        if self.context_length <= 0:
            raise ValueError(f"context_length must be positive, got {self.context_length}")
        
        # Validate attention window if specified
        if (self.attention == AttentionType.SLIDING_WINDOW and 
            self.sliding_window_size is None):
            raise ValueError("sliding_window_size must be specified for sliding window attention")
        
        if (self.sliding_window_size is not None and 
            self.sliding_window_size > self.context_length):
            raise ValueError(f"sliding_window_size ({self.sliding_window_size}) "
                           f"exceeds context_length ({self.context_length})")
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert spec to dictionary for serialization"""
        data = {}
        for field in self.__dataclass_fields__.values():
            value = getattr(self, field.name)
            if isinstance(value, Enum):
                data[field.name] = value.value
            else:
                data[field.name] = value
        return data
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'LLM':
        """Create spec from dictionary"""
        # Convert string enums back to Enum instances
        for field_name, field_type in cls.__dataclass_fields__.items():
            if field_name in data and hasattr(field_type.type, '__origin__'):
                if field_type.type.__origin__ == Union:
                    # Handle Optional types
                    for possible_type in field_type.type.__args__:
                        if hasattr(possible_type, '_member_names_'):
                            # This is an Enum type
                            if isinstance(data[field_name], str):
                                data[field_name] = possible_type(data[field_name])
                            break
                elif hasattr(field_type.type, '_member_names_'):
                    # This is an Enum type
                    if isinstance(data[field_name], str):
                        data[field_name] = field_type.type(data[field_name])
        
        return cls(**data)
ðŸ“„ ./llm_compiler/utils/math_utils.py
=============================
"""
Math Utilities
==============

Mathematical utilities for constraint solving and dimension calculations.
"""

import math
from typing import List, Tuple, Optional

def round_to_multiple(x: int, multiple: int) -> int:
    """Round integer to nearest multiple"""
    return ((x + multiple // 2) // multiple) * multiple

def find_divisors(n: int, min_val: int = 1, max_val: int = None) -> List[int]:
    """Find divisors of n within range"""
    divisors = []
    for i in range(1, int(math.sqrt(n)) + 1):
        if n % i == 0:
            if min_val <= i <= (max_val or n):
                divisors.append(i)
            if i != n // i and min_val <= n // i <= (max_val or n):
                divisors.append(n // i)
    return sorted(divisors)

def find_closest_divisor(n: int, target: int) -> int:
    """Find divisor of n closest to target"""
    divisors = find_divisors(n)
    if not divisors:
        return 1
    return min(divisors, key=lambda x: abs(x - target))

def solve_quadratic(a: float, b: float, c: float) -> Tuple[Optional[float], Optional[float]]:
    """Solve quadratic equation"""
    discriminant = b**2 - 4*a*c
    if discriminant < 0:
        return None, None
    sqrt_disc = math.sqrt(discriminant)
    x1 = (-b + sqrt_disc) / (2*a)
    x2 = (-b - sqrt_disc) / (2*a)
    return x1, x2

def estimate_transformer_params(
    num_layers: int,
    hidden_size: int,
    intermediate_size: int,
    vocab_size: int,
    num_heads: int,
    num_kv_heads: int,
    tie_embeddings: bool = True
) -> int:
    """Estimate transformer parameters"""
    # Embeddings
    params = vocab_size * hidden_size
    
    # Per layer
    per_layer = 0
    
    # Attention
    head_dim = hidden_size // num_heads
    q_size = num_heads * head_dim
    k_size = num_kv_heads * head_dim
    v_size = num_kv_heads * head_dim
    
    per_layer += hidden_size * (q_size + k_size + v_size)  # QKV
    per_layer += (num_heads * head_dim) * hidden_size  # Output
    
    # MLP (SwiGLU)
    per_layer += 2 * hidden_size * intermediate_size  # gate/up
    per_layer += intermediate_size * hidden_size  # down
    
    # Norms (RMSNorm - scale only)
    per_layer += 2 * hidden_size
    
    # Total layers
    params += per_layer * num_layers
    
    # Output layer (if not tied)
    if not tie_embeddings:
        params += hidden_size * vocab_size
    
    return params

def solve_for_hidden_size(
    target_params: int,
    num_layers: int,
    vocab_size: int,
    intermediate_multiplier: float = 2.6875,
    num_heads: int = 32,
    num_kv_heads: int = 8,
    tie_embeddings: bool = True
) -> int:
    """
    Solve for hidden size given target parameters.
    
    Based on transformer parameter formula:
    params = vocab * hidden + num_layers * (
        hidden * (hidden * (3 + 1)) +  # QKV + output
        2 * hidden * intermediate +    # MLP gate/up
        intermediate * hidden +        # MLP down
        2 * hidden                     # Norms
    )
    + (not tied) * hidden * vocab
    """
    # Intermediate size
    def intermediate(hidden):
        return int(round(hidden * intermediate_multiplier / 32) * 32)
    
    # Solve quadratic equation
    # Derived from parameter formula
    head_dim = 128  # assumption
    
    # Coefficients
    a = num_layers * (
        4 +  # QKV + output
        3 * intermediate_multiplier  # MLP
    )
    
    if not tie_embeddings:
        a += vocab_size
    
    b = 2 * num_layers  # Norms
    c = -target_params
    
    # Solve
    hidden = (-b + math.sqrt(b**2 - 4*a*c)) / (2*a)
    
    return int(round(hidden))

def validate_dimensions(
    hidden_size: int,
    num_heads: int,
    num_kv_heads: int
) -> List[str]:
    """Validate dimension constraints"""
    errors = []
    
    if hidden_size % num_heads != 0:
        errors.append(f"hidden_size ({hidden_size}) not divisible by num_heads ({num_heads})")
    
    if hidden_size % num_kv_heads != 0:
        errors.append(f"hidden_size ({hidden_size}) not divisible by num_kv_heads ({num_kv_heads})")
    
    if num_heads % num_kv_heads != 0:
        errors.append(f"num_heads ({num_heads}) not divisible by num_kv_heads ({num_kv_heads})")
    
    return errors
ðŸ“„ ./llm_compiler/utils/parameters.py
=============================
"""
Parameter Accounting
====================

Single source of truth for parameter counting derived from the IR graph.
All other components (solver estimates, emitters, validators) should
cross-check against this to avoid silent divergence.
"""

from __future__ import annotations
from typing import Dict

from ..ir.graph import IRGraph, NodeType


def count_parameters_from_ir(graph: IRGraph) -> int:
    """
    Count trainable parameters directly from the IR graph.

    Currently supports the node types emitted by the decoder-only template.
    Nodes without parameters (add, activation, attention kernels, etc.)
    contribute zero.
    """
    params = 0

    for op in graph.operations.values():
        if op.node_type == NodeType.LINEAR:
            in_features = op.attributes.get("in_features")
            out_features = op.attributes.get("out_features")
            use_bias = op.attributes.get("use_bias", True)
            if in_features and out_features:
                params += in_features * out_features
                if use_bias:
                    params += out_features

        elif op.node_type == NodeType.EMBEDDING:
            vocab_size = op.attributes.get("vocab_size")
            embedding_dim = op.attributes.get("embedding_dim")
            if vocab_size and embedding_dim:
                params += vocab_size * embedding_dim

        elif op.node_type in (NodeType.LAYERNORM, NodeType.RMSNORM):
            normalized_shape = op.attributes.get("normalized_shape")
            if normalized_shape:
                # LayerNorm has weight and bias; RMSNorm has only weight
                if op.node_type == NodeType.LAYERNORM:
                    params += 2 * normalized_shape
                else:
                    params += normalized_shape

        # Multi-head attention parameters are captured by their constituent
        # linear projections in the current IR; nothing to add here.

    return params

ðŸ“„ ./llm_compiler/utils/validation.py
=============================
"""
Validation Utilities
====================

Validation functions for specifications and generated models.
"""

from typing import Dict, Any, List, Tuple
import json
from pathlib import Path

from ..spec import LLM, NormType, AttentionType, ActivationType
from ..tokenizers.base import TokenizerTrainingStats
from ..datasets.spec import DatasetSpec

def validate_spec(
    spec: LLM,
    tokenizer_stats: TokenizerTrainingStats | None = None,
    dataset: DatasetSpec | None = None,
) -> Tuple[bool, List[str]]:
    """
    Validate LLM specification.
    
    Returns:
        (is_valid, errors)
    """
    errors = []
    
    # Check template exists
    from ..templates.registry import registry
    try:
        registry.get(spec.template)
    except ValueError:
        errors.append(f"Unknown template: {spec.template}")
    
    # Check parameter specification
    if spec.target_params is None and spec.explicit_dims is None:
        errors.append("Must specify either target_params or explicit_dims")
    
    if spec.target_params is not None and spec.target_params <= 0:
        errors.append(f"target_params must be positive: {spec.target_params}")
    
    # Check vocab size
    if spec.vocab_size <= 0:
        errors.append(f"vocab_size must be positive: {spec.vocab_size}")
    
    # Check context length
    if spec.context_length <= 0:
        errors.append(f"context_length must be positive: {spec.context_length}")
    
    # Check attention type
    if spec.attention not in AttentionType:
        errors.append(f"Invalid attention type: {spec.attention}")
    
    # Check norm type
    if spec.norm not in NormType:
        errors.append(f"Invalid norm type: {spec.norm}")
    
    # Check activation
    if spec.activation not in ActivationType:
        errors.append(f"Invalid activation type: {spec.activation}")
    
    # Check positional encoding
    from ..spec import PositionalEncodingType
    if spec.positional_encoding not in PositionalEncodingType:
        errors.append(f"Invalid positional encoding: {spec.positional_encoding}")
    
    # Check tokenizer
    from ..spec import TokenizerType
    if spec.tokenizer not in TokenizerType:
        errors.append(f"Invalid tokenizer: {spec.tokenizer}")
    else:
        # Prefer dataset-provided stats if present
        effective_stats = tokenizer_stats
        if dataset and dataset.tokenizer_stats:
            effective_stats = dataset.tokenizer_stats

        if effective_stats is not None:
            if effective_stats.unique_tokens < spec.vocab_size:
                errors.append(
                    f"vocab_size ({spec.vocab_size}) exceeds unique tokens in tokenizer stats "
                    f"({effective_stats.unique_tokens})"
                )
    
    # Check weight format
    from ..spec import WeightFormat
    if spec.weight_format not in WeightFormat:
        errors.append(f"Invalid weight format: {spec.weight_format}")
    
    # Check backend
    from ..spec import BackendTarget
    if spec.backend not in BackendTarget:
        errors.append(f"Invalid backend: {spec.backend}")
    
    # Check precision
    if spec.precision not in ['float32', 'float16', 'bfloat16']:
        errors.append(f"Invalid precision: {spec.precision}")
    
    # Check RoPE scaling
    if (spec.positional_encoding == PositionalEncodingType.ROPE and 
        spec.rope_scaling_factor is not None and
        spec.rope_scaling_factor <= 0):
        errors.append(f"rope_scaling_factor must be positive: {spec.rope_scaling_factor}")
    
    # Check sliding window
    if (spec.attention == AttentionType.SLIDING_WINDOW and
        spec.sliding_window_size is None):
        errors.append("sliding_window_size required for sliding_window attention")
    
    if (spec.sliding_window_size is not None and
        spec.sliding_window_size > spec.context_length):
        errors.append(f"sliding_window_size ({spec.sliding_window_size}) "
                     f"exceeds context_length ({spec.context_length})")
    
    return len(errors) == 0, errors

def validate_generated_model(model_dir: Path) -> Tuple[bool, List[str]]:
    """
    Validate generated model files.
    
    Args:
        model_dir: Directory containing generated model
        
    Returns:
        (is_valid, errors)
    """
    errors = []
    
    # Check required files
    required_files = [
        "spec.json",
        "solution.json",
        "compilation_report.json",
        "model/__init__.py",
        "model/config.py",
    ]
    
    for file in required_files:
        if not (model_dir / file).exists():
            errors.append(f"Missing required file: {file}")
    
    # Check spec and solution consistency
    try:
        spec_path = model_dir / "spec.json"
        solution_path = model_dir / "solution.json"
        ir_path = model_dir / "ir_graph.json"
        
        spec = json.loads(spec_path.read_text())
        solution = json.loads(solution_path.read_text())
        
        # Check parameter count matches
        if 'target_params' in spec:
            target = spec['target_params']
            actual = solution.get('parameters', 0)
            
            if abs(actual - target) / target > 0.01:  # 1% tolerance
                errors.append(f"Parameter mismatch: target={target:,}, actual={actual:,}")

        # Cross-validate against IR parameter count if available
        if ir_path.exists():
            from ..ir.graph import IRGraph
            from ..utils.parameters import count_parameters_from_ir
            ir_graph = IRGraph.from_json(ir_path.read_text())
            ir_params = count_parameters_from_ir(ir_graph)
            if 'parameters' in solution and solution['parameters'] != ir_params:
                errors.append(
                    f"IR parameter count {ir_params:,} disagrees with solution {solution['parameters']:,}"
                )
    
    except Exception as e:
        errors.append(f"Error reading spec/solution: {e}")
    
    # Check model code compiles
    try:
        model_files = list((model_dir / "model").glob("*.py"))
        if not model_files:
            errors.append("No Python model files found")
    except:
        errors.append("Model directory not found")
    
    return len(errors) == 0, errors

def validate_ir_graph(graph_file: Path) -> Tuple[bool, List[str]]:
    """
    Validate IR graph file.
    
    Args:
        graph_file: IR graph JSON file
        
    Returns:
        (is_valid, errors)
    """
    errors = []
    
    try:
        from ..ir.graph import IRGraph
        graph = IRGraph.from_json(graph_file.read_text())
        
        # Run graph validation
        graph_errors = graph.validate()
        errors.extend(graph_errors)
        
        # Check for cycles (simplified)
        # In full implementation would do topological sort
        
    except Exception as e:
        errors.append(f"Error loading IR graph: {e}")
    
    return len(errors) == 0, errors

ðŸ“„ ./llm_compiler/solver/constraints.py
=============================
"""
Constraint System
================

Implements constraint solving for architecture dimensions.
Handles equality, inequality, divisibility, and range constraints.
"""

from __future__ import annotations
import math
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass
from enum import Enum
import sympy as sp
from sympy import symbols, Eq, Le, Ge, Mod, solve

class ConstraintType(Enum):
    EQUALITY = "equality"
    INEQUALITY = "inequality"
    DIVISIBILITY = "divisibility"
    RANGE = "range"
    LINEAR = "linear"

@dataclass
class Constraint:
    """Base constraint class"""
    name: str
    constraint_type: ConstraintType
    
    def to_sympy(self) -> List[sp.Basic]:
        """Convert to sympy expressions"""
        raise NotImplementedError
    
    def __str__(self) -> str:
        return f"{self.name}: {self.constraint_type}"

@dataclass
class EqualityConstraint(Constraint):
    """Equality constraint: var1 == var2 or var == value"""
    var1: str
    var2: str  # Can be another variable or a constant expression
    
    def __post_init__(self):
        self.constraint_type = ConstraintType.EQUALITY
    
    def to_sympy(self) -> List[sp.Basic]:
        var1_sym = symbols(self.var1)
        
        # Try to parse var2 as expression
        try:
            # Check if var2 is a simple number
            if self.var2.replace('.', '').replace('-', '').isdigit():
                var2_val = float(self.var2) if '.' in self.var2 else int(self.var2)
                return [Eq(var1_sym, var2_val)]
            
            # Check if var2 is another variable
            if self.var2.isidentifier():
                var2_sym = symbols(self.var2)
                return [Eq(var1_sym, var2_sym)]
            
            # Try to parse as expression
            # Simple expression parser - in practice would use sympy parsing
            if '*' in self.var2:
                parts = self.var2.split('*')
                if len(parts) == 2:
                    coef, var = parts
                    coef = float(coef.strip())
                    var_sym = symbols(var.strip())
                    return [Eq(var1_sym, coef * var_sym)]
            
        except:
            pass
        
        # Default: treat as variable
        var2_sym = symbols(self.var2)
        return [Eq(var1_sym, var2_sym)]
    
    def __str__(self) -> str:
        return f"{self.name}: {self.var1} == {self.var2}"

@dataclass
class InequalityConstraint(Constraint):
    """Inequality constraint: var1 <= var2 or var1 >= var2"""
    var1: str
    var2: str
    greater_than: bool = False  # True for >=, False for <=
    
    def __post_init__(self):
        self.constraint_type = ConstraintType.INEQUALITY
    
    def to_sympy(self) -> List[sp.Basic]:
        var1_sym = symbols(self.var1)
        
        try:
            if self.var2.replace('.', '').replace('-', '').isdigit():
                var2_val = float(self.var2) if '.' in self.var2 else int(self.var2)
                if self.greater_than:
                    return [Ge(var1_sym, var2_val)]
                else:
                    return [Le(var1_sym, var2_val)]
        except:
            pass
        
        var2_sym = symbols(self.var2)
        if self.greater_than:
            return [Ge(var1_sym, var2_sym)]
        else:
            return [Le(var1_sym, var2_sym)]
    
    def __str__(self) -> str:
        op = ">=" if self.greater_than else "<="
        return f"{self.name}: {self.var1} {op} {self.var2}"

@dataclass  
class DivisibilityConstraint(Constraint):
    """Divisibility constraint: var1 % var2 == 0"""
    var1: str
    var2: str
    
    def __post_init__(self):
        self.constraint_type = ConstraintType.DIVISIBILITY
    
    def to_sympy(self) -> List[sp.Basic]:
        var1_sym = symbols(self.var1)
        
        try:
            if self.var2.replace('.', '').replace('-', '').isdigit():
                var2_val = int(self.var2)
                return [Eq(Mod(var1_sym, var2_val), 0)]
        except:
            pass
        
        var2_sym = symbols(self.var2)
        return [Eq(Mod(var1_sym, var2_sym), 0)]
    
    def __str__(self) -> str:
        return f"{self.name}: {self.var1} % {self.var2} == 0"

@dataclass
class RangeConstraint(Constraint):
    """Range constraint: min <= var <= max"""
    var: str
    min_val: Optional[Union[int, float, str]] = None
    max_val: Optional[Union[int, float, str]] = None
    
    def __post_init__(self):
        self.constraint_type = ConstraintType.RANGE
    
    def to_sympy(self) -> List[sp.Basic]:
        var_sym = symbols(self.var)
        constraints = []
        
        if self.min_val is not None:
            try:
                if isinstance(self.min_val, str) and self.min_val.replace('.', '').replace('-', '').isdigit():
                    min_val = float(self.min_val) if '.' in self.min_val else int(self.min_val)
                    constraints.append(Ge(var_sym, min_val))
                elif isinstance(self.min_val, (int, float)):
                    constraints.append(Ge(var_sym, self.min_val))
                else:
                    min_sym = symbols(self.min_val)
                    constraints.append(Ge(var_sym, min_sym))
            except:
                pass
        
        if self.max_val is not None:
            try:
                if isinstance(self.max_val, str) and self.max_val.replace('.', '').replace('-', '').isdigit():
                    max_val = float(self.max_val) if '.' in self.max_val else int(self.max_val)
                    constraints.append(Le(var_sym, max_val))
                elif isinstance(self.max_val, (int, float)):
                    constraints.append(Le(var_sym, self.max_val))
                else:
                    max_sym = symbols(self.max_val)
                    constraints.append(Le(var_sym, max_sym))
            except:
                pass
        
        return constraints
    
    def __str__(self) -> str:
        parts = [self.var]
        if self.min_val is not None:
            parts.insert(0, f"{self.min_val} <=")
        if self.max_val is not None:
            parts.append(f"<= {self.max_val}")
        return f"{self.name}: {' '.join(parts)}"

@dataclass
class LinearConstraint(Constraint):
    """Linear constraint: a*x + b*y + ... == c"""
    coefficients: Dict[str, float]  # variable -> coefficient
    constant: float
    equality: bool = True  # True for ==, False for <=
    
    def __post_init__(self):
        self.constraint_type = ConstraintType.LINEAR
    
    def to_sympy(self) -> List[sp.Basic]:
        expr = 0
        for var, coeff in self.coefficients.items():
            var_sym = symbols(var)
            expr += coeff * var_sym
        
        if self.equality:
            return [Eq(expr, self.constant)]
        else:
            return [Le(expr, self.constant)]
    
    def __str__(self) -> str:
        terms = []
        for var, coeff in self.coefficients.items():
            if coeff == 1:
                terms.append(var)
            elif coeff == -1:
                terms.append(f"-{var}")
            else:
                terms.append(f"{coeff}*{var}")
        
        expr = " + ".join(terms).replace("+ -", "- ")
        op = "==" if self.equality else "<="
        return f"{self.name}: {expr} {op} {self.constant}"

class ConstraintSystem:
    """System of constraints to solve"""
    
    def __init__(self):
        self.constraints: List[Constraint] = []
        self.variables: Dict[str, Any] = {}
    
    def add_constraint(self, constraint: Constraint):
        """Add a constraint to the system"""
        self.constraints.append(constraint)
        
        # Extract variables from constraint
        if isinstance(constraint, EqualityConstraint):
            self._add_variable(constraint.var1)
            if constraint.var2.isidentifier():
                self._add_variable(constraint.var2)
        elif isinstance(constraint, InequalityConstraint):
            self._add_variable(constraint.var1)
            if constraint.var2.isidentifier():
                self._add_variable(constraint.var2)
        elif isinstance(constraint, DivisibilityConstraint):
            self._add_variable(constraint.var1)
            self._add_variable(constraint.var2)
        elif isinstance(constraint, RangeConstraint):
            self._add_variable(constraint.var)
        elif isinstance(constraint, LinearConstraint):
            for var in constraint.coefficients.keys():
                self._add_variable(var)
    
    def _add_variable(self, var: str):
        """Add variable to tracking dict"""
        if var not in self.variables:
            self.variables[var] = {
                'min': None,
                'max': None,
                'fixed': False,
                'value': None
            }
    
    def solve(self, fixed_vars: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Solve constraint system.
        
        Args:
            fixed_vars: Variables with fixed values
            
        Returns:
            Dictionary of variable values
        """
        if fixed_vars:
            for var, value in fixed_vars.items():
                self._add_variable(var)
                self.variables[var]['fixed'] = True
                self.variables[var]['value'] = value
        
        # Convert to sympy
        sympy_constraints = []
        variables_set = set()
        
        for constraint in self.constraints:
            sympy_exprs = constraint.to_sympy()
            sympy_constraints.extend(sympy_exprs)
            
            # Collect variables
            for expr in sympy_exprs:
                variables_set.update(expr.free_symbols)
        
        # Create symbol list
        variables = list(variables_set)
        
        # Try to solve
        try:
            solution = solve(sympy_constraints, variables, dict=True)
            
            if not solution:
                raise ValueError("No solution found")
            
            # Convert to simple dict
            result = {}
            for sol in solution:
                for var, value in sol.items():
                    result[str(var)] = float(value) if value.is_Float else int(value)
            
            return result
            
        except Exception as e:
            # Fall back to iterative solving
            return self._solve_iteratively()
    
    def _solve_iteratively(self) -> Dict[str, Any]:
        """Iterative constraint solving"""
        # Initialize with reasonable defaults
        solution = {}
        for var in self.variables:
            if self.variables[var]['fixed']:
                solution[var] = self.variables[var]['value']
            else:
                # Set to midpoint of bounds if available
                if (self.variables[var]['min'] is not None and 
                    self.variables[var]['max'] is not None):
                    min_val = self.variables[var]['min']
                    max_val = self.variables[var]['max']
                    solution[var] = (min_val + max_val) // 2
                else:
                    solution[var] = 1  # Default
        
        # Apply constraints iteratively
        changed = True
        max_iter = 100
        iteration = 0
        
        while changed and iteration < max_iter:
            changed = False
            iteration += 1
            
            for constraint in self.constraints:
                if isinstance(constraint, EqualityConstraint):
                    result = self._apply_equality(constraint, solution)
                    if result:
                        solution.update(result)
                        changed = True
                
                elif isinstance(constraint, DivisibilityConstraint):
                    result = self._apply_divisibility(constraint, solution)
                    if result:
                        solution.update(result)
                        changed = True
                
                elif isinstance(constraint, RangeConstraint):
                    result = self._apply_range(constraint, solution)
                    if result:
                        solution.update(result)
                        changed = True
        
        return solution
    
    def _apply_equality(self, constraint: EqualityConstraint, solution: Dict[str, Any]) -> Dict[str, Any]:
        """Apply equality constraint"""
        updates = {}
        
        # Check if we can evaluate
        var1 = constraint.var1
        var2 = constraint.var2
        
        # Try to parse var2 as number
        try:
            if var2.replace('.', '').replace('-', '').isdigit():
                value = float(var2) if '.' in var2 else int(var2)
                updates[var1] = value
                return updates
        except:
            pass
        
        # Check if it's an expression
        if '*' in var2:
            parts = var2.split('*')
            if len(parts) == 2:
                try:
                    coef = float(parts[0].strip())
                    other_var = parts[1].strip()
                    if other_var in solution:
                        updates[var1] = coef * solution[other_var]
                        return updates
                except:
                    pass
        
        # Variable equality
        if var2 in solution and var1 not in solution:
            updates[var1] = solution[var2]
        elif var1 in solution and var2 not in solution:
            updates[var2] = solution[var1]
        
        return updates
    
    def _apply_divisibility(self, constraint: DivisibilityConstraint, solution: Dict[str, Any]) -> Dict[str, Any]:
        """Apply divisibility constraint"""
        updates = {}
        var1 = constraint.var1
        var2 = constraint.var2
        
        # Try to get divisor value
        divisor = None
        try:
            if var2.replace('.', '').replace('-', '').isdigit():
                divisor = int(var2)
        except:
            if var2 in solution:
                divisor = solution[var2]
        
        if divisor is None:
            return updates
        
        # Check if var1 needs adjustment
        if var1 in solution:
            value = solution[var1]
            if value % divisor != 0:
                # Round up to next multiple
                updates[var1] = ((value + divisor - 1) // divisor) * divisor
        
        return updates
    
    def _apply_range(self, constraint: RangeConstraint, solution: Dict[str, Any]) -> Dict[str, Any]:
        """Apply range constraint"""
        updates = {}
        var = constraint.var
        
        if var not in solution:
            return updates
        
        value = solution[var]
        
        # Apply min bound
        if constraint.min_val is not None:
            try:
                if isinstance(constraint.min_val, (int, float)):
                    min_val = constraint.min_val
                elif constraint.min_val.replace('.', '').replace('-', '').isdigit():
                    min_val = float(constraint.min_val) if '.' in constraint.min_val else int(constraint.min_val)
                else:
                    min_val = solution.get(constraint.min_val, value)
                
                if value < min_val:
                    updates[var] = min_val
            except:
                pass
        
        # Apply max bound
        if constraint.max_val is not None:
            try:
                if isinstance(constraint.max_val, (int, float)):
                    max_val = constraint.max_val
                elif constraint.max_val.replace('.', '').replace('-', '').isdigit():
                    max_val = float(constraint.max_val) if '.' in constraint.max_val else int(constraint.max_val)
                else:
                    max_val = solution.get(constraint.max_val, value)
                
                if value > max_val:
                    updates[var] = max_val
            except:
                pass
        
        return updates
    
    def __str__(self) -> str:
        lines = ["Constraint System:"]
        for constraint in self.constraints:
            lines.append(f"  {constraint}")
        return "\n".join(lines)
ðŸ“„ ./llm_compiler/solver/param_solver.py
=============================
"""
Parameter Solver
===============

Solves for architecture dimensions given parameter targets and constraints.
"""

from typing import Dict, Any, List, Tuple, Optional
import math
from dataclasses import dataclass

from .constraints import ConstraintSystem
from ..templates.base import ArchitectureTemplate, TemplateParameter
from ..utils.math_utils import find_divisors, round_to_multiple

@dataclass
class Solution:
    """Complete solution with dimensions and validation"""
    dimensions: Dict[str, int]
    actual_params: int
    target_params: Optional[int]
    template_name: str
    constraints_satisfied: bool
    warnings: List[str]
    errors: List[str]

class ParameterSolver:
    """Solves for architecture dimensions"""
    
    def __init__(self):
        self.solutions_tried = 0
        self.max_solutions = 1000
    
    def solve(self, 
              template: ArchitectureTemplate,
              spec: Dict[str, Any]) -> Solution:
        """
        Solve for dimensions given specification.
        
        Args:
            template: Architecture template
            spec: User specification
            
        Returns:
            Solution with dimensions
        """
        # Create constraint system from template
        constraint_system = template.create_constraint_system(spec)
        
        # Get fixed variables from spec
        fixed_vars = self._extract_fixed_variables(spec, template)
        
        # Try to solve
        try:
            # First try symbolic solving
            dimensions = constraint_system.solve(fixed_vars)
            
            # Ensure all required dimensions are present
            dimensions = self._ensure_complete_dimensions(dimensions, template, spec)
            
            # Round to integers
            dimensions = {k: int(round(v)) for k, v in dimensions.items()}
            
            # Apply template-specific adjustments
            dimensions = self._apply_template_adjustments(dimensions, template, spec)
            
            # Calculate actual parameters
            actual_params = template.calculate_parameters(dimensions)
            
            # Validate against target
            target_params = spec.get('target_params')
            warnings = []
            errors = []
            
            if target_params is not None:
                tolerance = 0.01  # 1% tolerance
                relative_diff = abs(actual_params - target_params) / target_params
                
                if relative_diff > tolerance:
                    warnings.append(
                        f"Parameter count mismatch: "
                        f"target={target_params:,}, actual={actual_params:,} "
                        f"(diff={relative_diff:.1%})"
                    )
            
            # Check constraints
            constraints_satisfied = self._check_constraints(dimensions, constraint_system)
            
            if not constraints_satisfied:
                warnings.append("Some constraints may not be fully satisfied")
            
            return Solution(
                dimensions=dimensions,
                actual_params=actual_params,
                target_params=target_params,
                template_name=template.info.name,
                constraints_satisfied=constraints_satisfied,
                warnings=warnings,
                errors=errors
            )
            
        except Exception as e:
            # Try iterative search
            return self._solve_iterative(template, spec, constraint_system, fixed_vars)
    
    def _extract_fixed_variables(self, spec: Dict[str, Any], template: ArchitectureTemplate) -> Dict[str, Any]:
        """Extract fixed variables from spec"""
        fixed = {}
        
        # Core fixed variables
        fixed['vocab_size'] = spec['vocab_size']
        fixed['context_length'] = spec['context_length']
        
        # Any explicit dimensions
        if 'explicit_dims' in spec:
            fixed.update(spec['explicit_dims'])
        
        return fixed
    
    def _ensure_complete_dimensions(self, 
                                   dimensions: Dict[str, Any],
                                   template: ArchitectureTemplate,
                                   spec: Dict[str, Any]) -> Dict[str, Any]:
        """Ensure all required dimensions are present"""
        required = {p.value for p in template.info.required_parameters}
        
        # Add any missing required dimensions with reasonable defaults
        for param in required:
            if param not in dimensions:
                if param == 'num_layers':
                    dimensions[param] = 32  # Reasonable default
                elif param == 'hidden_size':
                    dimensions[param] = 4096  # Reasonable default
                elif param == 'intermediate_size':
                    # Default to SwiGLU expansion
                    hidden = dimensions.get('hidden_size', 4096)
                    dimensions[param] = int(2 * round(hidden * 8/3 / 32) * 32)
                elif param == 'num_heads':
                    hidden = dimensions.get('hidden_size', 4096)
                    dimensions[param] = hidden // 128  # Default head_dim=128
                elif param == 'num_kv_heads':
                    # Default to GQA with groups of 8
                    num_heads = dimensions.get('num_heads', 32)
                    dimensions[param] = max(1, num_heads // 8)
                elif param == 'head_dim':
                    hidden = dimensions.get('hidden_size', 4096)
                    num_heads = dimensions.get('num_heads', 32)
                    dimensions[param] = hidden // num_heads
        
        return dimensions
    
    def _apply_template_adjustments(self,
                                   dimensions: Dict[str, int],
                                   template: ArchitectureTemplate,
                                   spec: Dict[str, Any]) -> Dict[str, int]:
        """Apply template-specific adjustments to dimensions"""
        
        # Ensure divisibility constraints
        hidden_size = dimensions.get('hidden_size')
        num_heads = dimensions.get('num_heads')
        num_kv_heads = dimensions.get('num_kv_heads')
        
        if hidden_size and num_heads:
            if hidden_size % num_heads != 0:
                # Adjust hidden_size to be divisible
                dimensions['hidden_size'] = ((hidden_size + num_heads - 1) // num_heads) * num_heads
        
        if hidden_size and num_kv_heads:
            if hidden_size % num_kv_heads != 0:
                # Adjust hidden_size to be divisible
                dimensions['hidden_size'] = ((hidden_size + num_kv_heads - 1) // num_kv_heads) * num_kv_heads
        
        # Ensure num_heads divisible by num_kv_heads for GQA
        if num_heads and num_kv_heads:
            if spec.get('attention') == 'gqa':
                if num_heads % num_kv_heads != 0:
                    # Adjust kv_heads to be divisor
                    divisors = find_divisors(num_heads)
                    if divisors:
                        # Find closest divisor
                        closest = min(divisors, key=lambda x: abs(x - num_kv_heads))
                        dimensions['num_kv_heads'] = closest
        
        # Ensure intermediate size matches activation
        if spec.get('activation') == 'swiglu':
            hidden = dimensions.get('hidden_size', 4096)
            # SwiGLU formula: 2 * round(8/3 * hidden / 32) * 32
            intermediate = int(2 * round(hidden * 8/3 / 32) * 32)
            dimensions['intermediate_size'] = intermediate
        
        return dimensions
    
    def _check_constraints(self, dimensions: Dict[str, int], constraint_system) -> bool:
        """Check if dimensions satisfy all constraints"""
        # Simplified check - in practice would evaluate each constraint
        return True
    
    def _solve_iterative(self,
                        template: ArchitectureTemplate,
                        spec: Dict[str, Any],
                        constraint_system,
                        fixed_vars: Dict[str, Any]) -> Solution:
        """Iterative search for solution"""
        
        target_params = spec.get('target_params')
        if target_params is None:
            raise ValueError("Iterative solving requires target_params")
        
        # Define search ranges
        search_ranges = self._define_search_ranges(template, spec, fixed_vars)
        
        # Try different combinations
        best_solution = None
        best_error = float('inf')
        
        # Simplified search - in practice would be more sophisticated
        for num_layers in range(search_ranges['num_layers'][0], 
                               search_ranges['num_layers'][1] + 1, 
                               search_ranges['num_layers'][2]):
            for hidden_size in range(search_ranges['hidden_size'][0],
                                    search_ranges['hidden_size'][1] + 1,
                                    search_ranges['hidden_size'][2]):
                
                dimensions = {
                    'num_layers': num_layers,
                    'hidden_size': hidden_size,
                    **fixed_vars
                }
                
                # Complete dimensions
                dimensions = self._ensure_complete_dimensions(dimensions, template, spec)
                dimensions = self._apply_template_adjustments(dimensions, template, spec)
                
                # Calculate parameters
                actual_params = template.calculate_parameters(dimensions)
                
                # Check error
                error = abs(actual_params - target_params) / target_params
                
                if error < best_error:
                    best_error = error
                    best_solution = {
                        'dimensions': dimensions,
                        'actual_params': actual_params,
                        'error': error
                    }
                
                self.solutions_tried += 1
                if self.solutions_tried >= self.max_solutions:
                    break
        
        if best_solution is None:
            raise ValueError("No valid solution found")
        
        return Solution(
            dimensions=best_solution['dimensions'],
            actual_params=best_solution['actual_params'],
            target_params=target_params,
            template_name=template.info.name,
            constraints_satisfied=True,
            warnings=[f"Approximate solution found (error={best_error:.1%})"],
            errors=[]
        )
    
    def _define_search_ranges(self, template, spec, fixed_vars):
        """Define reasonable search ranges for iterative solving"""
        # Based on typical LLM dimensions
        ranges = {
            'num_layers': (12, 80, 4),  # min, max, step
            'hidden_size': (2048, 16384, 512),
            'num_heads': (16, 128, 8),
        }
        
        # Adjust based on target params
        target = spec.get('target_params', 7000000000)
        
        if target < 1000000000:  # < 1B
            ranges['num_layers'] = (8, 32, 4)
            ranges['hidden_size'] = (1024, 4096, 256)
        elif target < 7000000000:  # < 7B
            ranges['num_layers'] = (24, 40, 4)
            ranges['hidden_size'] = (3072, 8192, 512)
        elif target < 13000000000:  # < 13B
            ranges['num_layers'] = (36, 48, 4)
            ranges['hidden_size'] = (5120, 10240, 512)
        else:  # > 13B
            ranges['num_layers'] = (40, 80, 4)
            ranges['hidden_size'] = (8192, 16384, 512)
        
        return ranges
ðŸ“„ ./llm_compiler/compile.py
=============================
"""
Main Compilation Entry Point
============================

Orchestrates the entire compilation pipeline.
"""

from typing import Dict, Any, Optional, Tuple
import json
from pathlib import Path
import shutil
import sys

from .spec import LLM
from .templates.registry import get_template
from .solver.param_solver import ParameterSolver
from .ir.builder import GraphBuilder
from .emitters.pytorch import PyTorchEmitter
from .emitters.safetensors import SafetensorsEmitter
from .utils.validation import validate_spec
from .utils.parameters import count_parameters_from_ir

class LLMCompiler:
    """Main compiler class"""
    
    def __init__(self, verbose: bool = False):
        self.verbose = verbose
        self.solver = ParameterSolver()
        self.pt_emitter = PyTorchEmitter()
        self.st_emitter = SafetensorsEmitter()
        
    def compile(self, spec: LLM, output_dir: Path, dataset=None) -> Dict[str, Any]:
        """
        Compile LLM from specification.
        
        Args:
            spec: LLM specification
            output_dir: Output directory
            
        Returns:
            Compilation report
        """
        # Create output directory
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Start compilation report
        report = {
            "spec": spec.to_dict(),
            "steps": [],
            "success": False,
            "output_dir": str(output_dir),
            "files": []
        }
        
        try:
            # Step 1: Get template
            self._log("Step 1: Getting template")
            template = get_template(spec.template)
            report["steps"].append({
                "step": "template_selection",
                "template": template.info.name,
                "status": "success"
            })
            
            # Step 2: Validate specification
            self._log("Step 2: Validating specification")
            validation_errors = template.validate_spec(spec.to_dict())
            if validation_errors:
                raise ValueError(f"Specification validation failed: {validation_errors}")
            # Cross-check tokenizer/dataset coupling
            if dataset is not None:
                ds_stats = getattr(dataset, "tokenizer_stats", None)
                _, ds_errors = validate_spec(spec, tokenizer_stats=ds_stats, dataset=dataset)
                if ds_errors:
                    raise ValueError(f"Dataset validation failed: {ds_errors}")
            
            report["steps"].append({
                "step": "spec_validation",
                "status": "success"
            })
            if dataset is not None:
                report["dataset"] = getattr(dataset, "to_dict", lambda: {})()
            
            # Step 3: Solve for dimensions
            self._log("Step 3: Solving architecture dimensions")
            spec_dict = spec.to_dict()
            solution = self.solver.solve(template, spec_dict)
            
            if solution.errors:
                raise ValueError(f"Constraint solving failed: {solution.errors}")
            
            report["solution"] = {
                "dimensions": solution.dimensions,
                "actual_params": solution.actual_params,
                "target_params": solution.target_params,
                "warnings": solution.warnings,
                "constraints_satisfied": solution.constraints_satisfied
            }
            
            report["steps"].append({
                "step": "dimension_solving",
                "status": "success",
                "parameters": solution.actual_params,
                "warnings": solution.warnings
            })
            
            # Step 4: Build IR graph
            self._log("Step 4: Building IR graph")
            builder = GraphBuilder()
            ir_graph = template.build_ir(
                solution.dimensions,
                builder,
                spec_dict
            )
            
            # Validate graph
            graph_errors = ir_graph.validate()
            if graph_errors:
                raise ValueError(f"IR graph validation failed: {graph_errors}")
            
            report["steps"].append({
                "step": "ir_generation",
                "status": "success",
                "nodes": len(ir_graph.operations),
                "tensors": len(ir_graph.tensors)
            })

            # Step 5: Canonical parameter count from IR
            self._log("Step 5: Computing IR parameter count")
            ir_param_count = count_parameters_from_ir(ir_graph)

            # Cross-validate against template estimate
            template_param_estimate = solution.actual_params
            relative_diff = 0.0
            if template_param_estimate:
                relative_diff = abs(ir_param_count - template_param_estimate) / template_param_estimate
                if relative_diff > 0.01:
                    raise ValueError(
                        f"Parameter mismatch between template ({template_param_estimate:,}) "
                        f"and IR ({ir_param_count:,})"
                    )

            # Promote IR count to canonical value
            solution.actual_params = ir_param_count
            solution.dimensions["total_params"] = ir_param_count
            report["solution"]["actual_params"] = ir_param_count
            report["solution"]["param_source"] = "ir_graph"

            report["steps"].append({
                "step": "parameter_accounting",
                "status": "success",
                "parameters_ir": ir_param_count,
                "template_estimate": template_param_estimate,
                "relative_diff": relative_diff
            })
            
            # Step 6: Emit code
            self._log("Step 6: Emitting code")
            
            # Generate model name
            model_name = self._generate_model_name(spec, solution.dimensions)
            
            # Emit PyTorch code
            pt_files = self.pt_emitter.emit(
                ir_graph,
                model_name,
                output_dir / "model"
            )
            
            # Write files
            model_dir = output_dir / "model"
            model_dir.mkdir(exist_ok=True)
            
            for filename, content in pt_files.items():
                file_path = model_dir / filename
                file_path.write_text(content)
                report["files"].append({
                    "path": str(file_path.relative_to(output_dir)),
                    "type": "code",
                    "size": len(content)
                })
            
            # Step 7: Generate weights (placeholder)
            self._log("Step 7: Generating weights structure")
            
            # Create weights directory
            weights_dir = output_dir / "weights"
            weights_dir.mkdir(exist_ok=True)
            
            # Generate weight metadata
            weight_names = self.st_emitter.generate_weight_names(
                model_name,
                solution.dimensions.get('num_layers', 32)
            )
            
            # Create weight manifest
            weight_manifest = {
                "model_name": model_name,
                "total_parameters": solution.actual_params,
                "weights": weight_names,
                "dtype": spec.precision,
                "quantization": spec.quantize
            }
            
            manifest_path = weights_dir / "manifest.json"
            manifest_path.write_text(json.dumps(weight_manifest, indent=2))
            
            report["files"].append({
                "path": str(manifest_path.relative_to(output_dir)),
                "type": "manifest",
                "size": manifest_path.stat().st_size
            })
            
            # Step 8: Generate compilation report
            self._log("Step 8: Generating final report")
            
            # Save spec
            spec_path = output_dir / "spec.json"
            spec_path.write_text(json.dumps(spec.to_dict(), indent=2))
            
            # Save solution
            solution_path = output_dir / "solution.json"
            solution_path.write_text(json.dumps({
                "dimensions": solution.dimensions,
                "parameters": solution.actual_params
            }, indent=2))
            
            # Save IR graph
            ir_path = output_dir / "ir_graph.json"
            ir_path.write_text(ir_graph.to_json())
            
            # Update report
            report.update({
                "success": True,
                "model_name": model_name,
                "model_dir": str(model_dir.relative_to(output_dir)),
                "weights_dir": str(weights_dir.relative_to(output_dir)),
                "total_files": len(report["files"]),
                "compilation_time": "N/A"  # Would track actual time
            })
            
            report["steps"].append({
                "step": "finalization",
                "status": "success"
            })
            
            # Write final report
            report_path = output_dir / "compilation_report.json"
            report_path.write_text(json.dumps(report, indent=2))
            
            self._log(f"\nCompilation successful!")
            self._log(f"Model: {model_name}")
            self._log(f"Parameters: {solution.actual_params:,}")
            self._log(f"Output directory: {output_dir}")
            
            if solution.warnings:
                self._log("\nWarnings:")
                for warning in solution.warnings:
                    self._log(f"  â€¢ {warning}")
            
            return report
            
        except Exception as e:
            report.update({
                "success": False,
                "error": str(e),
                "error_type": type(e).__name__
            })
            
            report["steps"].append({
                "step": "error",
                "status": "failed",
                "error": str(e)
            })
            
            # Write error report
            error_report = output_dir / "error_report.json"
            error_report.write_text(json.dumps(report, indent=2))
            
            self._log(f"\nCompilation failed: {e}")
            raise
    
    def _generate_model_name(self, spec: LLM, dims: Dict[str, int]) -> str:
        """Generate meaningful model name"""
        # Extract key information
        template = spec.template.replace("_", "-")
        
        # Get parameter count in billions
        param_b = dims.get('total_params', 0) / 1e9
        
        # Round to nearest decimal
        if param_b < 1:
            param_str = f"{param_b * 1000:.0f}M"
        else:
            param_str = f"{param_b:.1f}B".replace(".", "")
        
        # Architecture features
        features = []
        if spec.attention.value != "gqa":
            features.append(spec.attention.value)
        if spec.norm.value != "rmsnorm":
            features.append(spec.norm.value)
        if spec.activation.value != "swiglu":
            features.append(spec.activation.value)
        
        # Build name
        name_parts = [f"llmc-{template}", param_str]
        if features:
            name_parts.append("-".join(features[:2]))
        
        return "-".join(name_parts).lower()
    
    def _log(self, message: str):
        """Log message if verbose"""
        if self.verbose:
            print(message)

def compile_spec(spec: LLM, 
                 output_dir: Path,
                 verbose: bool = False,
                 dataset=None) -> Dict[str, Any]:
    """
    Compile LLM from specification.
    
    Args:
        spec: LLM specification
        output_dir: Output directory path
        verbose: Enable verbose logging
        
    Returns:
        Compilation report
    """
    compiler = LLMCompiler(verbose=verbose)
    return compiler.compile(spec, Path(output_dir), dataset=dataset)

def main():
    """Command-line interface"""
    import argparse
    
    parser = argparse.ArgumentParser(description="LLM Architecture Compiler")
    parser.add_argument("--spec", type=str, required=True,
                       help="Specification file (JSON)")
    parser.add_argument("--output", type=str, required=True,
                       help="Output directory")
    parser.add_argument("--verbose", action="store_true",
                       help="Verbose output")
    
    args = parser.parse_args()
    
    # Load spec
    spec_path = Path(args.spec)
    if not spec_path.exists():
        print(f"Error: Spec file not found: {spec_path}")
        sys.exit(1)
    
    spec_data = json.loads(spec_path.read_text())
    spec = LLM.from_dict(spec_data)
    
    # Compile
    try:
        report = compile_spec(spec, args.output, args.verbose)
        
        if report["success"]:
            print(f"\nâœ… Compilation successful!")
            print(f"ðŸ“ Output: {report['output_dir']}")
            print(f"ðŸ¤– Model: {report['model_name']}")
            print(f"ðŸ§® Parameters: {report['solution']['actual_params']:,}")
            sys.exit(0)
        else:
            print(f"\nâŒ Compilation failed: {report.get('error')}")
            sys.exit(1)
            
    except Exception as e:
        print(f"\nâŒ Compilation error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()

ðŸ“„ ./llm_compiler/templates/encoder_decoder.py
=============================
"""
Encoder-Decoder Transformer Template
====================================

Implements T5/BART-style encoder-decoder architectures.
"""

from typing import Dict, Any, List
from dataclasses import dataclass

from .base import ArchitectureTemplate, TemplateInfo, TemplateParameter
from ..ir.graph import IRGraph
from ..ir.builder import GraphBuilder
from ..solver.constraints import (
    ConstraintSystem, EqualityConstraint, 
    DivisibilityConstraint, RangeConstraint
)

class EncoderDecoderTemplate(ArchitectureTemplate):
    """Encoder-decoder transformer (T5/BART style)"""
    
    def __init__(self, version: str = "v1"):
        self.version = version
        
    @property
    def info(self) -> TemplateInfo:
        return TemplateInfo(
            name=f"encoder_decoder_{self.version}",
            description="Encoder-decoder transformer (T5/BART style)",
            version=self.version,
            parameters=[
                TemplateParameter.NUM_LAYERS,
                TemplateParameter.HIDDEN_SIZE,
                TemplateParameter.INTERMEDIATE_SIZE,
                TemplateParameter.NUM_HEADS,
                TemplateParameter.NUM_KV_HEADS,
                TemplateParameter.HEAD_DIM,
                TemplateParameter.VOCAB_SIZE,
                TemplateParameter.CONTEXT_LENGTH,
                "num_encoder_layers",
                "num_decoder_layers",
            ],
            required_parameters={
                TemplateParameter.VOCAB_SIZE,
                TemplateParameter.CONTEXT_LENGTH,
            },
            optional_parameters={
                TemplateParameter.NUM_LAYERS,
                TemplateParameter.HIDDEN_SIZE,
                TemplateParameter.INTERMEDIATE_SIZE,
                TemplateParameter.NUM_HEADS,
                TemplateParameter.NUM_KV_HEADS,
                TemplateParameter.HEAD_DIM,
                "num_encoder_layers",
                "num_decoder_layers",
            },
            default_constraints=[
                # Shared constraints
                DivisibilityConstraint(
                    var1=TemplateParameter.HIDDEN_SIZE.value,
                    var2=TemplateParameter.NUM_HEADS.value,
                    name="hidden_size_divisible_by_num_heads"
                ),
                # Encoder-decoder specific
                EqualityConstraint(
                    var1="num_layers",
                    var2="num_encoder_layers + num_decoder_layers",
                    name="total_layers_sum"
                ),
            ]
        )
    
    def create_constraint_system(self, spec: Dict[str, Any]) -> ConstraintSystem:
        """Create constraint system for encoder-decoder architecture"""
        system = ConstraintSystem()
        
        # Add default constraints
        for constraint in self.info.default_constraints:
            system.add_constraint(constraint)
        
        # Encoder-decoder typically uses same hidden size throughout
        system.add_constraint(EqualityConstraint(
            var1="encoder_hidden_size",
            var2=TemplateParameter.HIDDEN_SIZE.value,
            name="encoder_hidden_size_equals_main"
        ))
        
        system.add_constraint(EqualityConstraint(
            var1="decoder_hidden_size",
            var2=TemplateParameter.HIDDEN_SIZE.value,
            name="decoder_hidden_size_equals_main"
        ))
        
        # Add user constraints
        if 'explicit_dims' in spec:
            for param, value in spec['explicit_dims'].items():
                system.add_constraint(EqualityConstraint(
                    var1=param,
                    var2=str(value),
                    name=f"user_{param}"
                ))
        
        if 'target_params' in spec:
            system.add_constraint(EqualityConstraint(
                var1="total_params",
                var2=str(spec['target_params']),
                name="target_parameter_count"
            ))
        
        return system
    
    def calculate_parameters(self, dims: Dict[str, int]) -> int:
        """Calculate parameters for encoder-decoder transformer"""
        # Similar to decoder-only but with encoder and cross-attention
        hidden_size = dims[TemplateParameter.HIDDEN_SIZE.value]
        intermediate_size = dims[TemplateParameter.INTERMEDIATE_SIZE.value]
        vocab_size = dims[TemplateParameter.VOCAB_SIZE.value]
        num_heads = dims[TemplateParameter.NUM_HEADS.value]
        head_dim = dims.get(TemplateParameter.HEAD_DIM.value, hidden_size // num_heads)
        
        # Get layer counts
        num_encoder_layers = dims.get('num_encoder_layers', dims.get('num_layers', 12) // 2)
        num_decoder_layers = dims.get('num_decoder_layers', dims.get('num_layers', 12) // 2)
        
        # Encoder parameters (similar to decoder-only but without cross-attention)
        encoder_per_layer = self._calculate_encoder_layer_params(
            hidden_size, intermediate_size, num_heads, head_dim
        )
        
        # Decoder parameters (with cross-attention)
        decoder_per_layer = self._calculate_decoder_layer_params(
            hidden_size, intermediate_size, num_heads, head_dim
        )
        
        # Total
        total = (encoder_per_layer * num_encoder_layers +
                 decoder_per_layer * num_decoder_layers)
        
        # Embeddings (shared typically)
        total += vocab_size * hidden_size
        
        return total
    
    def _calculate_encoder_layer_params(self, hidden_size, intermediate_size, num_heads, head_dim):
        """Calculate parameters for one encoder layer"""
        params = 0
        
        # Self-attention
        params += 4 * hidden_size * hidden_size  # QKV + output
        params += hidden_size  # RMSNorm
        
        # MLP
        params += 2 * hidden_size * intermediate_size  # gate/up
        params += intermediate_size * hidden_size  # down
        params += hidden_size  # RMSNorm
        
        return params
    
    def _calculate_decoder_layer_params(self, hidden_size, intermediate_size, num_heads, head_dim):
        """Calculate parameters for one decoder layer"""
        params = self._calculate_encoder_layer_params(hidden_size, intermediate_size, num_heads, head_dim)
        
        # Add cross-attention
        params += 4 * hidden_size * hidden_size  # Cross-attention QKV + output
        params += hidden_size  # Additional RMSNorm for cross-attention
        
        return params
    
    def build_ir(self, dims: Dict[str, int], builder: GraphBuilder, spec: Dict[str, Any]) -> IRGraph:
        """Build encoder-decoder IR graph"""
        # Implementation similar to decoder-only but with encoder/decoder separation
        # This would be quite long - showing structure
        graph = IRGraph(name=f"encoder_decoder_{self.version}")
        
        # Build encoder
        # Build decoder with cross-attention
        # Connect them
        
        return graph
    
    def validate_spec(self, spec: Dict[str, Any]) -> List[str]:
        """Validate encoder-decoder spec"""
        errors = []
        
        if 'vocab_size' not in spec:
            errors.append("vocab_size is required")
        if 'context_length' not in spec:
            errors.append("context_length is required")
        
        return errors
ðŸ“„ ./llm_compiler/templates/decoder_only.py
=============================
"""
Decoder-Only Transformer Template
==================================

Implements GPT/LLaMA-style decoder-only architectures.
Supports MHA, MQA, GQA attention variants.
"""

import math
from typing import Dict, Any, List, Tuple, Optional
from dataclasses import dataclass

from .base import ArchitectureTemplate, TemplateInfo, TemplateParameter
from ..ir.graph import IRGraph, Tensor, Operation
from ..ir.builder import GraphBuilder
from ..ir.node import NodeType, Node
from ..solver.constraints import (
    Constraint, ConstraintSystem, EqualityConstraint, 
    DivisibilityConstraint, RangeConstraint, LinearConstraint
)
from ..utils.math_utils import round_to_multiple, find_divisors

class DecoderOnlyTemplate(ArchitectureTemplate):
    """Decoder-only transformer (GPT/LLaMA class)"""
    
    def __init__(self, version: str = "v1"):
        self.version = version
        
    @property
    def info(self) -> TemplateInfo:
        return TemplateInfo(
            name=f"decoder_only_{self.version}",
            description="Decoder-only transformer (GPT/LLaMA style)",
            version=self.version,
            parameters=[
                TemplateParameter.NUM_LAYERS,
                TemplateParameter.HIDDEN_SIZE,
                TemplateParameter.INTERMEDIATE_SIZE,
                TemplateParameter.NUM_HEADS,
                TemplateParameter.NUM_KV_HEADS,
                TemplateParameter.HEAD_DIM,
                TemplateParameter.VOCAB_SIZE,
                TemplateParameter.CONTEXT_LENGTH,
            ],
            required_parameters={
                TemplateParameter.VOCAB_SIZE,
                TemplateParameter.CONTEXT_LENGTH,
            },
            optional_parameters={
                TemplateParameter.NUM_LAYERS,
                TemplateParameter.HIDDEN_SIZE,
                TemplateParameter.INTERMEDIATE_SIZE,
                TemplateParameter.NUM_HEADS,
                TemplateParameter.NUM_KV_HEADS,
                TemplateParameter.HEAD_DIM,
            },
            default_constraints=[
                # Head dimension constraints
                DivisibilityConstraint(
                    var1=TemplateParameter.HIDDEN_SIZE.value,
                    var2=TemplateParameter.NUM_HEADS.value,
                    name="hidden_size_divisible_by_num_heads"
                ),
                DivisibilityConstraint(
                    var1=TemplateParameter.HIDDEN_SIZE.value,
                    var2=TemplateParameter.NUM_KV_HEADS.value,
                    name="hidden_size_divisible_by_num_kv_heads"
                ),
                # KV heads must divide query heads for GQA
                DivisibilityConstraint(
                    var1=TemplateParameter.NUM_HEADS.value,
                    var2=TemplateParameter.NUM_KV_HEADS.value,
                    name="num_heads_divisible_by_num_kv_heads"
                ),
                # Intermediate size typical expansion
                EqualityConstraint(
                    var1=TemplateParameter.INTERMEDIATE_SIZE.value,
                    var2=f"2.6875 * {TemplateParameter.HIDDEN_SIZE.value}",
                    name="swiglu_intermediate_size"
                ),
            ]
        )
    
    def create_constraint_system(self, spec: Dict[str, Any]) -> ConstraintSystem:
        """
        Create constraint system for decoder-only architecture.
        
        Handles attention type-specific constraints.
        """
        system = ConstraintSystem()
        
        # Add default constraints
        for constraint in self.info.default_constraints:
            system.add_constraint(constraint)
        
        # Add attention-specific constraints
        attention_type = spec.get('attention', 'gqa')
        
        if attention_type == 'mha':
            # Multi-head: num_kv_heads == num_heads
            system.add_constraint(EqualityConstraint(
                var1=TemplateParameter.NUM_KV_HEADS.value,
                var2=TemplateParameter.NUM_HEADS.value,
                name="mha_kv_equals_q"
            ))
        elif attention_type == 'mqa':
            # Multi-query: num_kv_heads == 1
            system.add_constraint(EqualityConstraint(
                var1=TemplateParameter.NUM_KV_HEADS.value,
                var2="1",
                name="mqa_single_kv_head"
            ))
        elif attention_type == 'gqa':
            # Grouped-query: already handled by divisibility constraint
            pass
        
        # Add user-specified constraints if any
        if 'explicit_dims' in spec:
            for param, value in spec['explicit_dims'].items():
                system.add_constraint(EqualityConstraint(
                    var1=param,
                    var2=str(value),
                    name=f"user_{param}"
                ))
        
        # Add parameter count constraint if target specified
        if 'target_params' in spec:
            system.add_constraint(EqualityConstraint(
                var1="total_params",
                var2=str(spec['target_params']),
                name="target_parameter_count"
            ))
        
        # Add activation-specific constraints
        if spec.get('activation') == 'swiglu':
            # SwiGLU intermediate size formula
            system.add_constraint(EqualityConstraint(
                var1=TemplateParameter.INTERMEDIATE_SIZE.value,
                var2=f"2 * round({TemplateParameter.HIDDEN_SIZE.value} * 8/3 / 32) * 32",
                name="swiglu_exact_intermediate"
            ))
        
        return system
    
    def calculate_parameters(self, dims: Dict[str, int]) -> int:
        """
        Calculate exact parameter count for decoder-only transformer.
        
        Formula based on LLaMA/GPT architecture:
        - Embedding: vocab_size * hidden_size
        - Output layer: hidden_size * vocab_size (if not tied)
        - Per layer:
          â€¢ Attention QKV: 3 * hidden_size * hidden_size
          â€¢ Attention output: hidden_size * hidden_size
          â€¢ Attention norm: hidden_size * 2
          â€¢ MLP gate/up: 2 * hidden_size * intermediate_size
          â€¢ MLP down: intermediate_size * hidden_size
          â€¢ MLP norm: hidden_size * 2
        """
        n_layers = dims[TemplateParameter.NUM_LAYERS.value]
        hidden_size = dims[TemplateParameter.HIDDEN_SIZE.value]
        intermediate_size = dims[TemplateParameter.INTERMEDIATE_SIZE.value]
        vocab_size = dims[TemplateParameter.VOCAB_SIZE.value]
        num_heads = dims[TemplateParameter.NUM_HEADS.value]
        num_kv_heads = dims.get(TemplateParameter.NUM_KV_HEADS.value, num_heads)
        head_dim = dims.get(TemplateParameter.HEAD_DIM.value, hidden_size // num_heads)
        
        # Calculate per-layer parameters
        per_layer = 0
        
        # Attention QKV projections
        # In grouped-query attention, KV heads are fewer
        q_size = num_heads * head_dim
        k_size = num_kv_heads * head_dim
        v_size = num_kv_heads * head_dim
        
        per_layer += hidden_size * (q_size + k_size + v_size)  # QKV weights
        per_layer += (q_size + k_size + v_size)  # QKV biases (if any)
        
        # Attention output projection
        per_layer += (num_heads * head_dim) * hidden_size  # Output weights
        per_layer += hidden_size  # Output bias (if any)
        
        # Attention normalization (RMSNorm)
        per_layer += hidden_size  # RMSNorm scale (no bias)
        
        # MLP (SwiGLU)
        # gate_proj + up_proj (both hidden_size -> intermediate_size)
        per_layer += 2 * hidden_size * intermediate_size
        per_layer += 2 * intermediate_size  # biases
        
        # down_proj (intermediate_size -> hidden_size)
        per_layer += intermediate_size * hidden_size
        per_layer += hidden_size  # bias
        
        # MLP normalization
        per_layer += hidden_size  # RMSNorm scale
        
        # Total layers
        total = per_layer * n_layers
        
        # Embeddings
        total += vocab_size * hidden_size  # token embeddings
        
        # Output layer (if not tied)
        tie_embeddings = True  # Default for decoder-only
        if not tie_embeddings:
            total += hidden_size * vocab_size
        
        return total
    
    def build_ir(self, 
                 dims: Dict[str, int],
                 builder: GraphBuilder,
                 spec: Dict[str, Any]) -> IRGraph:
        """
        Build decoder-only transformer IR graph.
        """
        # Extract dimensions
        n_layers = dims[TemplateParameter.NUM_LAYERS.value]
        hidden_size = dims[TemplateParameter.HIDDEN_SIZE.value]
        intermediate_size = dims[TemplateParameter.INTERMEDIATE_SIZE.value]
        vocab_size = dims[TemplateParameter.VOCAB_SIZE.value]
        context_length = dims[TemplateParameter.CONTEXT_LENGTH.value]
        num_heads = dims[TemplateParameter.NUM_HEADS.value]
        num_kv_heads = dims.get(TemplateParameter.NUM_KV_HEADS.value, num_heads)
        head_dim = dims.get(TemplateParameter.HEAD_DIM.value, hidden_size // num_heads)
        
        # Start building graph
        graph = IRGraph(name=f"decoder_only_{self.version}")
        
        # Input token IDs
        tokens = builder.create_input(
            name="input_ids",
            shape=[-1, context_length],  # batch size dynamic
            dtype="int32"
        )
        
        # Token embeddings
        embeddings = builder.create_embedding(
            name="token_embeddings",
            input=tokens,
            vocab_size=vocab_size,
            embedding_dim=hidden_size
        )
        
        # Positional embeddings
        if spec.get('positional_encoding') == 'rope':
            pos_emb = builder.create_rope(
                name="rope_positional",
                input=embeddings,
                dim=head_dim,
                theta=spec.get('rope_theta', 10000.0),
                scaling_factor=spec.get('rope_scaling_factor')
            )
            current = pos_emb
        elif spec.get('positional_encoding') == 'alibi':
            pos_emb = builder.create_alibi(
                name="alibi_bias",
                num_heads=num_heads,
                max_bias=spec.get('alibi_max_bias', 8.0)
            )
            current = embeddings
        else:
            current = embeddings
        
        # Initial normalization
        if spec.get('norm') == 'rmsnorm':
            current = builder.create_rmsnorm(
                name="input_norm",
                input=current,
                dim=hidden_size
            )
        
        # Build decoder layers
        for layer_idx in range(n_layers):
            layer_prefix = f"layer_{layer_idx}"
            
            # Attention residual path
            attn_norm = builder.create_rmsnorm(
                name=f"{layer_prefix}_attn_norm",
                input=current,
                dim=hidden_size
            )
            
            # Attention
            if spec.get('attention') in ['mha', 'gqa', 'mqa']:
                # QKV projections
                q_proj = builder.create_linear(
                    name=f"{layer_prefix}_q_proj",
                    input=attn_norm,
                    in_features=hidden_size,
                    out_features=num_heads * head_dim
                )
                
                k_proj = builder.create_linear(
                    name=f"{layer_prefix}_k_proj",
                    input=attn_norm,
                    in_features=hidden_size,
                    out_features=num_kv_heads * head_dim
                )
                
                v_proj = builder.create_linear(
                    name=f"{layer_prefix}_v_proj",
                    input=attn_norm,
                    in_features=hidden_size,
                    out_features=num_kv_heads * head_dim
                )
                
                # Multi-head attention
                attn_output = builder.create_multi_head_attention(
                    name=f"{layer_prefix}_attn",
                    query=q_proj,
                    key=k_proj,
                    value=v_proj,
                    num_heads=num_heads,
                    num_kv_heads=num_kv_heads,
                    head_dim=head_dim,
                    attention_type=spec.get('attention', 'gqa'),
                    use_alibi=(spec.get('positional_encoding') == 'alibi')
                )
            else:
                raise ValueError(f"Unsupported attention type: {spec.get('attention')}")
            
            # Attention output projection
            attn_out_proj = builder.create_linear(
                name=f"{layer_prefix}_attn_out_proj",
                input=attn_output,
                in_features=num_heads * head_dim,
                out_features=hidden_size
            )
            
            # First residual connection
            attn_residual = builder.create_add(
                name=f"{layer_prefix}_attn_residual",
                a=current,
                b=attn_out_proj
            )
            
            # MLP path
            mlp_norm = builder.create_rmsnorm(
                name=f"{layer_prefix}_mlp_norm",
                input=attn_residual,
                dim=hidden_size
            )
            
            # SwiGLU MLP
            if spec.get('activation') == 'swiglu':
                # Gate projection
                gate_proj = builder.create_linear(
                    name=f"{layer_prefix}_gate_proj",
                    input=mlp_norm,
                    in_features=hidden_size,
                    out_features=intermediate_size
                )
                
                # Up projection
                up_proj = builder.create_linear(
                    name=f"{layer_prefix}_up_proj",
                    input=mlp_norm,
                    in_features=hidden_size,
                    out_features=intermediate_size
                )
                
                # SwiGLU activation
                gate_act = builder.create_swiglu(
                    name=f"{layer_prefix}_swiglu",
                    gate=gate_proj,
                    up=up_proj
                )
            else:
                # Standard MLP
                fc1 = builder.create_linear(
                    name=f"{layer_prefix}_fc1",
                    input=mlp_norm,
                    in_features=hidden_size,
                    out_features=intermediate_size
                )
                
                gate_act = builder.create_activation(
                    name=f"{layer_prefix}_activation",
                    input=fc1,
                    activation_type=spec.get('activation', 'silu')
                )
            
            # Down projection
            down_proj = builder.create_linear(
                name=f"{layer_prefix}_down_proj",
                input=gate_act,
                in_features=intermediate_size,
                out_features=hidden_size
            )
            
            # Second residual connection
            current = builder.create_add(
                name=f"{layer_prefix}_mlp_residual",
                a=attn_residual,
                b=down_proj
            )
        
        # Final normalization
        if spec.get('norm') == 'rmsnorm':
            current = builder.create_rmsnorm(
                name="output_norm",
                input=current,
                dim=hidden_size
            )
        
        # Output projection (tied or separate)
        if spec.get('tie_word_embeddings', True):
            # Use embedding weights for output
            output = builder.create_linear(
                name="output_projection",
                input=current,
                in_features=hidden_size,
                out_features=vocab_size,
                use_bias=False,
                tie_weight="token_embeddings.weight"  # Reference embedding weights
            )
        else:
            # Separate output weights
            output = builder.create_linear(
                name="output_projection",
                input=current,
                in_features=hidden_size,
                out_features=vocab_size,
                use_bias=False
            )
        
        # Set as output
        builder.set_output(output, name="logits")
        
        return graph
    
    def validate_spec(self, spec: Dict[str, Any]) -> List[str]:
        """Validate specification against template requirements"""
        errors = []
        
        # Check required parameters
        if 'vocab_size' not in spec:
            errors.append("vocab_size is required")
        if 'context_length' not in spec:
            errors.append("context_length is required")
        
        # Check attention-specific requirements
        attention = spec.get('attention', 'gqa')
        if attention == 'sliding_window':
            if 'sliding_window_size' not in spec:
                errors.append("sliding_window_size required for sliding_window attention")
        
        # Check positional encoding compatibility
        pos_enc = spec.get('positional_encoding', 'rope')
        if pos_enc == 'rope':
            if attention in ['alibi', 'relative']:
                errors.append(f"rope positional encoding incompatible with {attention} attention")
        
        # Check activation function
        activation = spec.get('activation', 'swiglu')
        if activation not in ['gelu', 'relu', 'silu', 'swiglu']:
            errors.append(f"Unsupported activation: {activation}")
        
        return errors
ðŸ“„ ./llm_compiler/templates/registry.py
=============================
"""
Template Registry
================

Manages available architecture templates.
Central registry for all template implementations.
"""

from typing import Dict, Type, List
import inspect

from .base import ArchitectureTemplate
from .decoder_only import DecoderOnlyTemplate
from .encoder_decoder import EncoderDecoderTemplate

class TemplateRegistry:
    """Registry for all architecture templates"""
    
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._templates = {}
            cls._instance._initialize()
        return cls._instance
    
    def _initialize(self):
        """Initialize with built-in templates"""
        self.register(DecoderOnlyTemplate())
        self.register(DecoderOnlyTemplate(version="v2"))
        self.register(EncoderDecoderTemplate())
    
    def register(self, template: ArchitectureTemplate):
        """Register a template"""
        self._templates[template.info.name] = template
    
    def get(self, name: str) -> ArchitectureTemplate:
        """Get template by name"""
        if name not in self._templates:
            raise ValueError(f"Template not found: {name}. "
                           f"Available: {list(self._templates.keys())}")
        return self._templates[name]
    
    def list(self) -> List[str]:
        """List all registered template names"""
        return list(self._templates.keys())
    
    def list_with_info(self) -> Dict[str, dict]:
        """List templates with their metadata"""
        return {
            name: {
                'description': template.info.description,
                'version': template.info.version,
                'parameters': [p.value for p in template.info.parameters],
                'required': [p.value for p in template.info.required_parameters],
            }
            for name, template in self._templates.items()
        }

# Global registry instance
registry = TemplateRegistry()

def get_template(name: str) -> ArchitectureTemplate:
    """Get template from global registry"""
    return registry.get(name)

def list_templates() -> List[str]:
    """List all available templates"""
    return registry.list()

def list_templates_with_info() -> Dict[str, dict]:
    """List templates with metadata"""
    return registry.list_with_info()
ðŸ“„ ./llm_compiler/templates/base.py
=============================
"""
Base Template Class
===================

Defines the interface that all architecture templates must implement.
Templates are responsible for:
1. Declaring their degrees of freedom
2. Defining parameter calculation formulas
3. Generating IR graphs
4. Enforcing architecture-specific constraints
"""

from abc import ABC, abstractmethod
from typing import Dict, Any, List, Set, Tuple, Optional
from dataclasses import dataclass
from enum import Enum

from ..ir.graph import IRGraph, Tensor
from ..ir.builder import GraphBuilder
from ..solver.constraints import Constraint, ConstraintSystem

class TemplateParameter(Enum):
    """Standard template parameters"""
    NUM_LAYERS = "num_layers"
    HIDDEN_SIZE = "hidden_size"
    INTERMEDIATE_SIZE = "intermediate_size"
    NUM_HEADS = "num_heads"
    NUM_KV_HEADS = "num_kv_heads"
    HEAD_DIM = "head_dim"
    VOCAB_SIZE = "vocab_size"
    CONTEXT_LENGTH = "context_length"

@dataclass
class TemplateInfo:
    """Metadata about a template"""
    name: str
    description: str
    version: str
    parameters: List[TemplateParameter]
    required_parameters: Set[TemplateParameter]
    optional_parameters: Set[TemplateParameter]
    default_constraints: List[Constraint]

class ArchitectureTemplate(ABC):
    """
    Abstract base class for all architecture templates.
    
    Templates must be deterministic and explicit - they cannot make
    arbitrary choices on behalf of the user.
    """
    
    @property
    @abstractmethod
    def info(self) -> TemplateInfo:
        """Return template metadata"""
        pass
    
    @abstractmethod
    def create_constraint_system(self, spec: Dict[str, Any]) -> ConstraintSystem:
        """
        Create constraint system for this template.
        
        Args:
            spec: User specification with some parameters fixed
            
        Returns:
            ConstraintSystem with all architectural constraints
        """
        pass
    
    @abstractmethod
    def calculate_parameters(self, dims: Dict[str, int]) -> int:
        """
        Calculate total parameter count for given dimensions.
        
        Must match exactly what the generated model will have.
        
        Args:
            dims: Complete dimension dictionary
            
        Returns:
            Total number of trainable parameters
        """
        pass
    
    @abstractmethod
    def build_ir(self, 
                 dims: Dict[str, int],
                 builder: GraphBuilder,
                 spec: Dict[str, Any]) -> IRGraph:
        """
        Build the IR graph for this architecture.
        
        Args:
            dims: Solved dimensions
            builder: Graph builder helper
            spec: Complete specification
            
        Returns:
            Complete IR graph
        """
        pass
    
    @abstractmethod
    def validate_spec(self, spec: Dict[str, Any]) -> List[str]:
        """
        Validate specification against template requirements.
        
        Args:
            spec: User specification
            
        Returns:
            List of error messages (empty if valid)
        """
        pass
    
    def get_parameter_bounds(self) -> Dict[str, Tuple[Optional[int], Optional[int]]]:
        """
        Get reasonable bounds for each parameter.
        
        Returns:
            Dict mapping parameter name to (min, max) bounds
        """
        return {
            TemplateParameter.NUM_LAYERS.value: (1, 1000),
            TemplateParameter.HIDDEN_SIZE.value: (128, 65536),
            TemplateParameter.INTERMEDIATE_SIZE.value: (128, 262144),
            TemplateParameter.NUM_HEADS.value: (1, 256),
            TemplateParameter.NUM_KV_HEADS.value: (1, 256),
            TemplateParameter.HEAD_DIM.value: (32, 256),
            TemplateParameter.VOCAB_SIZE.value: (32, 1000000),
            TemplateParameter.CONTEXT_LENGTH.value: (1, 1000000),
        }
ðŸ“„ ./llm_compiler/emitters/pytorch.py
=============================
"""
PyTorch Emitter
===============

Emits PyTorch modules that *execute directly from the IR*.
The generated model:
- Parses the frozen IR JSON embedded in the emitted file
- Instantiates nn.Modules only for parameter-carrying IR nodes
- Executes operations in topological order without re-deciding structure
"""

from __future__ import annotations
from typing import Dict, Any, List
import json
import textwrap
from pathlib import Path

from ..ir.graph import IRGraph, NodeType


class PyTorchEmitter:
    """IR-driven PyTorch emitter."""

    def __init__(self, precision: str = "float32"):
        self.precision = precision
        self.torch_dtype_map = {
            "float32": "torch.float32",
            "float16": "torch.float16",
            "bfloat16": "torch.bfloat16",
            "int32": "torch.int32",
            "int64": "torch.int64",
        }

    # ------------------------------------------------------------------ public
    def emit(self, graph: IRGraph, model_name: str, output_dir: Path) -> Dict[str, str]:
        files: Dict[str, str] = {}

        files[f"{model_name}.py"] = self._emit_model_file(graph, model_name)
        files["config.py"] = self._emit_config(graph)
        files["__init__.py"] = self._emit_init_file(model_name)
        files["setup.py"] = self._emit_setup_file(model_name)
        files[f"test_{model_name}.py"] = self._emit_test_file(model_name)
        return files

    # ----------------------------------------------------------------- emitters
    def _emit_model_file(self, graph: IRGraph, model_name: str) -> str:
        """Emit the IR-driven PyTorch model."""
        graph_blob = graph.to_json()
        dtype_literal = self.torch_dtype_map.get(self.precision, "torch.float32")

        model_code = textwrap.dedent(
            f'''
            import json
            from typing import Dict, Any, Optional
            import torch
            import torch.nn as nn
            import torch.nn.functional as F

            from .config import ModelConfig

            IR_JSON = r"""{graph_blob}"""
            IR_DEF = json.loads(IR_JSON)


            # ------------------------------------------------------------- helpers
            def _topological_sort(ir: Dict[str, Any]):
                ops = ir["operations"]
                tensors = ir["tensors"]
                indegree = {{name: 0 for name in ops}}
                for op_name, op in ops.items():
                    for inp in op["inputs"]:
                        producer = tensors.get(inp, {{}}).get("node")
                        if producer and producer in indegree:
                            indegree[op_name] += 1
                ready = [name for name, deg in indegree.items() if deg == 0]
                order = []
                while ready:
                    current = ready.pop(0)
                    order.append(current)
                    for out in ops[current]["outputs"]:
                        for consumer in tensors.get(out, {{}}).get("consumers", []):
                            if consumer in indegree:
                                indegree[consumer] -= 1
                                if indegree[consumer] == 0:
                                    ready.append(consumer)
                if len(order) != len(ops):
                    # Fallback to insertion order if a cycle slipped in
                    return list(ops.keys())
                return order


            class RMSNorm(nn.Module):
                def __init__(self, dim: int, eps: float = 1e-6):
                    super().__init__()
                    self.eps = eps
                    self.weight = nn.Parameter(torch.ones(dim))

                def forward(self, x: torch.Tensor) -> torch.Tensor:
                    norm_x = x.pow(2).mean(-1, keepdim=True)
                    x_normed = x * torch.rsqrt(norm_x + self.eps)
                    return self.weight * x_normed


            class AttentionOp(nn.Module):
                def __init__(self, num_heads: int, num_kv_heads: int, head_dim: int, dropout: float = 0.0):
                    super().__init__()
                    self.num_heads = num_heads
                    self.num_kv_heads = num_kv_heads
                    self.head_dim = head_dim
                    self.dropout = dropout
                    self.num_heads_per_kv = max(1, num_heads // num_kv_heads)

                def forward(self, q, k, v, attention_mask=None):
                    bsz, seq_len, _ = q.shape
                    q = q.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
                    k = k.view(bsz, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)
                    v = v.view(bsz, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)

                    if self.num_kv_heads != self.num_heads:
                        k = k.repeat_interleave(self.num_heads_per_kv, dim=1)
                        v = v.repeat_interleave(self.num_heads_per_kv, dim=1)

                    attn = F.scaled_dot_product_attention(
                        q, k, v, attn_mask=attention_mask, dropout_p=self.dropout
                    )
                    attn = attn.transpose(1, 2).reshape(bsz, seq_len, self.num_heads * self.head_dim)
                    return attn


            def _apply_rope(x: torch.Tensor, theta: float = 10000.0):
                if x.size(-1) % 2 != 0:
                    return x
                half = x.size(-1) // 2
                freqs = torch.arange(half, device=x.device, dtype=x.dtype)
                freqs = theta ** (-freqs / half)
                positions = torch.arange(x.size(1), device=x.device, dtype=x.dtype)
                angles = torch.einsum("i,j->ij", positions, freqs)
                sin, cos = angles.sin(), angles.cos()
                x1, x2 = x[..., :half], x[..., half:]
                rotated = torch.stack([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)
                return rotated.reshape_as(x)


            def _activation(kind: str, tensor: torch.Tensor):
                if kind == "relu":
                    return F.relu(tensor)
                if kind == "gelu":
                    return F.gelu(tensor)
                if kind == "silu":
                    return F.silu(tensor)
                if kind == "swiglu":
                    # Caller supplies gate/up split; we only keep hook
                    return tensor
                raise ValueError(f"Unsupported activation: {{kind}}")


            # -------------------------------------------------------------- model
            class {model_name}(nn.Module):
                def __init__(self, config: Optional[ModelConfig] = None):
                    super().__init__()
                    self.ir = IR_DEF
                    self.order = _topological_sort(IR_DEF)
                    self.config = config or ModelConfig()
                    self.modules_by_op = nn.ModuleDict()

                    for name, op in self.ir["operations"].items():
                        op_type = op["type"]
                        attrs = op.get("attributes", {{}})
                        if op_type == "embedding":
                            self.modules_by_op[name] = nn.Embedding(
                                attrs["vocab_size"], attrs["embedding_dim"]
                            )
                        elif op_type == "linear":
                            self.modules_by_op[name] = nn.Linear(
                                attrs["in_features"],
                                attrs["out_features"],
                                bias=attrs.get("use_bias", True),
                            )
                        elif op_type == "rmsnorm":
                            self.modules_by_op[name] = RMSNorm(
                                attrs["normalized_shape"], eps=attrs.get("eps", 1e-6)
                            )
                        elif op_type == "layernorm":
                            self.modules_by_op[name] = nn.LayerNorm(
                                attrs["normalized_shape"], eps=attrs.get("eps", 1e-5)
                            )
                        elif op_type == "multi_head_attention":
                            self.modules_by_op[name] = AttentionOp(
                                attrs["num_heads"],
                                attrs.get("num_kv_heads", attrs["num_heads"]),
                                attrs["head_dim"],
                                dropout=self.config.attention_dropout,
                            )
                        # Parameter-free ops are executed directly in forward

                    # Handle tied weights declared in the IR
                    for name, op in self.ir["operations"].items():
                        attrs = op.get("attributes", {{}})
                        tie_target = attrs.get("tie_weight")
                        if tie_target and name in self.modules_by_op:
                            target_module = tie_target.split(".")[0]
                            if target_module in self.modules_by_op:
                                self.modules_by_op[name].weight = self.modules_by_op[target_module].weight

                    # Move to configured dtype
                    self.to(dtype={dtype_literal})

                def forward(self, **inputs):
                    values: Dict[str, torch.Tensor] = {{}}

                    # Bind graph inputs
                    for required in self.ir["inputs"]:
                        if required not in inputs:
                            raise ValueError(f"Missing required input '{{required}}'")
                        values[required] = inputs[required]

                    attention_mask = inputs.get("attention_mask")

                    for op_name in self.order:
                        op = self.ir["operations"][op_name]
                        op_type = op["type"]
                        attrs = op.get("attributes", {{}})
                        args = [values[i] for i in op.get("inputs", [])]

                        if op_type == "embedding":
                            out = self.modules_by_op[op_name](args[0])
                        elif op_type == "linear":
                            out = self.modules_by_op[op_name](args[0])
                        elif op_type == "rmsnorm" or op_type == "layernorm":
                            out = self.modules_by_op[op_name](args[0])
                        elif op_type == "add":
                            out = args[0] + args[1]
                        elif op_type == "mul":
                            out = args[0] * args[1]
                        elif op_type == "activation":
                            out = _activation(attrs.get("activation", "silu"), args[0])
                        elif op_type == "swiglu":
                            out = F.silu(args[0]) * args[1]
                        elif op_type == "multi_head_attention":
                            out = self.modules_by_op[op_name](args[0], args[1], args[2], attention_mask)
                        elif op_type == "rope":
                            out = _apply_rope(args[0], theta=attrs.get("theta", 10000.0))
                        elif op_type == "softmax":
                            out = F.softmax(args[0], dim=attrs.get("dim", -1))
                        else:
                            raise RuntimeError(f"Unsupported IR op type: {{op_type}}")

                        # Assume single output per op
                        out_name = op["outputs"][0]
                        values[out_name] = out

                    # Collect graph outputs
                    outputs = {{name: values[name] for name in self.ir["outputs"]}}
                    if len(outputs) == 1:
                        return next(iter(outputs.values()))
                    return outputs
            '''
        )

        return model_code

    def _derive_config_defaults(self, graph: IRGraph) -> Dict[str, Any]:
        dims: Dict[str, Any] = {}
        # Use first embedding as authoritative vocab/hidden sizes
        for op in graph.operations.values():
            if op.node_type == NodeType.EMBEDDING:
                dims["vocab_size"] = op.attributes.get("vocab_size", 50000)
                dims["hidden_size"] = op.attributes.get("embedding_dim", 4096)
                break

        # Heads / head_dim from first attention op
        for op in graph.operations.values():
            if op.node_type == NodeType.MULTI_HEAD_ATTENTION:
                dims["num_attention_heads"] = op.attributes.get("num_heads", 32)
                dims["num_key_value_heads"] = op.attributes.get("num_kv_heads", dims["num_attention_heads"])
                dims["head_dim"] = op.attributes.get("head_dim", 128)
                break

        # Intermediate size from first gate/up projection
        for op in graph.operations.values():
            if op.node_type == NodeType.LINEAR and "up_proj" in op.name:
                dims["intermediate_size"] = op.attributes.get("out_features", 11008)
                break

        # Layers
        dims["num_hidden_layers"] = sum(1 for op in graph.operations.values() if op.node_type == NodeType.MULTI_HEAD_ATTENTION)

        # Context length from input tensor shapes
        for tensor in graph.tensors.values():
            if tensor.name in graph.inputs and tensor.shape:
                if len(tensor.shape) >= 2 and tensor.shape[1] not in (-1, None):
                    dims["context_length"] = tensor.shape[1]
                    break
        dims.setdefault("context_length", 8192)

        return dims

    def _emit_config(self, graph: IRGraph) -> str:
        dims = self._derive_config_defaults(graph)
        return textwrap.dedent(
            f"""
            from dataclasses import dataclass
            from typing import Optional


            @dataclass
            class ModelConfig:
                vocab_size: int = {dims.get('vocab_size', 50000)}
                hidden_size: int = {dims.get('hidden_size', 4096)}
                intermediate_size: int = {dims.get('intermediate_size', 11008)}
                num_hidden_layers: int = {dims.get('num_hidden_layers', 32)}
                num_attention_heads: int = {dims.get('num_attention_heads', 32)}
                num_key_value_heads: int = {dims.get('num_key_value_heads', dims.get('num_attention_heads', 32))}
                head_dim: int = {dims.get('head_dim', 128)}
                max_position_embeddings: int = {dims.get('context_length', 8192)}

                attention_dropout: float = 0.0
                hidden_dropout: float = 0.0
                torch_dtype: str = "{self.precision}"

                def to_dict(self):
                    return self.__dict__.copy()
            """
        )

    def _emit_init_file(self, model_name: str) -> str:
        return textwrap.dedent(
            f"""
            from .config import ModelConfig
            from .{model_name} import {model_name}

            __all__ = [
                "ModelConfig",
                "{model_name}",
            ]

            __version__ = "1.0.0"
            """
        )

    def _emit_setup_file(self, model_name: str) -> str:
        return textwrap.dedent(
            f"""
            from setuptools import setup, find_packages

            setup(
                name="{model_name.lower()}",
                version="1.0.0",
                author="LLM Compiler",
                description="Generated LLM model",
                packages=find_packages(),
                python_requires=">=3.8",
                install_requires=[
                    "torch>=2.0.0",
                ],
            )
            """
        )

    def _emit_test_file(self, model_name: str) -> str:
        return textwrap.dedent(
            f"""
            import torch
            from .config import ModelConfig
            from .{model_name} import {model_name}


            def test_model():
                config = ModelConfig()
                model = {model_name}(config)
                batch_size = 2
                seq_len = min(16, config.max_position_embeddings)

                input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))
                outputs = model(input_ids=input_ids)
                if isinstance(outputs, dict):
                    logits = list(outputs.values())[0]
                else:
                    logits = outputs
                assert logits.shape[:2] == (batch_size, seq_len)
                print("Model forward succeeded; output shape", logits.shape)


            if __name__ == "__main__":
                test_model()
            """
        )

ðŸ“„ ./llm_compiler/emitters/safetensors.py
=============================
"""
Safetensors Emitter
===================

Emits model weights in safetensors format.
Generates both weights and metadata.
"""

from typing import Dict, List, Any, Optional
import json
import struct
import numpy as np
from pathlib import Path

class SafetensorsEmitter:
    """Emits weights in safetensors format"""
    
    def __init__(self):
        self.header_size = 8  # bytes for header length
        
    def emit(self,
             weights: Dict[str, np.ndarray],
             metadata: Dict[str, Any],
             output_path: Path) -> Dict[str, Any]:
        """
        Emit weights in safetensors format.
        
        Args:
            weights: Dictionary of tensor name -> numpy array
            metadata: Model metadata
            output_path: Output file path
            
        Returns:
            Dictionary with emission info
        """
        # Prepare tensors
        tensor_data = {}
        offsets = {}
        
        current_offset = 0
        
        # Calculate offsets
        for name, tensor in weights.items():
            # Convert to contiguous array
            tensor = np.ascontiguousarray(tensor)
            
            # Store data
            tensor_data[name] = tensor
            
            # Calculate offset and size
            size = tensor.nbytes
            dtype = self._numpy_to_safetensors_dtype(tensor.dtype)
            shape = list(tensor.shape)
            
            offsets[name] = {
                "dtype": dtype,
                "shape": shape,
                "data_offsets": [current_offset, current_offset + size]
            }
            
            current_offset += size
        
        # Create header
        header = {
            "__metadata__": metadata
        }
        header.update(offsets)
        
        # Serialize header
        header_json = json.dumps(header).encode('utf-8')
        header_length = len(header_json)
        
        # Calculate padding
        total_header_size = self.header_size + header_length
        pad = 8 - (total_header_size % 8)
        if pad == 8:
            pad = 0
        
        # Write file
        with open(output_path, 'wb') as f:
            # Write header length
            f.write(struct.pack('<Q', header_length))
            
            # Write header
            f.write(header_json)
            
            # Write padding
            if pad > 0:
                f.write(b'\x00' * pad)
            
            # Write tensor data
            for name in weights.keys():
                tensor = tensor_data[name]
                f.write(tensor.tobytes())
        
        # Return info
        return {
            "file_path": str(output_path),
            "file_size": output_path.stat().st_size,
            "num_tensors": len(weights),
            "total_bytes": current_offset,
            "header_size": total_header_size + pad,
            "metadata": metadata
        }
    
    def _numpy_to_safetensors_dtype(self, dtype: np.dtype) -> str:
        """Convert numpy dtype to safetensors dtype string"""
        mapping = {
            np.float32: "F32",
            np.float16: "F16",
            np.bfloat16: "BF16",
            np.int32: "I32",
            np.int64: "I64",
            np.uint8: "U8",
            np.bool_: "BOOL",
        }
        
        for np_type, st_type in mapping.items():
            if np.issubdtype(dtype, np_type):
                return st_type
        
        raise ValueError(f"Unsupported dtype: {dtype}")
    
    def create_metadata(self,
                       model_name: str,
                       config: Dict[str, Any],
                       parameter_count: int) -> Dict[str, Any]:
        """Create safetensors metadata"""
        return {
            "model_name": model_name,
            "format": "pytorch",
            "architecture": config.get('template', 'unknown'),
            "vocab_size": config.get('vocab_size', 0),
            "hidden_size": config.get('hidden_size', 0),
            "num_layers": config.get('num_layers', 0),
            "num_attention_heads": config.get('num_attention_heads', 0),
            "num_key_value_heads": config.get('num_key_value_heads', 0),
            "head_dim": config.get('head_dim', 0),
            "intermediate_size": config.get('intermediate_size', 0),
            "max_position_embeddings": config.get('context_length', 0),
            "rope_theta": config.get('rope_theta', 10000.0),
            "attention_type": config.get('attention_type', 'gqa'),
            "norm_type": config.get('norm', 'rmsnorm'),
            "activation": config.get('activation', 'swiglu'),
            "parameter_count": parameter_count,
            "generator": "llm_compiler",
            "version": "1.0.0"
        }
    
    def generate_weight_names(self, 
                             model_name: str,
                             num_layers: int) -> List[str]:
        """Generate weight names for model"""
        names = []
        
        # Embeddings
        names.append(f"{model_name}.embed_tokens.weight")
        
        # Layers
        for i in range(num_layers):
            prefix = f"{model_name}.layers.{i}"
            
            # Attention
            names.append(f"{prefix}.self_attn.q_proj.weight")
            names.append(f"{prefix}.self_attn.k_proj.weight")
            names.append(f"{prefix}.self_attn.v_proj.weight")
            names.append(f"{prefix}.self_attn.o_proj.weight")
            
            # Norms
            names.append(f"{prefix}.input_layernorm.weight")
            names.append(f"{prefix}.post_attention_layernorm.weight")
            
            # MLP
            names.append(f"{prefix}.mlp.w1.weight")
            names.append(f"{prefix}.mlp.w2.weight")
            names.append(f"{prefix}.mlp.w3.weight")
        
        # Final norm
        names.append(f"{model_name}.norm.weight")
        
        # LM head (if not tied)
        names.append(f"{model_name}.lm_head.weight")
        
        return names%                                                                                                 apple@Apples-MacBook-Air alwork % 
